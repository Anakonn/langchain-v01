---
translated: true
---

# Google Memorystore for Redis

> [Google Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) es un servicio totalmente administrado que se basa en el almac√©n de datos en memoria Redis para crear cach√©s de aplicaciones que proporcionan acceso a datos en menos de un milisegundo. Extienda su aplicaci√≥n de base de datos para crear experiencias impulsadas por IA aprovechando las integraciones de Langchain de Memorystore for Redis.

Este cuaderno explica c√≥mo usar [Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) para almacenar incrustaciones vectoriales con la clase `MemorystoreVectorStore`.

M√°s informaci√≥n sobre el paquete en [GitHub](https://github.com/googleapis/langchain-google-memorystore-redis-python/).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googleapis/langchain-google-memorystore-redis-python/blob/main/docs/vector_store.ipynb)

## Requisitos previos

## Antes de comenzar

Para ejecutar este cuaderno, deber√° hacer lo siguiente:

* [Crear un proyecto de Google Cloud](https://developers.google.com/workspace/guides/create-project)
* [Habilitar la API de Memorystore for Redis](https://console.cloud.google.com/flows/enableapi?apiid=redis.googleapis.com)
* [Crear una instancia de Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis/create-instance-console). Aseg√∫rese de que la versi√≥n sea mayor o igual a 7.2.

### ü¶úüîó Instalaci√≥n de la biblioteca

La integraci√≥n se encuentra en su propio paquete `langchain-google-memorystore-redis`, por lo que debemos instalarlo.

```python
%pip install -upgrade --quiet langchain-google-memorystore-redis langchain
```

**Solo Colab:** Descomenta la siguiente celda para reiniciar el kernel o usa el bot√≥n para reiniciar el kernel. Para Vertex AI Workbench, puede reiniciar el terminal usando el bot√≥n de la parte superior.

```python
# # Automatically restart kernel after installs so that your environment can access the new packages
# import IPython

# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)
```

### ‚òÅ Establece tu proyecto de Google Cloud

Establece tu proyecto de Google Cloud para poder aprovechar los recursos de Google Cloud dentro de este cuaderno.

Si no sabes tu ID de proyecto, prueba lo siguiente:

* Ejecuta `gcloud config list`.
* Ejecuta `gcloud projects list`.
* Consulta la p√°gina de soporte: [Ubicar el ID del proyecto](https://support.google.com/googleapi/answer/7014113).

```python
# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.

PROJECT_ID = "my-project-id"  # @param {type:"string"}

# Set the project id
!gcloud config set project {PROJECT_ID}
```

### üîê Autenticaci√≥n

Autent√≠cate en Google Cloud como el usuario de IAM que ha iniciado sesi√≥n en este cuaderno para acceder a tu proyecto de Google Cloud.

* Si est√°s usando Colab para ejecutar este cuaderno, usa la celda a continuaci√≥n y contin√∫a.
* Si est√°s usando Vertex AI Workbench, consulta las instrucciones de configuraci√≥n [aqu√≠](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env).

```python
from google.colab import auth

auth.authenticate_user()
```

## Uso b√°sico

### Inicializar un √≠ndice vectorial

```python
import redis
from langchain_google_memorystore_redis import (
    DistanceStrategy,
    HNSWConfig,
    RedisVectorStore,
)

# Connect to a Memorystore for Redis instance
redis_client = redis.from_url("redis://127.0.0.1:6379")

# Configure HNSW index with descriptive parameters
index_config = HNSWConfig(
    name="my_vector_index", distance_strategy=DistanceStrategy.COSINE, vector_size=128
)

# Initialize/create the vector store index
RedisVectorStore.init_index(client=redis_client, index_config=index_config)
```

### Preparar documentos

El texto necesita procesamiento y representaci√≥n num√©rica antes de interactuar con un almac√©n vectorial. Esto implica:

* Cargar texto: El TextLoader obtiene datos de texto de un archivo (por ejemplo, "state_of_the_union.txt").
* Divisi√≥n de texto: El CharacterTextSplitter divide el texto en trozos m√°s peque√±os para los modelos de incrustaci√≥n.

```python
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

loader = TextLoader("./state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
```

### Agregar documentos al almac√©n vectorial

Despu√©s de la preparaci√≥n del texto y la generaci√≥n de incrustaciones, los siguientes m√©todos los insertan en el almac√©n vectorial de Redis.

#### M√©todo 1: M√©todo de clase para inserci√≥n directa

Este enfoque combina la creaci√≥n de incrustaciones y la inserci√≥n en un solo paso utilizando el m√©todo de clase `from_documents`:

```python
from langchain_community.embeddings.fake import FakeEmbeddings

embeddings = FakeEmbeddings(size=128)
redis_client = redis.from_url("redis://127.0.0.1:6379")
rvs = RedisVectorStore.from_documents(
    docs, embedding=embeddings, client=redis_client, index_name="my_vector_index"
)
```

#### M√©todo 2: Inserci√≥n basada en instancias

Este enfoque ofrece flexibilidad al trabajar con una nueva o existente RedisVectorStore:

* [Opcional] Crear una instancia de RedisVectorStore: Instancia un objeto RedisVectorStore para personalizaci√≥n. Si ya tienes una instancia, pasa al siguiente paso.
* Agregar texto con metadatos: Proporciona texto sin procesar y metadatos a la instancia. La generaci√≥n de incrustaciones y la inserci√≥n en el almac√©n vectorial se manejan autom√°ticamente.

```python
rvs = RedisVectorStore(
    client=redis_client, index_name="my_vector_index", embeddings=embeddings
)
ids = rvs.add_texts(
    texts=[d.page_content for d in docs], metadatas=[d.metadata for d in docs]
)
```

### Realizar una b√∫squeda de similitud (KNN)

Con el almac√©n vectorial poblado, es posible buscar texto sem√°nticamente similar a una consulta. As√≠ es como usar KNN (K-Nearest Neighbors) con la configuraci√≥n predeterminada:

* Formular la consulta: Una pregunta en lenguaje natural expresa la intenci√≥n de b√∫squeda (por ejemplo, "¬øQu√© dijo el presidente sobre Ketanji Brown Jackson?").
* Recuperar resultados similares: El m√©todo `similarity_search` encuentra los elementos del almac√©n vectorial m√°s cercanos a la consulta en significado.

```python
import pprint

query = "What did the president say about Ketanji Brown Jackson"
knn_results = rvs.similarity_search(query=query)
pprint.pprint(knn_results)
```

### Realizar una b√∫squeda de similitud basada en rango

Las consultas de rango proporcionan m√°s control al especificar un umbral de similitud deseado junto con el texto de la consulta:

* Formular la consulta: Una pregunta en lenguaje natural define la intenci√≥n de b√∫squeda.
* Establecer el umbral de similitud: El par√°metro `distance_threshold` determina qu√© tan cerca debe estar una coincidencia para ser considerada relevante.
* Recuperar resultados: El m√©todo `similarity_search_with_score` encuentra elementos del almac√©n vectorial que se encuentren dentro del umbral de similitud especificado.

```python
rq_results = rvs.similarity_search_with_score(query=query, distance_threshold=0.8)
pprint.pprint(rq_results)
```

### Realizar una b√∫squeda de Relevancia Marginal M√°xima (MMR)

Las consultas MMR tienen como objetivo encontrar resultados que sean relevantes para la consulta y diversos entre s√≠, reduciendo la redundancia en los resultados de b√∫squeda.

* Formular la consulta: Una pregunta en lenguaje natural define la intenci√≥n de b√∫squeda.
* Equilibrar relevancia y diversidad: El par√°metro `lambda_mult` controla el equilibrio entre la relevancia estricta y la promoci√≥n de la variedad en los resultados.
* Recuperar resultados de MMR: El m√©todo `max_marginal_relevance_search` devuelve elementos que optimizan la combinaci√≥n de relevancia y diversidad en funci√≥n de la configuraci√≥n de lambda.

```python
mmr_results = rvs.max_marginal_relevance_search(query=query, lambda_mult=0.90)
pprint.pprint(mmr_results)
```

## Usar el almac√©n vectorial como un Recuperador

Para una integraci√≥n fluida con otros componentes de LangChain, un almac√©n vectorial se puede convertir en un Recuperador. Esto ofrece varias ventajas:

* Compatibilidad con LangChain: Muchas herramientas y m√©todos de LangChain est√°n dise√±ados para interactuar directamente con los recuperadores.
* Facilidad de uso: El m√©todo `as_retriever()` convierte el almac√©n vectorial en un formato que simplifica la consulta.

```python
retriever = rvs.as_retriever()
results = retriever.invoke(query)
pprint.pprint(results)
```

## Limpieza

### Eliminar documentos del almac√©n vectorial

Ocasionalmente, es necesario eliminar documentos (y sus vectores asociados) del almac√©n vectorial. El m√©todo `delete` proporciona esta funcionalidad.

```python
rvs.delete(ids)
```

### Eliminar un √≠ndice de vector

Puede haber circunstancias en las que sea necesario eliminar un √≠ndice de vector existente. Los motivos comunes incluyen:

* Cambios en la configuraci√≥n del √≠ndice: si los par√°metros del √≠ndice necesitan modificaci√≥n, a menudo se requiere eliminar y volver a crear el √≠ndice.
* Gesti√≥n del almacenamiento: eliminar los √≠ndices no utilizados puede ayudar a liberar espacio dentro de la instancia de Redis.

Precauci√≥n: la eliminaci√≥n del √≠ndice de vector es una operaci√≥n irreversible. Aseg√∫rese de que los vectores almacenados y la funcionalidad de b√∫squeda ya no se requieran antes de proceder.

```python
# Delete the vector index
RedisVectorStore.drop_index(client=redis_client, index_name="my_vector_index")
```
