---
translated: true
---

# Seguridad de Layerup

La integración de [Seguridad de Layerup](https://uselayerup.com) le permite asegurar sus llamadas a cualquier LLM, cadena de LLM o agente de LLM de LangChain. El objeto LLM se envuelve alrededor de cualquier objeto LLM existente, permitiendo una capa de seguridad entre sus usuarios y sus LLM.

Si bien el objeto de Seguridad de Layerup está diseñado como un LLM, en realidad no es un LLM en sí mismo, simplemente envuelve a un LLM, permitiéndole adaptar la misma funcionalidad que el LLM subyacente.

## Configuración

Primero, necesitará una cuenta de Seguridad de Layerup desde el [sitio web](https://uselayerup.com) de Layerup.

A continuación, cree un proyecto a través del [panel de control](https://dashboard.uselayerup.com) y copie su clave API. Le recomendamos que coloque su clave API en el entorno de su proyecto.

Instale el SDK de Seguridad de Layerup:

```bash
pip install LayerupSecurity
```

E instale la Comunidad LangChain:

```bash
pip install langchain-community
```

¡Y ahora está listo para comenzar a proteger sus llamadas a LLM con Seguridad de Layerup!

```python
<!--IMPORTS:[{"imported": "LayerupSecurity", "source": "langchain_community.llms.layerup_security", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.layerup_security.LayerupSecurity.html", "title": "Layerup Security"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Layerup Security"}]-->
from langchain_community.llms.layerup_security import LayerupSecurity
from langchain_openai import OpenAI

# Create an instance of your favorite LLM
openai = OpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key="OPENAI_API_KEY",
)

# Configure Layerup Security
layerup_security = LayerupSecurity(
    # Specify a LLM that Layerup Security will wrap around
    llm=openai,

    # Layerup API key, from the Layerup dashboard
    layerup_api_key="LAYERUP_API_KEY",

    # Custom base URL, if self hosting
    layerup_api_base_url="https://api.uselayerup.com/v1",

    # List of guardrails to run on prompts before the LLM is invoked
    prompt_guardrails=[],

    # List of guardrails to run on responses from the LLM
    response_guardrails=["layerup.hallucination"],

    # Whether or not to mask the prompt for PII & sensitive data before it is sent to the LLM
    mask=False,

    # Metadata for abuse tracking, customer tracking, and scope tracking.
    metadata={"customer": "example@uselayerup.com"},

    # Handler for guardrail violations on the prompt guardrails
    handle_prompt_guardrail_violation=(
        lambda violation: {
            "role": "assistant",
            "content": (
                "There was sensitive data! I cannot respond. "
                "Here's a dynamic canned response. Current date: {}"
            ).format(datetime.now())
        }
        if violation["offending_guardrail"] == "layerup.sensitive_data"
        else None
    ),

    # Handler for guardrail violations on the response guardrails
    handle_response_guardrail_violation=(
        lambda violation: {
            "role": "assistant",
            "content": (
                "Custom canned response with dynamic data! "
                "The violation rule was {}."
            ).format(violation["offending_guardrail"])
        }
    ),
)

response = layerup_security.invoke(
    "Summarize this message: my name is Bob Dylan. My SSN is 123-45-6789."
)
```
