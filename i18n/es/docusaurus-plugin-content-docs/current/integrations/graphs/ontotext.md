---
translated: true
---

# Ontotext GraphDB

>[Ontotext GraphDB](https://graphdb.ontotext.com/) es una base de datos de grafos y una herramienta de descubrimiento de conocimiento que cumple con [RDF](https://www.w3.org/RDF/) y [SPARQL](https://www.w3.org/TR/sparql11-query/).

>Este cuaderno muestra c√≥mo usar LLM para proporcionar consultas en lenguaje natural (NLQ a SPARQL, tambi√©n llamado `text2sparql`) para `Ontotext GraphDB`.

## Funcionalidades de GraphDB LLM

`GraphDB` admite algunas funcionalidades de integraci√≥n de LLM como se describe [aqu√≠](https://github.com/w3c/sparql-dev/issues/193):

[gpt-queries](https://graphdb.ontotext.com/documentation/10.5/gpt-queries.html)

* predicados m√°gicos para preguntar a un LLM por texto, lista o tabla usando datos de tu grafo de conocimiento (KG)
* explicaci√≥n de consulta
* explicaci√≥n, resumen, reformulaci√≥n y traducci√≥n de resultados

[retrieval-graphdb-connector](https://graphdb.ontotext.com/documentation/10.5/retrieval-graphdb-connector.html)

* Indexaci√≥n de entidades de KG en una base de datos de vectores
* Admite cualquier algoritmo de incrustaci√≥n de texto y base de datos de vectores
* Utiliza el mismo lenguaje de conector potente (indexaci√≥n) que GraphDB usa para Elastic, Solr, Lucene
* Sincronizaci√≥n autom√°tica de cambios en datos RDF con el √≠ndice de entidades de KG
* Admite objetos anidados (sin soporte de UI en la versi√≥n 10.5 de GraphDB)
* Serializa las entidades de KG a texto como este (por ejemplo, para un conjunto de datos de Wines):

```text
Franvino:
- is a RedWine.
- made from grape Merlo.
- made from grape Cabernet Franc.
- has sugar dry.
- has year 2012.
```

[talk-to-graph](https://graphdb.ontotext.com/documentation/10.5/talk-to-graph.html)

* Un chatbot simple que usa un √≠ndice de entidades de KG definido

Para este tutorial, no usaremos la integraci√≥n de LLM de GraphDB, sino la generaci√≥n de `SPARQL` a partir de NLQ. Usaremos la ontolog√≠a y el conjunto de datos de la API de Star Wars (`SWAPI`) que puedes examinar [aqu√≠](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo/blob/main/starwars-data.trig).

## Configuraci√≥n

Necesitas una instancia de GraphDB en ejecuci√≥n. Este tutorial muestra c√≥mo ejecutar la base de datos localmente usando la [imagen de Docker de GraphDB](https://hub.docker.com/r/ontotext/graphdb). Proporciona un conjunto de composici√≥n de Docker, que llena GraphDB con el conjunto de datos de Star Wars. Todos los archivos necesarios, incluido este cuaderno, se pueden descargar del [repositorio de GitHub langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo).

* Instala [Docker](https://docs.docker.com/get-docker/). Este tutorial se cre√≥ usando la versi√≥n 24.0.7 de Docker, que incluye [Docker Compose](https://docs.docker.com/compose/). Para versiones anteriores de Docker, es posible que tengas que instalar Docker Compose por separado.
* Clona [el repositorio de GitHub langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo) en una carpeta local de tu m√°quina.
* Inicia GraphDB con el siguiente script ejecutado desde la misma carpeta

```bash
docker build --tag graphdb .
docker compose up -d graphdb
```

  Debes esperar unos segundos para que la base de datos se inicie en `http://localhost:7200/`. El conjunto de datos de Star Wars `starwars-data.trig` se carga autom√°ticamente en el repositorio `langchain`. El punto final SPARQL local `http://localhost:7200/repositories/langchain` se puede usar para ejecutar consultas. Tambi√©n puedes abrir el GraphDB Workbench desde tu navegador web favorito `http://localhost:7200/sparql` donde puedes hacer consultas de forma interactiva.
* Configura el entorno de trabajo

Si usas `conda`, crea y activa un nuevo entorno conda (por ejemplo, `conda create -n graph_ontotext_graphdb_qa python=3.9.18`).

Instala las siguientes bibliotecas:

```bash
pip install jupyter==1.0.0
pip install openai==1.6.1
pip install rdflib==7.0.0
pip install langchain-openai==0.0.2
pip install langchain>=0.1.5
```

Ejecuta Jupyter con

```bash
jupyter notebook
```

## Especificaci√≥n de la ontolog√≠a

Para que el LLM pueda generar SPARQL, necesita conocer el esquema del grafo de conocimiento (la ontolog√≠a). Se puede proporcionar utilizando uno de los dos par√°metros de la clase `OntotextGraphDBGraph`:

* `query_ontology`: una consulta `CONSTRUCT` que se ejecuta en el punto final SPARQL y devuelve las declaraciones del esquema del KG. Recomendamos que almacenes la ontolog√≠a en su propio grafo con nombre, lo que facilitar√° obtener solo las declaraciones relevantes (como en el ejemplo a continuaci√≥n). No se admiten consultas `DESCRIBE`, porque `DESCRIBE` devuelve la Descripci√≥n Acotada Concisa Sim√©trica (SCBD), es decir, tambi√©n los enlaces de clase entrantes. En el caso de gr√°ficos grandes con un mill√≥n de instancias, esto no es eficiente. Consulta https://github.com/eclipse-rdf4j/rdf4j/issues/4857
* `local_file`: un archivo de ontolog√≠a RDF local. Los formatos RDF admitidos son `Turtle`, `RDF/XML`, `JSON-LD`, `N-Triples`, `Notation-3`, `Trig`, `Trix`, `N-Quads`.

En cualquier caso, el volcado de ontolog√≠a debe:

* Incluir suficiente informaci√≥n sobre clases, propiedades, adjunci√≥n de propiedades a clases (usando rdfs:domain, schema:domainIncludes u OWL restrictions) y taxonom√≠as (individuos importantes).
* No incluir definiciones y ejemplos excesivamente verbosos e irrelevantes que no ayuden a la construcci√≥n de SPARQL.

```python
from langchain_community.graphs import OntotextGraphDBGraph

# feeding the schema using a user construct query

graph = OntotextGraphDBGraph(
    query_endpoint="http://localhost:7200/repositories/langchain",
    query_ontology="CONSTRUCT {?s ?p ?o} FROM <https://swapi.co/ontology/> WHERE {?s ?p ?o}",
)
```

```python
# feeding the schema using a local RDF file

graph = OntotextGraphDBGraph(
    query_endpoint="http://localhost:7200/repositories/langchain",
    local_file="/path/to/langchain_graphdb_tutorial/starwars-ontology.nt",  # change the path here
)
```

De cualquier manera, la ontolog√≠a (esquema) se alimenta al LLM como `Turtle` ya que `Turtle` con los prefijos apropiados es m√°s compacto y m√°s f√°cil de recordar para el LLM.

La ontolog√≠a de Star Wars es un poco inusual en que incluye muchos triples espec√≠ficos sobre clases, por ejemplo, que la especie `:Aleena` vive en `<planet/38>`, son una subclase de `:Reptile`, tienen ciertas caracter√≠sticas t√≠picas (altura promedio, esperanza de vida promedio, color de piel) y los individuos espec√≠ficos (personajes) son representantes de esa clase:

```output
@prefix : <https://swapi.co/vocabulary/> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

:Aleena a owl:Class, :Species ;
    rdfs:label "Aleena" ;
    rdfs:isDefinedBy <https://swapi.co/ontology/> ;
    rdfs:subClassOf :Reptile, :Sentient ;
    :averageHeight 80.0 ;
    :averageLifespan "79" ;
    :character <https://swapi.co/resource/aleena/47> ;
    :film <https://swapi.co/resource/film/4> ;
    :language "Aleena" ;
    :planet <https://swapi.co/resource/planet/38> ;
    :skinColor "blue", "gray" .

    ...

```

Para mantener este tutorial simple, usamos GraphDB sin seguridad. Si GraphDB est√° asegurado, debes establecer las variables de entorno 'GRAPHDB_USERNAME' y 'GRAPHDB_PASSWORD' antes de la inicializaci√≥n de `OntotextGraphDBGraph`.

```python
os.environ["GRAPHDB_USERNAME"] = "graphdb-user"
os.environ["GRAPHDB_PASSWORD"] = "graphdb-password"

graph = OntotextGraphDBGraph(
    query_endpoint=...,
    query_ontology=...
)
```

## Respuesta a preguntas contra el conjunto de datos de Star Wars

Ahora podemos usar `OntotextGraphDBQAChain` para hacer algunas preguntas.

```python
import os

from langchain.chains import OntotextGraphDBQAChain
from langchain_openai import ChatOpenAI

# We'll be using an OpenAI model which requires an OpenAI API Key.
# However, other models are available as well:
# https://python.langchain.com/docs/integrations/chat/

# Set the environment variable `OPENAI_API_KEY` to your OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-***"

# Any available OpenAI model can be used here.
# We use 'gpt-4-1106-preview' because of the bigger context window.
# The 'gpt-4-1106-preview' model_name will deprecate in the future and will change to 'gpt-4-turbo' or similar,
# so be sure to consult with the OpenAI API https://platform.openai.com/docs/models for the correct naming.

chain = OntotextGraphDBQAChain.from_llm(
    ChatOpenAI(temperature=0, model_name="gpt-4-1106-preview"),
    graph=graph,
    verbose=True,
)
```

Hagamos una simple.

```python
chain.invoke({chain.input_key: "What is the climate on Tatooine?"})[chain.output_key]
```

```output


[1m> Entering new OntotextGraphDBQAChain chain...[0m
Generated SPARQL:
[32;1m[1;3mPREFIX : <https://swapi.co/vocabulary/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?climate
WHERE {
  ?planet rdfs:label "Tatooine" ;
          :climate ?climate .
}[0m

[1m> Finished chain.[0m
```

```output
'The climate on Tatooine is arid.'
```

Y una un poco m√°s complicada.

```python
chain.invoke({chain.input_key: "What is the climate on Luke Skywalker's home planet?"})[
    chain.output_key
]
```

```output


[1m> Entering new OntotextGraphDBQAChain chain...[0m
Generated SPARQL:
[32;1m[1;3mPREFIX : <https://swapi.co/vocabulary/>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?climate
WHERE {
  ?character rdfs:label "Luke Skywalker" .
  ?character :homeworld ?planet .
  ?planet :climate ?climate .
}[0m

[1m> Finished chain.[0m
```

```output
"The climate on Luke Skywalker's home planet is arid."
```

Tambi√©n podemos hacer preguntas m√°s complicadas como

```python
chain.invoke(
    {
        chain.input_key: "What is the average box office revenue for all the Star Wars movies?"
    }
)[chain.output_key]
```

```output


[1m> Entering new OntotextGraphDBQAChain chain...[0m
Generated SPARQL:
[32;1m[1;3mPREFIX : <https://swapi.co/vocabulary/>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

SELECT (AVG(?boxOffice) AS ?averageBoxOffice)
WHERE {
  ?film a :Film .
  ?film :boxOffice ?boxOfficeValue .
  BIND(xsd:decimal(?boxOfficeValue) AS ?boxOffice)
}
[0m

[1m> Finished chain.[0m
```

```output
'The average box office revenue for all the Star Wars movies is approximately 754.1 million dollars.'
```

## Modificadores de cadena

La cadena de QA de Ontotext GraphDB permite el refinamiento de indicaciones para mejorar a√∫n m√°s su cadena de QA y mejorar la experiencia general del usuario de su aplicaci√≥n.

### Indicaci√≥n de "Generaci√≥n de SPARQL"

La indicaci√≥n se usa para la generaci√≥n de consultas SPARQL en funci√≥n de la pregunta del usuario y el esquema del KG.

- `sparql_generation_prompt`

    Valor predeterminado:
  ````python
    GRAPHDB_SPARQL_GENERATION_TEMPLATE = """
    Escribe una consulta SPARQL SELECT para consultar una base de datos de gr√°ficos.
    El esquema de ontolog√≠a delimitado por tres comillas inversas en formato Turtle es:
    ```
    {schema}
    ```
    Usa solo las clases y propiedades proporcionadas en el esquema para construir la consulta SPARQL.
    No uses ninguna clase o propiedad que no se proporcione expl√≠citamente en la consulta SPARQL.
    Incluye todos los prefijos necesarios.
    No incluyas explicaciones ni disculpas en tus respuestas.
    No envuelvas la consulta en comillas inversas.
    No incluyas ning√∫n texto excepto la consulta SPARQL generada.
    La pregunta delimitada por tres comillas inversas es:
    ```
    {prompt}
    ```
    """
    GRAPHDB_SPARQL_GENERATION_PROMPT = PromptTemplate(
        input_variables=["schema", "prompt"],
        template=GRAPHDB_SPARQL_GENERATION_TEMPLATE,
    )
  ````

### Indicaci√≥n de "Correcci√≥n de SPARQL"

A veces, el LLM puede generar una consulta SPARQL con errores sint√°cticos o que le faltan prefijos, etc. La cadena intentar√° enmendar esto solicitando al LLM que la corrija un cierto n√∫mero de veces.

- `sparql_fix_prompt`

    Valor predeterminado:
  ````python
    GRAPHDB_SPARQL_FIX_TEMPLATE = """
    Esta siguiente consulta SPARQL delimitada por tres comillas inversas
    ```
    {generated_sparql}
    ```
    no es v√°lida.
    El error delimitado por tres comillas inversas es
    ```
    {error_message}
    ```
    Dame una versi√≥n correcta de la consulta SPARQL.
    No cambies la l√≥gica de la consulta.
    No incluyas explicaciones ni disculpas en tus respuestas.
    No envuelvas la consulta en comillas inversas.
    No incluyas ning√∫n texto excepto la consulta SPARQL generada.
    El esquema de ontolog√≠a delimitado por tres comillas inversas en formato Turtle es:
    ```
    {schema}
    ```
    """

    GRAPHDB_SPARQL_FIX_PROMPT = PromptTemplate(
        input_variables=["error_message", "generated_sparql", "schema"],
        template=GRAPHDB_SPARQL_FIX_TEMPLATE,
    )
  ````

- `max_fix_retries`

    Valor predeterminado: `5`

### "Responder" a la indicaci√≥n

La indicaci√≥n se usa para responder a la pregunta en funci√≥n de los resultados devueltos desde la base de datos y la pregunta inicial del usuario. De forma predeterminada, se instruye al LLM para que solo use la informaci√≥n de los resultados devueltos. Si el conjunto de resultados est√° vac√≠o, el LLM debe informar de que no puede responder a la pregunta.

- `qa_prompt`

  Valor predeterminado:
  ````python
    GRAPHDB_QA_TEMPLATE = """Tarea: Generar una respuesta en lenguaje natural a partir de los resultados de una consulta SPARQL.
    Eres un asistente que crea respuestas bien escritas y comprensibles para los humanos.
    La parte de informaci√≥n contiene la informaci√≥n proporcionada, que puedes usar para construir una respuesta.
    La informaci√≥n proporcionada es autoritativa, nunca debes dudarla ni intentar usar tu conocimiento interno para corregirla.
    Haz que tu respuesta suene como si la informaci√≥n proviniera de un asistente de IA, pero no agregues ninguna informaci√≥n.
    No uses conocimiento interno para responder a la pregunta, simplemente di que no sabes si no hay informaci√≥n disponible.
    Informaci√≥n:
    {context}

    Pregunta: {prompt}
    Respuesta √∫til:"""
    GRAPHDB_QA_PROMPT = PromptTemplate(
        input_variables=["context", "prompt"], template=GRAPHDB_QA_TEMPLATE
    )
  ````

Una vez que hayas terminado de jugar con QA con GraphDB, puedes apagar el entorno Docker ejecutando
``
docker compose down -v --remove-orphans
``
desde el directorio con el archivo Docker compose.
