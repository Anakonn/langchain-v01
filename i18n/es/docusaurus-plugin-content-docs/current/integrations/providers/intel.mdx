---
translated: true
---

# Intel

>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel) es la interfaz entre las bibliotecas  Transformers y Diffusers y las diferentes herramientas y bibliotecas proporcionadas por Intel para acelerar las tuber铆as de extremo a extremo en arquitecturas Intel.

>[Intel庐 Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX) es un kit de herramientas innovador dise帽ado para acelerar GenAI/LLM en todas partes con el rendimiento 贸ptimo de los modelos basados en Transformer en varias plataformas Intel, incluidos Intel Gaudi2, Intel CPU e Intel GPU.

Esta p谩gina cubre c贸mo usar optimum-intel e ITREX con LangChain.

## Optimum-intel

Toda la funcionalidad relacionada con [optimum-intel](https://github.com/huggingface/optimum-intel.git) e [IPEX](https://github.com/intel/intel-extension-for-pytorch).

### Instalaci贸n

Instalar usando optimum-intel e ipex:

```bash
pip install optimum[neural-compressor]
pip install intel_extension_for_pytorch
```

Sigue las instrucciones de instalaci贸n que se especifican a continuaci贸n:

* Instalar optimum-intel como se muestra [aqu铆](https://github.com/huggingface/optimum-intel).
* Instalar IPEX como se muestra [aqu铆](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu).

### Modelos de incrustaci贸n

Consulta un [ejemplo de uso](/docs/integrations/text_embedding/optimum_intel).
Tambi茅n ofrecemos un cuaderno tutorial completo "rag_with_quantized_embeddings.ipynb" para usar el incrustador en una tuber铆a RAG en el directorio de recetas.

```python
<!--IMPORTS:[{"imported": "QuantizedBiEncoderEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.optimum_intel.QuantizedBiEncoderEmbeddings.html", "title": "Intel"}]-->
from langchain_community.embeddings import QuantizedBiEncoderEmbeddings
```

## Intel庐 Extension for Transformers (ITREX)

(ITREX) es un kit de herramientas innovador para acelerar los modelos basados en Transformer en plataformas Intel, en particular, efectivo en el procesador Intel Xeon Scalable de 4.a generaci贸n Sapphire Rapids (con el c贸digo Sapphire Rapids).

La cuantizaci贸n es un proceso que implica reducir la precisi贸n de estos pesos represent谩ndolos usando una cantidad menor de bits. La cuantizaci贸n solo de pesos se enfoca espec铆ficamente en cuantizar los pesos de la red neuronal mientras mantiene otros componentes, como las activaciones, en su precisi贸n original.

A medida que los modelos de lenguaje grandes (LLM) se vuelven m谩s prevalentes, existe una necesidad creciente de nuevos y mejorados m茅todos de cuantizaci贸n que puedan satisfacer las demandas computacionales de estas arquitecturas modernas mientras mantienen la precisi贸n. En comparaci贸n con la [cuantizaci贸n normal](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md) como W8A8, la cuantizaci贸n solo de pesos probablemente sea un mejor equilibrio para equilibrar el rendimiento y la precisi贸n, ya que veremos a continuaci贸n que el cuello de botella de implementar LLM es el ancho de banda de memoria y normalmente la cuantizaci贸n solo de pesos podr铆a conducir a una mejor precisi贸n.

Aqu铆, presentaremos los Modelos de incrustaci贸n y la cuantizaci贸n solo de pesos para los modelos de lenguaje grandes de Transformers con ITREX. La cuantizaci贸n solo de pesos es una t茅cnica utilizada en el aprendizaje profundo para reducir los requisitos de memoria y c贸mputo de las redes neuronales. En el contexto de las redes neuronales profundas, los par谩metros del modelo, tambi茅n conocidos como pesos, se representan t铆picamente usando n煤meros de punto flotante, lo que puede consumir una cantidad significativa de memoria y requerir recursos computacionales intensivos.

Toda la funcionalidad relacionada con [intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers).

### Instalaci贸n

Instalar intel-extension-for-transformers. Para los requisitos del sistema y otros consejos de instalaci贸n, consulta la [Gu铆a de instalaci贸n](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)

```bash
pip install intel-extension-for-transformers
```

Instalar otros paquetes requeridos.

```bash
pip install -U torch onnx accelerate datasets
```

### Modelos de incrustaci贸n

Consulta un [ejemplo de uso](/docs/integrations/text_embedding/itrex).

```python
<!--IMPORTS:[{"imported": "QuantizedBgeEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.itrex.QuantizedBgeEmbeddings.html", "title": "Intel"}]-->
from langchain_community.embeddings import QuantizedBgeEmbeddings
```

### Cuantizaci贸n solo de pesos con ITREX

Consulta un [ejemplo de uso](/docs/integrations/llms/weight_only_quantization).

## Detalles de los par谩metros de configuraci贸n

Aqu铆 est谩 el detalle de la clase `WeightOnlyQuantConfig`.

#### weight_dtype (cadena): Tipo de datos de peso, el valor predeterminado es "nf4".

Admitimos cuantificar los pesos a los siguientes tipos de datos para almacenar (weight_dtype en WeightOnlyQuantConfig):
* **int8**: Usa el tipo de datos de 8 bits.
* **int4_fullrange**: Usa el valor -8 del rango int4 en comparaci贸n con el rango int4 normal [-7,7].
* **int4_clip**: Recorta y retiene los valores dentro del rango int4, estableciendo otros en cero.
* **nf4**: Usa el tipo de datos de punto flotante normalizado de 4 bits.
* **fp4_e2m1**: Usa el tipo de datos de punto flotante regular de 4 bits. "e2" significa que se usan 2 bits para el exponente y "m1" significa que se usa 1 bit para la mantisa.

#### compute_dtype (cadena): Tipo de datos de c谩lculo, el valor predeterminado es "fp32".

Mientras que estas t茅cnicas almacenan los pesos en 4 u 8 bits, el c谩lculo a煤n se realiza en float32, bfloat16 o int8 (compute_dtype en WeightOnlyQuantConfig):
* **fp32**: Usa el tipo de datos float32 para calcular.
* **bf16**: Usa el tipo de datos bfloat16 para calcular.
* **int8**: Usa el tipo de datos de 8 bits para calcular.

#### llm_int8_skip_modules (lista de nombres de m贸dulos): M贸dulos para omitir la cuantizaci贸n, el valor predeterminado es None.

Es una lista de m贸dulos que se omitir谩n de la cuantizaci贸n.

#### scale_dtype (cadena): El tipo de datos de escala, el valor predeterminado es "fp32".

Ahora solo se admite "fp32" (float32).

#### mse_range (booleano): Si se debe buscar el mejor rango de recorte del rango [0.805, 1.0, 0.005], el valor predeterminado es False.

#### use_double_quant (booleano): Si se debe cuantificar la escala, el valor predeterminado es False.

A煤n no se admite.

#### double_quant_dtype (cadena): Reservado para la doble cuantizaci贸n.

#### double_quant_scale_dtype (cadena): Reservado para la doble cuantizaci贸n.

#### group_size (int): Tama帽o del grupo al cuantizar.

#### scheme (cadena): Qu茅 formato se utilizar谩 para cuantizar los pesos. El valor predeterminado es "sym".

* **sym**: Sim茅trico.
* **asym**: Asim茅trico.

#### algorithm (string): Qu茅 algoritmo mejorar la precisi贸n. El valor predeterminado es "RTN"

* **RTN**: Round-to-nearest (RTN) es un m茅todo de cuantificaci贸n que podemos pensar de manera muy intuitiva.
* **AWQ**: Proteger solo el 1% de los pesos salientes puede reducir en gran medida el error de cuantizaci贸n. los canales de pesos salientes se seleccionan observando la distribuci贸n de activaci贸n y peso por canal. Los pesos salientes tambi茅n se cuantizan despu茅s de multiplicar un factor de escala grande antes de la cuantizaci贸n para preservar.
* **TEQ**: Una transformaci贸n equivalente entrenable que preserva la precisi贸n FP32 en la cuantizaci贸n de pesos 煤nicamente.
