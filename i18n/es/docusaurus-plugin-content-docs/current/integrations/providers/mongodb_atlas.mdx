---
translated: true
---

# MongoDB Atlas

>[MongoDB Atlas](https://www.mongodb.com/docs/atlas/) es una base de datos en la nube totalmente administrada disponible en AWS, Azure y GCP. Ahora tiene soporte para Búsqueda de Vectores nativa en los datos de documentos de MongoDB.

## Instalación y configuración

Consulta las [instrucciones de configuración detalladas](/docs/integrations/vectorstores/mongodb_atlas).

Necesitamos instalar el paquete de Python `langchain-mongodb`.

```bash
pip install langchain-mongodb
```

## Vector Store

Consulta un [ejemplo de uso](/docs/integrations/vectorstores/mongodb_atlas).

```python
<!--IMPORTS:[{"imported": "MongoDBAtlasVectorSearch", "source": "langchain_mongodb", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain_mongodb.vectorstores.MongoDBAtlasVectorSearch.html", "title": "MongoDB Atlas"}]-->
from langchain_mongodb import MongoDBAtlasVectorSearch
```

## Cachés de LLM

### MongoDBCache

Una abstracción para almacenar una caché simple en MongoDB. Esto no usa Semantic Caching, ni requiere que se cree un índice en la colección antes de la generación.

Para importar esta caché:

```python
<!--IMPORTS:[{"imported": "MongoDBCache", "source": "langchain_mongodb.cache", "docs": "https://api.python.langchain.com/en/latest/cache/langchain_mongodb.cache.MongoDBCache.html", "title": "MongoDB Atlas"}]-->
from langchain_mongodb.cache import MongoDBCache
```

Para usar esta caché con tus LLM:

```python
<!--IMPORTS:[{"imported": "set_llm_cache", "source": "langchain_core.globals", "docs": "https://api.python.langchain.com/en/latest/globals/langchain_core.globals.set_llm_cache.html", "title": "MongoDB Atlas"}]-->
from langchain_core.globals import set_llm_cache

# use any embedding provider...
from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings

mongodb_atlas_uri = "<YOUR_CONNECTION_STRING>"
COLLECTION_NAME="<YOUR_CACHE_COLLECTION_NAME>"
DATABASE_NAME="<YOUR_DATABASE_NAME>"

set_llm_cache(MongoDBCache(
    connection_string=mongodb_atlas_uri,
    collection_name=COLLECTION_NAME,
    database_name=DATABASE_NAME,
))
```

### MongoDBAtlasSemanticCache

El Semantic Caching permite a los usuarios recuperar consultas almacenadas en caché en función de la similitud semántica entre la entrada del usuario y los resultados almacenados previamente. Bajo el capó, combina MongoDBAtlas tanto como caché como como vector store.
MongoDBAtlasSemanticCache hereda de `MongoDBAtlasVectorSearch` y necesita un índice de búsqueda de vectores de Atlas definido para funcionar. Por favor, consulta el [ejemplo de uso](/docs/integrations/vectorstores/mongodb_atlas) sobre cómo configurar el índice.

Para importar esta caché:

```python
<!--IMPORTS:[{"imported": "MongoDBAtlasSemanticCache", "source": "langchain_mongodb.cache", "docs": "https://api.python.langchain.com/en/latest/cache/langchain_mongodb.cache.MongoDBAtlasSemanticCache.html", "title": "MongoDB Atlas"}]-->
from langchain_mongodb.cache import MongoDBAtlasSemanticCache
```

Para usar esta caché con tus LLM:

```python
<!--IMPORTS:[{"imported": "set_llm_cache", "source": "langchain_core.globals", "docs": "https://api.python.langchain.com/en/latest/globals/langchain_core.globals.set_llm_cache.html", "title": "MongoDB Atlas"}]-->
from langchain_core.globals import set_llm_cache

# use any embedding provider...
from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings

mongodb_atlas_uri = "<YOUR_CONNECTION_STRING>"
COLLECTION_NAME="<YOUR_CACHE_COLLECTION_NAME>"
DATABASE_NAME="<YOUR_DATABASE_NAME>"

set_llm_cache(MongoDBAtlasSemanticCache(
    embedding=FakeEmbeddings(),
    connection_string=mongodb_atlas_uri,
    collection_name=COLLECTION_NAME,
    database_name=DATABASE_NAME,
))
```

``
