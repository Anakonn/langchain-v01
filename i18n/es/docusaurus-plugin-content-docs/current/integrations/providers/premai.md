---
translated: true
---

# PremAI

>[PremAI](https://app.premai.io) es una plataforma unificada que le permite construir aplicaciones listas para la producci칩n y potentes impulsadas por GenAI con el menor esfuerzo, para que pueda centrarse m치s en la experiencia del usuario y el crecimiento general.

## ChatPremAI

Este ejemplo explica c칩mo usar LangChain para interactuar con diferentes modelos de chat con `ChatPremAI`

### Instalaci칩n y configuraci칩n

Comenzamos instalando langchain y premai-sdk. Puede escribir el siguiente comando para instalar:

```bash
pip install premai langchain
```

Antes de continuar, aseg칰rese de haber creado una cuenta en PremAI y ya haber iniciado un proyecto. Si no es as칤, aqu칤 est치 c칩mo puede comenzar de forma gratuita:

1. Inicie sesi칩n en [PremAI](https://app.premai.io/accounts/login/), si es la primera vez que viene, y cree su clave API [aqu칤](https://app.premai.io/api_keys/).

2. Vaya a [app.premai.io](https://app.premai.io) y esto lo llevar치 al panel de control del proyecto.

3. Cree un proyecto y esto generar치 un ID de proyecto (escrito como ID). Este ID lo ayudar치 a interactuar con su aplicaci칩n implementada.

4. Dir칤jase a LaunchPad (el que tiene el icono 游). Y all칤 implementa el modelo de tu elecci칩n. Tu modelo predeterminado ser치 `gpt-4`. Tambi칠n puede establecer y fijar diferentes par치metros de generaci칩n (como max-tokens, temperatura, etc.) y tambi칠n preestablecer tu sistema de mensajes.

Felicitaciones por crear tu primera aplicaci칩n implementada en PremAI 游꿀 Ahora podemos usar langchain para interactuar con nuestra aplicaci칩n.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "PremAI"}, {"imported": "SystemMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html", "title": "PremAI"}, {"imported": "ChatPremAI", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.premai.ChatPremAI.html", "title": "PremAI"}]-->
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.chat_models import ChatPremAI
```

### Configurar la instancia de ChatPrem en LangChain

Una vez que importemos nuestros m칩dulos requeridos, configuremos nuestro cliente. Por ahora, supongamos que nuestro `project_id` es 8. Pero aseg칰rate de usar tu ID de proyecto, de lo contrario, arrojar치 un error.

Para usar langchain con prem, no necesitas pasar ning칰n nombre de modelo ni establecer ning칰n par치metro con nuestro cliente de chat. Todos ellos usar치n el nombre del modelo predeterminado y los par치metros del modelo de LaunchPad.

`NOTA:` Si cambia el `model_name` o cualquier otro par치metro como `temperature` al configurar el cliente, anular치 las configuraciones predeterminadas existentes.

```python
import os
import getpass

if "PREMAI_API_KEY" not in os.environ:
    os.environ["PREMAI_API_KEY"] = getpass.getpass("PremAI API Key:")

chat = ChatPremAI(project_id=8)
```

### Llamando al modelo

Ahora est치s listo. Ahora podemos comenzar a interactuar con nuestra aplicaci칩n. `ChatPremAI` admite dos m칠todos `invoke` (que es lo mismo que `generate`) y `stream`.

El primero nos dar치 un resultado est치tico. Mientras que el segundo transmitir치 tokens uno por uno. As칤 es como puede generar completaciones similares a un chat.

### Generaci칩n

```python
human_message = HumanMessage(content="Who are you?")

chat.invoke([human_message])
```

쯃o anterior se ve interesante, verdad? Establec칤 mi sistema de mensajes predeterminado de LaunchPad como: `Siempre suena como un pirata` Tambi칠n puede anular el sistema de mensajes predeterminado si lo necesita. As칤 es como puede hacerlo.

```python
system_message = SystemMessage(content="You are a friendly assistant.")
human_message = HumanMessage(content="Who are you?")

chat.invoke([system_message, human_message])
```

Tambi칠n puede cambiar los par치metros de generaci칩n al llamar al modelo. As칤 es como puede hacer eso:

```python
chat.invoke(
    [system_message, human_message],
    temperature = 0.7, max_tokens = 20, top_p = 0.95
)
```

### Notas importantes:

Antes de continuar, tenga en cuenta que la versi칩n actual de ChatPrem no admite los par치metros: [n](https://platform.openai.com/docs/api-reference/chat/create#chat-create-n) y [stop](https://platform.openai.com/docs/api-reference/chat/create#chat-create-stop) no son compatibles.

Proporcionaremos soporte para esos dos par치metros anteriores en versiones posteriores.

### Transmisi칩n

Y finalmente, as칤 es como se hace la transmisi칩n de tokens para aplicaciones din치micas similares a un chat.

```python
import sys

for chunk in chat.stream("hello how are you"):
    sys.stdout.write(chunk.content)
    sys.stdout.flush()
```

Similar a lo anterior, si desea anular el sistema de mensajes y los par치metros de generaci칩n, as칤 es como puede hacerlo.

```python
import sys

for chunk in chat.stream(
    "hello how are you",
    system_prompt = "You are an helpful assistant", temperature = 0.7, max_tokens = 20
):
    sys.stdout.write(chunk.content)
    sys.stdout.flush()
```

## Incrustaci칩n

En esta secci칩n, vamos a discutir c칩mo podemos acceder a diferentes modelos de incrustaci칩n usando `PremEmbeddings`. Comencemos haciendo algunas importaciones y definiendo nuestro objeto de incrustaci칩n

```python
from langchain_community.embeddings import PremEmbeddings
```

Una vez que importemos nuestros m칩dulos requeridos, configuremos nuestro cliente. Por ahora, supongamos que nuestro `project_id` es 8. Pero aseg칰rate de usar tu ID de proyecto, de lo contrario, arrojar치 un error.

```python

import os
import getpass

if os.environ.get("PREMAI_API_KEY") is None:
    os.environ["PREMAI_API_KEY"] = getpass.getpass("PremAI API Key:")

# Define a model as a required parameter here since there is no default embedding model

model = "text-embedding-3-large"
embedder = PremEmbeddings(project_id=8, model=model)
```

Hemos definido nuestro modelo de incrustaci칩n. Admitimos muchos modelos de incrustaci칩n. Aqu칤 hay una tabla que muestra la cantidad de modelos de incrustaci칩n que admitimos.

| Proveedor   | Slug                                     | Tokens de contexto |
|-------------|------------------------------------------|-------------------|
| cohere      | embed-english-v3.0                       | N/A               |
| openai      | text-embedding-3-small                   | 8191              |
| openai      | text-embedding-3-large                   | 8191              |
| openai      | text-embedding-ada-002                   | 8191              |
| replicate   | replicate/all-mpnet-base-v2              | N/A               |
| together    | togethercomputer/Llama-2-7B-32K-Instruct | N/A               |
| mistralai   | mistral-embed                            | 4096              |

Para cambiar el modelo, simplemente necesita copiar el `slug` y acceder a su modelo de incrustaci칩n. Ahora comencemos a usar nuestro modelo de incrustaci칩n con una sola consulta seguida de m칰ltiples consultas (que tambi칠n se llama documento)

```python
query = "Hello, this is a test query"
query_result = embedder.embed_query(query)

# Let's print the first five elements of the query embedding vector

print(query_result[:5])
```

Finalmente, incrustemos un documento

```python
documents = [
    "This is document1",
    "This is document2",
    "This is document3"
]

doc_result = embedder.embed_documents(documents)

# Similar to the previous result, let's print the first five element
# of the first document vector

print(doc_result[0][:5])
```
