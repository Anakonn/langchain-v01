---
sidebar_class_name: hidden
sidebar_custom_props:
  description: Interfaz con modelos de lenguaje
sidebar_position: 0
translated: true
---

# Entrada/Salida del Modelo

El elemento central de cualquier aplicación de modelo de lenguaje es... el modelo. LangChain le brinda los bloques de construcción para interactuar con cualquier modelo de lenguaje.

![Diagrama de flujo que ilustra el proceso de Entrada/Salida del Modelo con los pasos Formato, Predicción y Análisis, mostrando la transformación de las variables de entrada a la salida estructurada.](/img/model_io.jpg "Diagrama del Proceso de Entrada/Salida del Modelo")

# Inicio Rápido

El siguiente inicio rápido cubrirá los conceptos básicos del uso de los componentes de Entrada/Salida del Modelo de LangChain. Presentará los dos tipos diferentes de modelos: LLM y Modelos de Chat. Luego cubrirá cómo usar Plantillas de Indicaciones para dar formato a las entradas de estos modelos y cómo usar Analizadores de Salida para trabajar con las salidas.

Los modelos de lenguaje en LangChain se presentan en dos variantes:

### [Modelos de Chat](/docs/modules/model_io/chat/)

[Los modelos de chat](/docs/modules/model_io/chat/) a menudo se basan en LLM, pero se han ajustado específicamente para mantener conversaciones.
Crucialmente, sus API de proveedores utilizan una interfaz diferente a los modelos de finalización de texto puro. En lugar de una sola cadena,
toman una lista de mensajes de chat como entrada y devuelven un mensaje de IA como salida. Consulte la sección a continuación para obtener más detalles sobre lo que exactamente consiste un mensaje. GPT-4 y Claude-2 de Anthropic se implementan como modelos de chat.

### [LLMs](/docs/modules/model_io/llms/)

[LLMs](/docs/modules/model_io/llms/) en LangChain se refieren a modelos de finalización de texto puro.
Las API que envuelven toman una cadena de texto como entrada y devuelven una cadena de finalización. GPT-3 de OpenAI se implementa como un LLM.

Estos dos tipos de API tienen diferentes esquemas de entrada y salida.

Además, no todos los modelos son iguales. Diferentes modelos tienen diferentes estrategias de solicitud que funcionan mejor para ellos. Por ejemplo, los modelos de Anthropic funcionan mejor con XML, mientras que los de OpenAI funcionan mejor con JSON. Debes tener esto en cuenta al diseñar tus aplicaciones.

Para esta guía de inicio, usaremos modelos de chat y proporcionaremos algunas opciones: usar una API como Anthropic u OpenAI, o usar un modelo de código abierto local a través de Ollama.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="openai" label="OpenAI" default>

Primero necesitaremos instalar su paquete asociado:

```shell
pip install langchain-openai
```

Acceder a la API requiere una clave API, que puedes obtener creando una cuenta y dirigiéndote [aquí](https://platform.openai.com/account/api-keys). Una vez que tengamos una clave, la estableceremos como una variable de entorno ejecutando:

```shell
export OPENAI_API_KEY="..."
```

Luego podemos inicializar el modelo:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Model I/O"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Model I/O"}]-->
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI

llm = OpenAI()
chat_model = ChatOpenAI(model="gpt-3.5-turbo-0125")
```

Si prefieres no establecer una variable de entorno, puedes pasar la clave directamente a través del parámetro con nombre `api_key` al iniciar la clase LLM de OpenAI:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Model I/O"}]-->
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(api_key="...")
```

Tanto `llm` como `chat_model` son objetos que representan la configuración de un modelo en particular.
Puedes inicializarlos con parámetros como `temperature` y otros, y pasarlos.
La principal diferencia entre ellos es su esquema de entrada y salida.
Los objetos LLM toman una cadena como entrada y devuelven una cadena.
Los objetos ChatModel toman una lista de mensajes como entrada y devuelven un mensaje.

Podemos ver la diferencia entre un LLM y un ChatModel cuando lo invocamos.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Model I/O"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

El LLM devuelve una cadena, mientras que el ChatModel devuelve un mensaje.

  </TabItem>
  <TabItem value="local" label="Local (usando Ollama)">

[Ollama](https://ollama.ai/) permite ejecutar modelos de lenguaje grande de código abierto, como Llama 2, de forma local.

Primero, sigue [estas instrucciones](https://github.com/jmorganca/ollama) para configurar y ejecutar una instancia local de Ollama:

* [Descargar](https://ollama.ai/download)
* Obtener un modelo a través de `ollama pull llama2`

Luego, asegúrate de que el servidor Ollama se esté ejecutando. Después de eso, puedes hacer:

```python
<!--IMPORTS:[{"imported": "Ollama", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html", "title": "Model I/O"}, {"imported": "ChatOllama", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html", "title": "Model I/O"}]-->
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

llm = Ollama(model="llama2")
chat_model = ChatOllama()
```

Tanto `llm` como `chat_model` son objetos que representan la configuración de un modelo en particular.
Puedes inicializarlos con parámetros como `temperature` y otros, y pasarlos.
La principal diferencia entre ellos es su esquema de entrada y salida.
Los objetos LLM toman una cadena como entrada y devuelven una cadena.
Los objetos ChatModel toman una lista de mensajes como entrada y devuelven un mensaje.

Podemos ver la diferencia entre un LLM y un ChatModel cuando lo invocamos.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Model I/O"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

El LLM devuelve una cadena, mientras que el ChatModel devuelve un mensaje.

  </TabItem>
  <TabItem value="anthropic" label="Anthropic (solo modelo de chat)">

Primero necesitaremos importar el paquete LangChain x Anthropic.

```shell
pip install langchain-anthropic
```

Acceder a la API requiere una clave API, que puedes obtener creando una cuenta [aquí](https://claude.ai/login). Una vez que tengamos una clave, la estableceremos como una variable de entorno ejecutando:

```shell
export ANTHROPIC_API_KEY="..."
```

Luego podemos inicializar el modelo:

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Model I/O"}]-->
from langchain_anthropic import ChatAnthropic

chat_model = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0.2, max_tokens=1024)
```

Si prefieres no establecer una variable de entorno, puedes pasar la clave directamente a través del parámetro con nombre `api_key` al iniciar la clase Anthropic Chat Model:

```python
chat_model = ChatAnthropic(api_key="...")
```

  </TabItem>
  <TabItem value="cohere" label="Cohere (solo modelo de chat)">

Primero necesitaremos instalar su paquete asociado:

```shell
pip install langchain-cohere
```

Acceder a la API requiere una clave API, que puedes obtener creando una cuenta y dirigiéndote [aquí](https://dashboard.cohere.com/api-keys). Una vez que tengamos una clave, la estableceremos como una variable de entorno ejecutando:

```shell
export COHERE_API_KEY="..."
```

Luego podemos inicializar el modelo:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Model I/O"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere()
```

Si prefieres no establecer una variable de entorno, puedes pasar la clave directamente a través del parámetro con nombre `cohere_api_key` al iniciar la clase Cohere LLM:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Model I/O"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere(cohere_api_key="...")
```

  </TabItem>
</Tabs>

## Plantillas de solicitud

La mayoría de las aplicaciones de LLM no pasan la entrada del usuario directamente a un LLM. Por lo general, agregarán la entrada del usuario a una pieza de texto más grande, llamada plantilla de solicitud, que proporciona contexto adicional sobre la tarea específica en cuestión.

En el ejemplo anterior, el texto que le pasamos al modelo contenía instrucciones para generar un nombre de empresa. Para nuestra aplicación, sería genial si el usuario solo tuviera que proporcionar la descripción de una empresa/producto sin preocuparse por dar instrucciones al modelo.

¡Las PromptTemplates ayudan exactamente con esto!
¡Agrupan toda la lógica para pasar de la entrada del usuario a un aviso completamente formateado!
Esto puede comenzar de manera muy simple: por ejemplo, un aviso para producir la cadena anterior sería:

```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Model I/O"}]-->
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```python
What is a good name for a company that makes colorful socks?
```

Hay varias ventajas de usar estos en lugar del formato de cadena sin procesar.
Puede "parcializar" variables, es decir, puede formatear solo algunas de las variables a la vez.
Puede componerlos juntos, combinando fácilmente diferentes plantillas en un solo aviso.
Para obtener explicaciones de estas funcionalidades, consulte la [sección sobre avisos](/docs/modules/model_io/prompts) para obtener más detalles.

Los `PromptTemplate`s también se pueden usar para producir una lista de mensajes.
En este caso, el aviso no solo contiene información sobre el contenido, sino también sobre cada mensaje (su rol, su posición en la lista, etc.).
Aquí, lo que sucede con más frecuencia es que un `ChatPromptTemplate` es una lista de `ChatMessageTemplates`.
Cada `ChatMessageTemplate` contiene instrucciones sobre cómo formatear ese `ChatMessage`: su rol y luego también su contenido.
Echemos un vistazo a esto a continuación:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts.chat", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Model I/O"}]-->
from langchain_core.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```pycon
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

Los ChatPromptTemplates también se pueden construir de otras maneras; consulte la [sección sobre avisos](/docs/modules/model_io/prompts) para obtener más detalles.

## Analizadores de salida

Los `OutputParser`s convierten la salida sin procesar de un modelo de lenguaje en un formato que se puede usar posteriormente.
Hay algunos tipos principales de `OutputParser`s, que incluyen:

- Convertir texto de `LLM` en información estructurada (por ejemplo, JSON)
- Convertir un `ChatMessage` en solo una cadena
- Convertir la información adicional devuelta de una llamada además del mensaje (como la invocación de la función de OpenAI) en una cadena.

Para obtener información completa sobre esto, consulte la [sección sobre analizadores de salida](/docs/modules/model_io/output_parsers).

En esta guía de inicio, usamos uno simple que analiza una lista de valores separados por comas.

```python
<!--IMPORTS:[{"imported": "CommaSeparatedListOutputParser", "source": "langchain.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html", "title": "Model I/O"}]-->
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
output_parser.parse("hi, bye")
# >> ['hi', 'bye']
```

## Composición con LCEL

Ahora podemos combinar todo esto en una sola cadena.
Esta cadena tomará variables de entrada, pasará esas variables a una plantilla de aviso para crear un aviso, pasará el aviso a un modelo de lenguaje y luego pasará la salida a través de un analizador de salida (opcional).
Esta es una forma conveniente de agrupar una pieza de lógica modular.
¡Veámoslo en acción!

```python
template = "Generate a list of 5 {text}.\n\n{format_instructions}"

chat_prompt = ChatPromptTemplate.from_template(template)
chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())
chain = chat_prompt | chat_model | output_parser
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

Tenga en cuenta que estamos usando la sintaxis `|` para unir estos componentes.
Esta sintaxis `|` está impulsada por el Lenguaje de Expresión LangChain (LCEL) y se basa en la interfaz `Runnable` universal que implementan todos estos objetos.
Para obtener más información sobre LCEL, lea la documentación [aquí](/docs/expression_language).

## Conclusión

¡Eso es todo para comenzar con avisos, modelos y analizadores de salida! Esto solo cubrió la superficie de lo que hay que aprender. Para obtener más información, consulte:

- La [sección de avisos](./prompts) para obtener información sobre cómo trabajar con plantillas de aviso
- La [sección de ChatModel](./chat) para obtener más información sobre la interfaz ChatModel
- La [sección de LLM](./llms) para obtener más información sobre la interfaz LLM
- La [sección de analizador de salida](./output_parsers) para obtener información sobre los diferentes tipos de analizadores de salida.
