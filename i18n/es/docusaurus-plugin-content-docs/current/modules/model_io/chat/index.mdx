---
sidebar_class_name: hidden
sidebar_position: 3
translated: true
---

# Modelos de chat

Los modelos de chat son un componente fundamental de LangChain.

Un modelo de chat es un modelo de lenguaje que utiliza mensajes de chat como entradas y devuelve mensajes de chat como salidas (a diferencia de utilizar texto plano).

LangChain tiene integraciones con muchos proveedores de modelos (OpenAI, Cohere, Hugging Face, etc.) y expone una interfaz estándar para interactuar con todos estos modelos.

LangChain le permite utilizar modelos en modos sincrónico, asincrónico, por lotes y de transmisión, y proporciona otras funciones (por ejemplo, almacenamiento en caché) y más.

## [Inicio rápido](./quick_start)

Consulte [este inicio rápido](./quick_start) para obtener una descripción general del trabajo con ChatModels, incluidos todos los diferentes métodos que exponen.

## [Integraciones](/docs/integrations/chat/)

Para obtener una lista completa de todas las integraciones de LLM que proporciona LangChain, visite la [página de integraciones](/docs/integrations/chat/).

## Guías prácticas

Tenemos varias guías prácticas para un uso más avanzado de los LLM.
Esto incluye:

- [Cómo almacenar en caché las respuestas de ChatModel](./chat_model_caching)
- [Cómo usar ChatModels que admiten la llamada a funciones](./function_calling)
- [Cómo transmitir respuestas desde un ChatModel](./streaming)
- [Cómo realizar un seguimiento del uso de tokens en una llamada a ChatModel](./token_usage_tracking)
- [Cómo crear un ChatModel personalizado](./custom_chat_model)
