---
translated: true
---

# Privacidad y seguridad

Una de las principales preocupaciones con el uso de LLM es que pueden hacer un uso indebido de datos privados o generar texto dañino o poco ético. Esta es un área de investigación activa en el campo. Aquí presentamos algunas cadenas incorporadas inspiradas en esta investigación, que tienen la intención de hacer que las salidas de los LLM sean más seguras.

- [Cadena de moderación de Amazon Comprehend](/docs/guides/productionization/safety/amazon_comprehend_chain): Use [Amazon Comprehend](https://aws.amazon.com/comprehend/) para detectar y manejar información de identificación personal (PII) y toxicidad.
- [Cadena constitucional](/docs/guides/productionization/safety/constitutional_chain): Solicite al modelo un conjunto de principios que deberían guiar el comportamiento del modelo.
- [Identificación de inyección de indicaciones de Hugging Face](/docs/guides/productionization/safety/hugging_face_prompt_injection): Detectar y manejar ataques de inyección de indicaciones.
- [Seguridad de Layerup](/docs/guides/productionization/safety/layerup_security): Enmascarar fácilmente PII y datos confidenciales, detectar y mitigar más de 10 vectores de amenaza basados en LLM, incluidos PII y datos confidenciales, inyección de indicaciones, alucinación, abuso y más.
- [Cadena de falacias lógicas](/docs/guides/productionization/safety/logical_fallacy_chain): Verifica la salida del modelo contra falacias lógicas para corregir cualquier desviación.
- [Cadena de moderación](/docs/guides/productionization/safety/moderation): Comprueba si algún texto de salida es dañino y lo marca.
- [Anonimización de datos de Presidio](/docs/guides/productionization/safety/presidio_data_anonymization): Ayuda a garantizar que los datos confidenciales se gestionen y gobiernen adecuadamente.
