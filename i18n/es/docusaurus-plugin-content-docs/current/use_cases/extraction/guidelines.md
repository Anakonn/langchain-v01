---
sidebar_position: 5
title: Pautas
translated: true
---

La calidad de los resultados de extracciÃ³n depende de muchos factores.

AquÃ­ hay un conjunto de pautas para ayudarte a obtener el mejor rendimiento de tus modelos:

* Establece la temperatura del modelo en `0`.
* Mejora el prompt. El prompt debe ser preciso y conciso.
* Documenta el esquema: AsegÃºrate de que el esquema estÃ© documentado para proporcionar mÃ¡s informaciÃ³n al LLM.
* Â¡Proporciona ejemplos de referencia! Los ejemplos diversos pueden ayudar, incluyendo ejemplos donde no se debe extraer nada.
* Si tienes muchos ejemplos, usa un buscador para recuperar los ejemplos mÃ¡s relevantes.
* Realiza pruebas de referencia con el mejor LLM/Chat Model disponible (por ejemplo, gpt-4, claude-3, etc.) -- Â¡consulta con el proveedor del modelo cuÃ¡l es el mÃ¡s reciente y mejor!
* Si el esquema es muy grande, intenta dividirlo en mÃºltiples esquemas mÃ¡s pequeÃ±os, ejecuta extracciones separadas y combina los resultados.
* Â¡AsegÃºrate de que el esquema permita que el modelo RECHACE la extracciÃ³n de informaciÃ³n! Si no lo hace, el modelo se verÃ¡ obligado a inventar informaciÃ³n.
* Agrega pasos de verificaciÃ³n/correcciÃ³n (pide a un LLM que corrija o verifique los resultados de la extracciÃ³n).

## Prueba de referencia

* Crea y realiza pruebas de referencia de datos para tu caso de uso utilizando [LangSmith ğŸ¦œï¸ğŸ› ï¸](https://docs.smith.langchain.com/).
* Â¿Tu LLM es lo suficientemente bueno? Usa [langchain-benchmarks ğŸ¦œğŸ’¯ ](https://github.com/langchain-ai/langchain-benchmarks) para probar tu LLM utilizando conjuntos de datos existentes.

## Â¡TÃ©n en cuenta! ğŸ˜¶â€ğŸŒ«ï¸

* Los LLM son excelentes, pero no son necesarios para todos los casos. Si estÃ¡s extrayendo informaciÃ³n de una sola fuente estructurada (por ejemplo, LinkedIn), usar un LLM no es una buena idea; el web-scraping tradicional serÃ¡ mucho mÃ¡s barato y confiable.

* **ser humano en el bucle** Si necesitas **calidad perfecta**, probablemente tendrÃ¡s que planificar tener un ser humano en el bucle, incluso los mejores LLM cometerÃ¡n errores al tratar con tareas de extracciÃ³n complejas.
