---
sidebar_position: 2
translated: true
---

# Paso atrás en la consulta

A veces, la calidad de la búsqueda y las generaciones del modelo pueden verse afectadas por los detalles de una pregunta. Una forma de manejar esto es generar primero una pregunta más abstracta y "dar un paso atrás" y consultar en función de la pregunta original y la pregunta de paso atrás.

Por ejemplo, si hacemos una pregunta del tipo "¿Por qué mi agente LangGraph astream_events devuelve {LONG_TRACE} en lugar de {DESIRED_OUTPUT}", probablemente recuperaremos documentos más relevantes si buscamos con la pregunta más genérica "Cómo funciona astream_events con un agente LangGraph" que si buscamos con la pregunta específica del usuario.

Veamos cómo podríamos usar el paso atrás en la consulta en el contexto de nuestro bot de preguntas y respuestas sobre los videos de LangChain YouTube.

## Configuración

#### Instalar dependencias

```python
# %pip install -qU langchain-core langchain-openai
```

#### Establecer variables de entorno

Usaremos OpenAI en este ejemplo:

```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

## Generación de preguntas de paso atrás

Generar buenas preguntas de paso atrás se reduce a escribir una buena indicación:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

system = """You are an expert at taking a specific question and extracting a more generic question that gets at \
the underlying principles needed to answer the specific question.

You will be asked about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.

LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.
LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.
LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.
LangSmith is a platform that makes it easy to trace and test LLM applications.

Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question. \

If you don't recognize a word or acronym to not try to rewrite it.

Write concise questions."""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
step_back = prompt | llm | StrOutputParser()
```

```python
question = (
    "I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search. "
    "How do I get just the LLM calls from the event stream"
)
result = step_back.invoke({"question": question})
print(result)
```

```output
What are the specific methods or functions provided by LangGraph for extracting LLM calls from an event stream that includes various types of interactions and data sources?
```

## Devolver la pregunta de paso atrás y la pregunta original

Para aumentar nuestro recuerdo, probablemente querramos recuperar documentos en función de la pregunta de paso atrás y la pregunta original. Podemos devolver fácilmente ambas de la siguiente manera:

```python
from langchain_core.runnables import RunnablePassthrough

step_back_and_original = RunnablePassthrough.assign(step_back=step_back)

step_back_and_original.invoke({"question": question})
```

```output
{'question': 'I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search. How do I get just the LLM calls from the event stream',
 'step_back': 'What are the specific methods or functions provided by LangGraph for extracting LLM calls from an event stream generated by an agent built using external tools like Gemini Pro, vectorstores, and DuckDuckGo search?'}
```

## Usar la llamada a función para obtener una salida estructurada

Si estuviéramos componiendo esta técnica con otros métodos de análisis de consultas, probablemente estaríamos usando la llamada a función para obtener objetos de consulta estructurados. Podemos usar la llamada a función para el paso atrás en la consulta de la siguiente manera:

```python
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.pydantic_v1 import BaseModel, Field


class StepBackQuery(BaseModel):
    step_back_question: str = Field(
        ...,
        description="Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question.",
    )


llm_with_tools = llm.bind_tools([StepBackQuery])
hyde_chain = prompt | llm_with_tools | PydanticToolsParser(tools=[StepBackQuery])
hyde_chain.invoke({"question": question})
```

```output
[StepBackQuery(step_back_question='What are the steps to filter and extract specific types of calls from an event stream in a Python framework like LangGraph?')]
```
