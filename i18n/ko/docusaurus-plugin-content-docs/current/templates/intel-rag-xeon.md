---
translated: true
---

# Intel Xeonì—ì„œì˜ RAG ì˜ˆì œ

ì´ í…œí”Œë¦¿ì€ IntelÂ® XeonÂ® Scalable Processorsì—ì„œ Chromaì™€ Text Generation Inferenceë¥¼ ì‚¬ìš©í•˜ì—¬ RAGë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
IntelÂ® XeonÂ® Scalable í”„ë¡œì„¸ì„œëŠ” ë” ë§ì€ ì„±ëŠ¥ ë‹¹ ì½”ì–´ì™€ ë¹„êµí•  ìˆ˜ ì—†ëŠ” AI ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” ë‚´ì¥ ê°€ì†ê¸°ë¥¼ íŠ¹ì§•ìœ¼ë¡œ í•˜ë©°, ê°€ì¥ ë§ì€ í´ë¼ìš°ë“œ ì„ íƒê³¼ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ì‹ì„±ì„ ì œê³µí•˜ëŠ” ê°€ì¥ ê¹Œë‹¤ë¡œìš´ ì›Œí¬ë¡œë“œ ìš”êµ¬ ì‚¬í•­ì„ ìœ„í•œ ê³ ê¸‰ ë³´ì•ˆ ê¸°ìˆ ì„ ì œê³µí•©ë‹ˆë‹¤. [IntelÂ® XeonÂ® Scalable Processors](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

## í™˜ê²½ ì„¤ì •

IntelÂ® XeonÂ® Scalable Processorsì—ì„œ [ğŸ¤— text-generation-inference](https://github.com/huggingface/text-generation-inference)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

### Intel Xeon ì„œë²„ì—ì„œ ë¡œì»¬ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘:

```bash
model=Intel/neural-chat-7b-v3-3
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4 --model-id $model
```

`LLAMA-2`ì™€ ê°™ì€ ê²Œì´íŠ¸ ëª¨ë¸ì˜ ê²½ìš° ìœ„ì˜ docker run ëª…ë ¹ì— -e HUGGING_FACE_HUB_TOKEN=\<token\>ì„ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

[huggingface token](https://huggingface.co/docs/hub/security-tokens) ë§í¬ë¥¼ ë”°ë¼ ì•¡ì„¸ìŠ¤ í† í°ì„ ì–»ê³  `HUGGINGFACEHUB_API_TOKEN` í™˜ê²½ ë³€ìˆ˜ì— í† í°ì„ ë‚´ë³´ë‚´ì„¸ìš”.

```bash
export HUGGINGFACEHUB_API_TOKEN=<token>
```

ì—”ë“œí¬ì¸íŠ¸ê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ìš”ì²­ì„ ë³´ë‚´ì„¸ìš”:

```bash
curl localhost:8080/generate -X POST -d '{"inputs":"Which NFL team won the Super Bowl in the 2010 season?","parameters":{"max_new_tokens":128, "do_sample": true}}'   -H 'Content-Type: application/json'
```

ìì„¸í•œ ë‚´ìš©ì€ [text-generation-inference](https://github.com/huggingface/text-generation-inference)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ë°ì´í„° ì±„ìš°ê¸°

ì˜ˆì œ ë°ì´í„°ë¥¼ DBì— ì±„ìš°ë ¤ë©´ ì•„ë˜ ëª…ë ¹ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```shell
poetry install
poetry run python ingest.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Nike `nke-10k-2023.pdf`ì˜ Edgar 10k ì œì¶œ ë°ì´í„°ì—ì„œ ì„¹ì…˜ì„ ì²˜ë¦¬í•˜ê³  Chroma ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

## ì‚¬ìš©ë²•

ì´ íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € LangChain CLIë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

```shell
pip install -U langchain-cli
```

ìƒˆ LangChain í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³  ì´ê²ƒì„ ìœ ì¼í•œ íŒ¨í‚¤ì§€ë¡œ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```shell
langchain app new my-app --package intel-rag-xeon
```

ê¸°ì¡´ í”„ë¡œì íŠ¸ì— ì¶”ê°€í•˜ë ¤ë©´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤:

```shell
langchain app add intel-rag-xeon
```

ê·¸ë¦¬ê³  `server.py` íŒŒì¼ì— ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”:

```python
from intel_rag_xeon import chain as xeon_rag_chain

add_routes(app, xeon_rag_chain, path="/intel-rag-xeon")
```

(ì„ íƒ ì‚¬í•­) ì´ì œ LangSmithë¥¼ êµ¬ì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤. LangSmithëŠ” LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¶”ì , ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. [ì—¬ê¸°](https://smith.langchain.com/)ì—ì„œ LangSmithì— ê°€ì…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ì—†ëŠ” ê²½ìš° ì´ ì„¹ì…˜ì„ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>
export LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to "default"
```

ì´ ë””ë ‰í† ë¦¬ ë‚´ì— ìˆë‹¤ë©´ ë‹¤ìŒì„ í†µí•´ LangServe ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§ì ‘ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```shell
langchain serve
```

ì´ë ‡ê²Œ í•˜ë©´ FastAPI ì•±ì´ ì‹œì‘ë˜ë©° ë¡œì»¬ì—ì„œ [http://localhost:8000](http://localhost:8000)ì—ì„œ ì„œë²„ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.

[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)ì—ì„œ ëª¨ë“  í…œí”Œë¦¿ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
[http://127.0.0.1:8000/intel-rag-xeon/playground](http://127.0.0.1:8000/intel-rag-xeon/playground)ì—ì„œ playgroundì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì½”ë“œì—ì„œ í…œí”Œë¦¿ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from langserve.client import RemoteRunnable

runnable = RemoteRunnable("http://localhost:8000/intel-rag-xeon")
```
