---
translated: true
---

# 배포

오늘날 빠르게 발전하는 기술 환경에서 대형 언어 모델(LLM)의 사용이 급격히 확산되고 있습니다. 따라서 개발자들이 이러한 모델을 프로덕션 환경에 효과적으로 배포하는 방법을 이해하는 것이 중요합니다. LLM 인터페이스는 일반적으로 두 가지 범주로 나뉩니다:

- **사례 1: 외부 LLM 제공업체(OpenAI, Anthropic 등) 활용**
    이 시나리오에서는 대부분의 계산 부담을 LLM 제공업체가 처리하며, LangChain은 이러한 서비스 주변의 비즈니스 로직 구현을 간소화합니다. 이 접근 방식에는 프롬프트 템플릿, 채팅 메시지 생성, 캐싱, 벡터 임베딩 데이터베이스 생성, 전처리 등과 같은 기능이 포함됩니다.

- **사례 2: 자체 호스팅 오픈 소스 모델**
    대안으로, 개발자들은 더 작지만 비교적 유능한 자체 호스팅 오픈 소스 LLM 모델을 사용할 수 있습니다. 이 접근 방식은 외부 LLM 제공업체로 데이터를 전송할 때 발생하는 비용, 대기 시간 및 개인정보 문제를 크게 줄일 수 있습니다.

당신의 제품의 백본을 구성하는 프레임워크와 상관없이, LLM 애플리케이션을 배포하는 것은 자체적인 도전 과제를 수반합니다. 서비스 프레임워크를 평가할 때의 트레이드오프와 주요 고려사항을 이해하는 것이 중요합니다.

## 개요

이 가이드는 프로덕션 환경에서 LLM을 배포하기 위한 요구 사항에 대한 종합적인 개요를 제공하는 것을 목표로 하며, 다음에 중점을 둡니다:

- **견고한 LLM 애플리케이션 서비스 설계**
- **비용 효율성 유지**
- **빠른 반복 보장**

이 구성 요소들을 이해하는 것은 서비스 시스템을 평가할 때 중요합니다. LangChain은 이러한 문제를 해결하기 위해 설계된 여러 오픈 소스 프로젝트와 통합되어, LLM 애플리케이션을 프로덕션화하는 데 견고한 프레임워크를 제공합니다. 몇 가지 주목할만한 프레임워크는 다음과 같습니다:

- [Ray Serve](/docs/integrations/providers/ray_serve)
- [BentoML](https://github.com/bentoml/BentoML)
- [OpenLLM](/docs/integrations/providers/openllm)
- [Modal](/docs/integrations/providers/modal)
- [Jina](/docs/integrations/providers/jina)

이 링크들은 각 생태계에 대한 추가 정보를 제공하며, 당신의 LLM 배포 요구 사항에 가장 적합한 솔루션을 찾는 데 도움이 될 것입니다.

## 견고한 LLM 애플리케이션 서비스 설계

프로덕션에서 LLM 서비스를 배포할 때는 중단 없는 원활한 사용자 경험을 제공하는 것이 중요합니다. 24/7 서비스 가용성을 달성하기 위해 애플리케이션 주변에 여러 하위 시스템을 구축하고 유지해야 합니다.

### 모니터링

모니터링은 프로덕션 환경에서 실행되는 시스템의 필수적인 부분을 형성합니다. LLM의 경우 성능 및 품질 지표를 모니터링하는 것이 중요합니다.

**성능 지표:** 이러한 지표는 모델의 효율성과 용량에 대한 통찰력을 제공합니다. 주요 예는 다음과 같습니다:

- 초당 쿼리(QPS): 초당 모델이 처리하는 쿼리 수를 측정하여 모델의 활용도를 나타냅니다.
- 대기 시간: 클라이언트가 요청을 보낸 시점부터 응답을 받을 때까지의 지연 시간을 정량화합니다.
- 초당 토큰(TPS): 모델이 초당 생성할 수 있는 토큰 수를 나타냅니다.

**품질 지표:** 이러한 지표는 일반적으로 비즈니스 사용 사례에 맞게 사용자 정의됩니다. 예를 들어, 시스템의 출력이 이전 버전과 같은 기준에 어떻게 비교되는지를 평가할 수 있습니다. 이러한 지표는 오프라인으로 계산될 수 있지만, 나중에 사용할 수 있도록 필요한 데이터를 로그로 남겨야 합니다.

### 장애 내성

애플리케이션은 모델 추론 또는 비즈니스 로직 코드에서 예외와 같은 오류를 만나 교통 흐름을 방해할 수 있습니다. 예상치 못한 하드웨어 고장이나 높은 수요 기간 동안의 스팟 인스턴스 손실 등 애플리케이션을 실행하는 기계에서 발생할 수 있는 다른 문제들도 있습니다. 이러한 위험을 완화하는 한 가지 방법은 복제 확장을 통해 중복성을 높이고, 실패한 복제본에 대한 복구 메커니즘을 구현하는 것입니다. 그러나 모델 복제본만이 유일한 실패 지점이 아닙니다. 스택의 어느 지점에서나 발생할 수 있는 다양한 실패에 대해 내성을 구축하는 것이 중요합니다.

### 무중단 업그레이드

시스템 업그레이드는 종종 필요하지만, 올바르게 처리되지 않으면 서비스 중단을 초래할 수 있습니다. 업그레이드 중 다운타임을 방지하는 한 가지 방법은 구버전에서 신버전으로의 원활한 전환 프로세스를 구현하는 것입니다. 이상적으로는 새로운 버전의 LLM 서비스가 배포되고, 트래픽이 구버전에서 신버전으로 점진적으로 이동하여 전체 프로세스 동안 일정한 QPS를 유지합니다.

### 로드 밸런싱

로드 밸런싱은 여러 컴퓨터, 서버 또는 기타 리소스에 작업을 균등하게 분산하여 시스템의 활용도를 최적화하고, 처리량을 최대화하며, 응답 시간을 최소화하고, 단일 리소스의 과부하를 방지하는 기술입니다. 이는 마치 교통 경찰관이 차량(요청)을 다른 도로(서버)로 안내하여 어떤 도로도 너무 혼잡해지지 않도록 하는 것과 같습니다.

로드 밸런싱에는 여러 가지 전략이 있습니다. 예를 들어, 가장 일반적인 방법 중 하나는 *라운드 로빈* 전략으로, 각 요청을 순서대로 다음 서버로 보내고, 모든 서버가 요청을 받은 후 첫 번째 서버로 돌아가는 방식입니다. 이 방법은 모든 서버가 동일한 능력을 가질 때 잘 작동합니다. 그러나 일부 서버가 다른 서버보다 더 강력한 경우, *가중 라운드 로빈* 또는 *최소 연결* 전략을 사용할 수 있으며, 더 많은 요청을 더 강력한 서버 또는 현재 가장 적은 활성 요청을 처리 중인 서버로 보냅니다. LLM 체인을 실행하는 경우를 상상해보세요. 애플리케이션이 인기를 끌면 수백 명 또는 수천 명의 사용자가 동시에 질문할 수 있습니다. 한 서버가 너무 바빠지면(높은 부하), 로드 밸런서는 새로운 요청을 덜 바쁜 다른 서버로 보낼 것입니다. 이렇게 하면 모든 사용자가 적시에 응답을 받을 수 있고 시스템이 안정적으로 유지됩니다.

## 비용 효율성 및 확장성 유지

LLM 서비스를 배포하는 것은 비용이 많이 들 수 있으며, 특히 많은 사용자 상호작용을 처리할 때 더욱 그렇습니다. LLM 제공업체는 일반적으로 사용된 토큰 수를 기준으로 요금을 부과하므로 이러한 모델에 대한 채팅 시스템 추론이 잠재적으로 비용이 많이 들 수 있습니다. 그러나 서비스 품질을 저하시키지 않으면서 이러한 비용을 관리할 수 있는 여러 전략이 있습니다.

### 자체 호스팅 모델

LLM 제공업체에 대한 의존 문제를 해결하기 위해 여러 작은 오픈 소스 LLM이 등장하고 있습니다. 자체 호스팅은 LLM 제공업체 모델과 유사한 품질을 유지하면서 비용을 관리할 수 있게 해줍니다. 과제는 자체 기계에서 신뢰할 수 있고 고성능의 LLM 서빙 시스템을 구축하는 데 있습니다.

### 자원 관리 및 자동 확장

애플리케이션 내의 계산 로직은 정확한 자원 할당이 필요합니다. 예를 들어, 일부 트래픽은 OpenAI 엔드포인트에서 서비스되고, 다른 일부는 자체 호스팅 모델에서 서비스될 때, 각 부분에 적합한 자원을 할당하는 것이 중요합니다. 트래픽을 기반으로 자원 할당을 조정하는 자동 확장(auto-scaling)은 애플리케이션 운영 비용에 크게 영향을 미칠 수 있습니다. 이 전략은 비용과 응답성 사이의 균형을 요구하며, 자원 과다 할당이나 응답성 저하를 방지해야 합니다.

### 스팟 인스턴스 활용

AWS와 같은 플랫폼에서 스팟 인스턴스는 상당한 비용 절감을 제공하며, 일반적으로 주문형 인스턴스의 약 1/3 가격입니다. 그러나 더 높은 충돌률이 발생할 수 있으므로 효과적으로 사용하려면 강력한 장애 내성 메커니즘이 필요합니다.

### 독립적 확장

모델을 자체 호스팅할 때 독립적인 확장을 고려해야 합니다. 예를 들어, 프랑스어로 세밀하게 조정된 모델과 스페인어로 세밀하게 조정된 모델이 두 개 있다면, 들어오는 요청에 따라 각각 다른 확장 요구 사항이 있을 수 있습니다.

### 요청 일괄 처리

대형 언어 모델의 경우 요청을 일괄 처리하면 GPU 자원을 더 잘 활용하여 효율성을 높일 수 있습니다. GPU는 본질적으로 병렬 프로세서로, 여러 작업을 동시에 처리하도록 설계되었습니다. 개별 요청을 모델에 보내면 GPU가 한 번에 단일 작업만 처리하여 완전히 활용되지 않을 수 있습니다. 반면에 요청을 일괄 처리하여 GPU가 여러 작업을 동시에 처리할 수 있도록 하면, GPU 활용도가 최대화되고 추론 속도가 향상됩니다. 이는 비용 절감뿐만 아니라 전체 LLM 서비스의 대기 시간을 개선할 수도 있습니다.

요약하자면, LLM 서비스를 확장하면서 비용을 관리하려면 전략적인 접근이 필요합니다. 자체 호스팅 모델을 활용하고 자원을 효과적으로 관리하며 자동 확장을 사용하고 스팟 인스턴스를 활용하며 모델을 독립적으로 확장하고 요청을 일괄 처리하는 것이 고려해야 할 주요 전략입니다. Ray Serve와 BentoML과 같은 오픈 소스 라이브러리는 이러한 복잡성을 처리하도록 설계되었습니다.

## 빠른 반복 보장

LLM 환경은 전례 없는 속도로 진화하고 있으며, 새로운 라이브러리와 모델 아키텍처가 지속적으로 도입되고 있습니다. 따라서 특정 프레임워크에 얽매이지 않는 솔루션을 추구하는 것이 중요합니다. 이는 특히 인프라 변경이 시간과 비용이 많이 들고 위험할 수 있는 서빙(Serving)에서 더욱 관련이 있습니다. 특정 기계 학습 라이브러리나 프레임워크에 잠기지 않고, 일반 목적의 확장 가능한 서빙 레이어를 제공하는 인프라를 구축하는 것이 좋습니다. 여기에서 유연성이 중요한 역할을 하는 몇 가지 측면을 설명하겠습니다:

### 모델 구성

LangChain과 같은 시스템을 배포할 때는 서로 다른 모델을 결합하고 이를 논리를 통해 연결할 수 있는 능력이 필요합니다. 자연어 입력 SQL 쿼리 엔진을 구축하는 예를 들어 보겠습니다. LLM에 쿼리하고 SQL 명령을 얻는 것은 시스템의 일부일 뿐입니다. 연결된 데이터베이스에서 메타데이터를 추출하고, LLM에 대한 프롬프트를 구성하고, 엔진에서 SQL 쿼리를 실행하고, 실행 중인 쿼리에 대한 응답을 수집하여 LLM에 피드백을 제공하고, 결과를 사용자에게 표시해야 합니다. 이는 파이썬으로 작성된 다양한 복잡한 구성 요소를 동적으로 논리 블록으로 결합하여 함께 서빙할 수 있어야 함을 보여줍니다.

## 클라우드 제공업체

많은 호스팅 솔루션은 단일 클라우드 제공업체에 제한되어 있으며, 이는 오늘날의 멀티 클라우드 환경에서 옵션을 제한할 수 있습니다. 다른 인프라 구성 요소가 어디에 구축되어 있는지에 따라, 선택한 클라우드 제공업체를 고수하고 싶을 수 있습니다.

## 코드로서의 인프라(IaC)

빠른 반복에는 인프라를 빠르고 신뢰할 수 있게 재구성할 수 있는 능력이 포함됩니다. 이때 코드로서의 인프라(IaC) 도구인 Terraform, CloudFormation 또는 Kubernetes YAML 파일이 도움이 됩니다. 이러한 도구들은 인프라를 코드 파일로 정의할 수 있게 해주며, 이는 버전 관리가 가능하고 빠르고 신뢰할 수 있게 배포할 수 있어 더 빠르고 신뢰할 수 있는 반복을 가능하게 합니다.

## CI/CD

빠르게 변화하는 환경에서 CI/CD 파이프라인을 구현하면 반복 프로세스를 크게 가속화할 수 있습니다. 이러한 파이프라인은 LLM 애플리케이션의 테스트와 배포를 자동화하여 오류의 위험을 줄이고 더 빠른 피드백과 반복을 가능하게 합니다.