---
translated: true
---

# 논리적 오류 체인

이 예시는 모델 출력에서 논리적 오류를 제거하는 방법을 보여줍니다.

## 논리적 오류

`논리적 오류`는 모델 출력의 타당성을 훼손할 수 있는 잘못된 추론이나 거짓 논증입니다.

예로는 순환 논증, 거짓 딜레마, 인신 공격 등이 있습니다. 머신 러닝 모델은 정확도, 혼란도, 손실과 같은 특정 지표에서 잘 수행하도록 최적화됩니다. 그러나 지표를 최적화하는 것만으로는 논리적으로 타당한 추론을 보장할 수 없습니다.

언어 모델은 그럴듯하게 들리지만 논리적으로 잘못된 주장을 생성하기 위해 추론의 결함을 이용하는 법을 배울 수 있습니다. 모델이 오류에 의존할 때, 비록 높은 점수를 얻더라도 그 출력은 신뢰할 수 없고 신뢰할 수 없습니다. 사용자는 이러한 출력을 의존할 수 없습니다. 논리적 오류를 전파하면 오해를 퍼뜨리고 사용자를 혼란스럽게 하며, 제품이나 서비스에 모델을 배포할 때 유해한 현실적 결과를 초래할 수 있습니다.

다른 품질 문제와 달리 논리적 결함을 모니터링하고 테스트하는 것은 어렵습니다. 패턴 매칭이 아닌 주장을 추론하는 것이 필요합니다.

따라서, 모델 개발자는 지표 최적화 후 논리적 오류를 적극적으로 해결하는 것이 중요합니다. 인과 모델링, 견고성 테스트 및 편향 완화와 같은 전문 기술이 결함이 있는 추론을 방지하는 데 도움이 될 수 있습니다. 전체적으로 논리적 결함을 허용하면 모델이 덜 안전하고 비윤리적으로 됩니다. 오류를 제거하면 모델 출력이 논리적으로 타당하고 인간의 추론과 일치하게 유지됩니다. 이는 사용자 신뢰를 유지하고 위험을 완화합니다.

## 예제

```python
<!--IMPORTS:[{"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Logical Fallacy chain"}, {"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Logical Fallacy chain"}, {"imported": "LLMChain", "source": "langchain.chains.llm", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Logical Fallacy chain"}, {"imported": "FallacyChain", "source": "langchain_experimental.fallacy_removal.base", "docs": "https://api.python.langchain.com/en/latest/fallacy_removal/langchain_experimental.fallacy_removal.base.FallacyChain.html", "title": "Logical Fallacy chain"}]-->
# 임포트

from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain_experimental.fallacy_removal.base import FallacyChain
```

```python
# 논리적 오류가 포함된 모델 출력 예제

misleading_prompt = PromptTemplate(
    template="""당신은 대답 설명에 내재된 논리적 오류만 사용해야 합니다.

Question: {question}

잘못된 답변:""",
    input_variables=["question"],
)

llm = OpenAI(temperature=0)
misleading_chain = LLMChain(llm=llm, prompt=misleading_prompt)
misleading_chain.run(question="지구가 둥글다는 것을 어떻게 알 수 있나요?")
```

```
    '지구가 둥글다는 것은 교수님이 그렇게 말했다는 이유로, 모두가 교수님을 믿기 때문입니다.'
```

```python
fallacies = FallacyChain.get_fallacies(["correction"])
fallacy_chain = FallacyChain.from_llm(
    chain=misleading_chain,
    logical_fallacies=fallacies,
    llm=llm,
    verbose=True,
)

fallacy_chain.run(question="지구가 둥글다는 것을 어떻게 알 수 있나요?")
```

```


    > 새로운 FallacyChain 체인에 들어갑니다...
    초기 응답: 지구가 둥글다는 것은 교수님이 그렇게 말했다는 이유로, 모두가 교수님을 믿기 때문입니다.

    수정 적용 중...

    오류 비판: 모델의 응답은 권위에 호소하고 대중의 믿음(모두가 교수님을 믿는다)을 사용합니다. 오류 비판 필요.

    업데이트된 응답: 지구가 둥글다는 것은 우주에서 찍은 사진, 지평선 너머로 사라지는 배 관찰, 달의 곡선 그림자, 지구를 일주할 수 있는 능력과 같은 경험적 증거를 통해 알 수 있습니다.


    > 체인 완료.





    '지구가 둥글다는 것은 우주에서 찍은 사진, 지평선 너머로 사라지는 배 관찰, 달의 곡선 그림자, 지구를 일주할 수 있는 능력과 같은 경험적 증거를 통해 알 수 있습니다.'
```