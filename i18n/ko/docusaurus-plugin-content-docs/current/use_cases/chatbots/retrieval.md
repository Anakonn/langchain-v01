---
sidebar_position: 2
translated: true
---

# ê²€ìƒ‰

ê²€ìƒ‰ì€ ì±—ë´‡ì´ ì±— ëª¨ë¸ì˜ í›ˆë ¨ ë°ì´í„° ì™¸ë¶€ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ê°•í™”í•˜ëŠ” ì¼ë°˜ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ì±—ë´‡ì˜ ë§¥ë½ì—ì„œ ê²€ìƒ‰ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¨ì§€ë§Œ, ê²€ìƒ‰ì€ ë§¤ìš° ë¯¸ë¬˜í•˜ê³  ê¹Šì´ ìˆëŠ” ì£¼ì œì´ë¯€ë¡œ [ë¬¸ì„œì˜ ë‹¤ë¥¸ ë¶€ë¶„](/docs/use_cases/question_answering/)ì„ íƒêµ¬í•´ë³´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤!

## ì„¤ì •

ëª‡ ê°€ì§€ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  OpenAI API í‚¤ë¥¼ `OPENAI_API_KEY`ë¼ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:

```python
%pip install --upgrade --quiet langchain langchain-openai langchain-chroma beautifulsoup4

# í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°:

import dotenv

dotenv.load_dotenv()
```

```output
[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.
```

```output
True
```

ì•„ë˜ ì˜ˆì œì—ì„œ ì‚¬ìš©í•  ì±„íŒ… ëª¨ë¸ë„ ì„¤ì •í•´ë´…ì‹œë‹¤.

```python
from langchain_openai import ChatOpenAI

chat = ChatOpenAI(model="gpt-3.5-turbo-1106", temperature=0.2)
```

## ê²€ìƒ‰ê¸° ìƒì„±

[LangSmith ë¬¸ì„œ](https://docs.smith.langchain.com/overview)ë¥¼ ì†ŒìŠ¤ ìë£Œë¡œ ì‚¬ìš©í•˜ì—¬ ë‚´ìš©ì„ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥í•œ í›„ ë‚˜ì¤‘ì— ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” ë°ì´í„° ì†ŒìŠ¤ë¥¼ êµ¬ë¬¸ ë¶„ì„í•˜ê³  ì €ì¥í•˜ëŠ” ê²ƒì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì„ ë‹¤ë£¨ì§€ ì•ŠìŠµë‹ˆë‹¤ - ë” ë§ì€ ë‚´ìš©ì„ ì•Œê³  ì‹¶ë‹¤ë©´ [ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒì„±ì— ëŒ€í•œ ì‹¬ì¸µ ë¬¸ì„œ](https://docs.smith.langchain.com/overview)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

ë¬¸ì„œ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê² ìŠµë‹ˆë‹¤:

```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
data = loader.load()
```

ë‹¤ìŒìœ¼ë¡œ, LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
all_splits = text_splitter.split_documents(data)
```

ê·¸ëŸ° ë‹¤ìŒ ì´ ì²­í¬ë¥¼ ì„ë² ë“œí•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤:

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
```

ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```python
# këŠ” ê²€ìƒ‰í•  ì²­í¬ì˜ ìˆ˜ì…ë‹ˆë‹¤.

retriever = vectorstore.as_retriever(k=4)

docs = retriever.invoke("Can LangSmith help test my LLM applications?")

docs
```

```output
[Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})]
```

ìœ„ì—ì„œ ê²€ìƒ‰ê¸°ë¥¼ í˜¸ì¶œí•˜ë©´ LangSmith ë¬¸ì„œì˜ í…ŒìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ ì •ë³´ê°€ í¬í•¨ëœ ì¼ë¶€ ë¶€ë¶„ì´ ë°˜í™˜ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì œ LangSmith ë¬¸ì„œì—ì„œ ê´€ë ¨ ë°ì´í„°ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆëŠ” ê²€ìƒ‰ê¸°ë¥¼ ê°–ì¶”ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤!

## ë¬¸ì„œ ì²´ì¸

ì´ì œ LangChain ë¬¸ì„œë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆëŠ” ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í–ˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì²´ì¸ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤. ëª¨ë“  ì…ë ¥ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— "ì±„ì›Œ ë„£ëŠ”" `create_stuff_documents_chain` í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·í•˜ëŠ” ì‘ì—…ë„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì´ í•¨ìˆ˜ëŠ” ì±— ëª¨ë¸ ì™¸ì—ë„ `context` ë³€ìˆ˜ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ì™€ `messages`ë¼ëŠ” ì±„íŒ… ê¸°ë¡ ë©”ì‹œì§€ì˜ ìë¦¬ í‘œì‹œìë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì „ë‹¬í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

SYSTEM_TEMPLATE = """
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´, ì•„ë¬´ê²ƒë„ ë§Œë“¤ì§€ ë§ê³  "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”:

<context>
{context}
</context>
"""

question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            SYSTEM_TEMPLATE,
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(chat, question_answering_prompt)
```

ì´ `document_chain`ì„ ë‹¨ë…ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ê²€ìƒ‰í•œ ë¬¸ì„œì™€ ë™ì¼í•œ ì§ˆë¬¸ì¸ `how can langsmith help with testing?`ì„ ì‚¬ìš©í•´ ë´…ì‹œë‹¤:

```python
from langchain_core.messages import HumanMessage

document_chain.invoke(
    {
        "context": docs,
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?")
        ],
    }
)
```

```output
'ë„¤, LangSmithëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í…ŒìŠ¤íŠ¸ì™€ í‰ê°€ë¥¼ ë„ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ˆê¸° ì„¤ì •ì„ ê°„ì†Œí™”í•˜ê³  ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ëª¨ë‹ˆí„°ë§í•˜ë©°, ëª¨ë“  ì¶”ì ì„ ê¸°ë¡í•˜ê³ , ëŒ€ê¸° ì‹œê°„ ë° í† í° ì‚¬ìš© í†µê³„ë¥¼ ì‹œê°í™”í•˜ê³ , ë°œìƒí•˜ëŠ” íŠ¹ì • ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
```

ì¢‹ìŠµë‹ˆë‹¤! ë¹„êµë¥¼ ìœ„í•´ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ì—†ì´ ì‹œë„í•´ ë³´ê³  ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
document_chain.invoke(
    {
        "context": [],
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?")
        ],
    }
)
```

```output
"LangSmithì˜ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸ ê´€ë ¨ êµ¬ì²´ì ì¸ ê¸°ëŠ¥ì— ëŒ€í•´ ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. LangSmithì— ì§ì ‘ ë¬¸ì˜í•˜ì—¬ ê·¸ë“¤ì˜ ì„œë¹„ìŠ¤ì™€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸ë¥¼ ë•ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
```

LLMì´ ì•„ë¬´ëŸ° ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê²€ìƒ‰ ì²´ì¸

ì´ì œ ë¬¸ì„œ ì²´ì¸ê³¼ ê²€ìƒ‰ê¸°ë¥¼ ê²°í•©í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ê·¸ ì˜ˆì…ë‹ˆë‹¤:

```python
from typing import Dict

from langchain_core.runnables import RunnablePassthrough


def parse_retriever_input(params: Dict):
    return params["messages"][-1].content


retrieval_chain = RunnablePassthrough.assign(
    context=parse_retriever_input | retriever,
).assign(
    answer=document_chain,
)
```

ì…ë ¥ ë©”ì‹œì§€ ëª©ë¡ì—ì„œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ê¸°ì— ì „ë‹¬í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ ë¬¸ì„œë¥¼ ë¬¸ì„œ ì²´ì¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

ì´ ì²´ì¸ì„ í˜¸ì¶œí•˜ë©´ ì•ì„œ ì„¤ëª…í•œ ë‘ ë‹¨ê³„ë¥¼ ê²°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
retrieval_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?")
        ],
    }
)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],
 'context': [Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})],
 'answer': 'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}
```

ì¢‹ì•„ ë³´ì…ë‹ˆë‹¤!

## ì¿¼ë¦¬ ë³€í™˜

ìš°ë¦¬ì˜ ê²€ìƒ‰ ì²´ì¸ì€ LangSmithì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆì§€ë§Œ, ë¬¸ì œëŠ” ì±—ë´‡ì´ ì‚¬ìš©ìì™€ ëŒ€í™”ì‹ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ë©° í›„ì† ì§ˆë¬¸ì„ ì²˜ë¦¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

í˜„ì¬ í˜•íƒœì˜ ì²´ì¸ì€ ì´ê²ƒì— ì–´ë ¤ì›€ì„ ê²ªì„ ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ì¸ `Tell me more!`ì„ ê³ ë ¤í•´ ë³´ì„¸ìš”. ê²€ìƒ‰ê¸°ì— í•´ë‹¹ ì¿¼ë¦¬ë¥¼ ì§ì ‘ ì „ë‹¬í•˜ë©´ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸ì™€ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œê°€ ë°˜í™˜ë©ë‹ˆë‹¤:

```python
retriever.invoke("Tell me more!")
```

```output
[Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content='playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
 Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})]
```

ì´ëŠ” ê²€ìƒ‰ê¸°ê°€ ìƒíƒœë¥¼ ê°–ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, ì£¼ì–´ì§„ ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë§Œ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì¿¼ë¦¬ë¥¼ ì™¸ë¶€ ì°¸ì¡° ì—†ì´ ë…ë¦½ì ì¸ ì¿¼ë¦¬ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ ê·¸ ì˜ˆì…ë‹ˆë‹¤:

```python
from langchain_core.messages import AIMessage, HumanMessage

query_transform_prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="messages"),
        (
            "user",
            "ìœ„ì˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ëŒ€í™”ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ê²€ìƒ‰í•  ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”. ì¿¼ë¦¬ë§Œ ì‘ë‹µí•˜ê³ , ë‹¤ë¥¸ ê²ƒì€ í•˜ì§€ ë§ˆì„¸ìš”.",
        ),
    ]
)

query_transformation_chain = query_transform_prompt | chat

query_transformation_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
            AIMessage(
                content="Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            ),
            HumanMessage(content="Tell me more!"),
        ],
    }
)
```

```output
AIMessage(content='"LangSmith LLM application testing and evaluation"')
```

ë©‹ì§‘ë‹ˆë‹¤! ë³€í™˜ëœ ì¿¼ë¦¬ëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ì´ë¥¼ ê²€ìƒ‰ ì²´ì¸ì— ì¶”ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ê²€ìƒ‰ê¸°ë¥¼ ê°ìŒ€ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch

query_transforming_retriever_chain = RunnableBranch(
    (
        lambda x: len(x.get("messages", [])) == 1,
        # ë©”ì‹œì§€ê°€ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ ë©”ì‹œì§€ ë‚´ìš©ì„ ê²€ìƒ‰ê¸°ì— ì „ë‹¬í•©ë‹ˆë‹¤
        (lambda x: x["messages"][-1].content) | retriever,
    ),
    # ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ LLM ì²´ì¸ì— ì…ë ¥ì„ ì „ë‹¬í•´ ì¿¼ë¦¬ë¥¼ ë³€í™˜í•œ ë‹¤ìŒ, ê²€ìƒ‰ê¸°ì— ì „ë‹¬í•©ë‹ˆë‹¤
    query_transform_prompt | chat | StrOutputParser() | retriever,
).with_config(run_name="chat_retriever_chain")
```

ê·¸ëŸ° ë‹¤ìŒ, ì´ ì¿¼ë¦¬ ë³€í™˜ ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì²´ì¸ì´ í›„ì† ì§ˆë¬¸ì„ ë” ì˜ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤:

```python
SYSTEM_TEMPLATE = """
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´, ì•„ë¬´ê²ƒë„ ë§Œë“¤ì§€ ë§ê³  "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”:

<context>
{context}
</context>
"""

question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            SYSTEM_TEMPLATE,
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(chat, question_answering_prompt)

conversational_retrieval_chain = RunnablePassthrough.assign(
    context=query_transforming_retriever_chain,
).assign(
    answer=document_chain,
)
```

ë©‹ì§‘ë‹ˆë‹¤! ì´ì „ê³¼ ë™ì¼í•œ ì…ë ¥ìœ¼ë¡œ ì´ ìƒˆë¡œìš´ ì²´ì¸ì„ í˜¸ì¶œí•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
conversational_retrieval_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
        ]
    }
)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],
 'context': [Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})],
 'answer': 'ë„¤, LangSmithëŠ” LLM (ì–¸ì–´ ëª¨ë¸) ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í…ŒìŠ¤íŠ¸ ë° í‰ê°€ë¥¼ ë„ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ˆê¸° ì„¤ì •ì„ ê°„ì†Œí™”í•˜ê³ , ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ëª¨ë‹ˆí„°ë§í•˜ë©°, ëª¨ë“  ì¶”ì ì„ ê¸°ë¡í•˜ê³ , ëŒ€ê¸° ì‹œê°„ ë° í† í° ì‚¬ìš© í†µê³„ë¥¼ ì‹œê°í™”í•˜ê³ , ë°œìƒí•˜ëŠ” íŠ¹ì • ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}
```

```python
conversational_retrieval_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
            AIMessage(
                content="Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            ),
            HumanMessage(content="Tell me more!"),
        ],
    }
)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),
  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),
  HumanMessage(content='Tell me more!')],
 'context': [Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),
  Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})],
 'answer': 'LangSmith simplifies the initial setup for building reliable LLM applications, but it acknowledges that there is still work needed to bring the performance of prompts, chains, and agents up to the level where they are reliable enough to be used in production. It also provides the capability to manually review and annotate runs through annotation queues, allowing you to select runs based on criteria like model type or automatic evaluation scores for human review. This feature is particularly useful for assessing subjective qualities that automatic evaluators struggle with.'}
```

[ì´ LangSmith ì¶”ì ](https://smith.langchain.com/public/bb329a3b-e92a-4063-ad78-43f720fbb5a2/r)ì„ í™•ì¸í•˜ì—¬ ë‚´ë¶€ ì¿¼ë¦¬ ë³€í™˜ ë‹¨ê³„ë¥¼ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ìŠ¤íŠ¸ë¦¬ë°

ì´ ì²´ì¸ì€ LCELë¡œ êµ¬ì„±ë˜ì—ˆê¸° ë•Œë¬¸ì— `.stream()`ê³¼ ê°™ì€ ìµìˆ™í•œ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
stream = conversational_retrieval_chain.stream(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
            AIMessage(
                content="Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            ),
            HumanMessage(content="Tell me more!"),
        ],
    }
)

for chunk in stream:
    print(chunk)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'), HumanMessage(content='Tell me more!')]}
{'context': [Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}), Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}), Document(page_content='Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}), Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})]}
{'answer': ''}
{'answer': 'Lang'}
{'answer': 'Smith'}
{'answer': ' simpl'}
{'answer': 'ifies'}
{'answer': ' the'}
{'answer': ' initial'}
{'answer': ' setup'}
{'answer': ' for'}
{'answer': ' building'}
{'answer': ' reliable'}
{'answer': ' L'}
{'answer': 'LM'}
{'answer': ' applications'}
{'answer': '.'}
{'answer': ' It'}
{'answer': ' provides'}
{'answer': ' features'}
{'answer': ' for'}
{'answer': ' manually'}
{'answer': ' reviewing'}
{'answer': ' and'}
{'answer': ' annot'}
{'answer': 'ating'}
{'answer': ' runs'}
{'answer': ' through'}
{'answer': ' annotation'}
{'answer': ' queues'}
{'answer': ','}
{'answer': ' allowing'}
{'answer': ' you'}
{'answer': ' to'}
{'answer': ' select'}
{'answer': ' runs'}
{'answer': ' based'}
{'answer': ' on'}
{'answer': ' criteria'}
{'answer': ' like'}
{'answer': ' model'}
{'answer': ' type'}
{'answer': ' or'}
{'answer': ' automatic'}
{'answer': ' evaluation'}
{'answer': ' scores'}
{'answer': ','}
{'answer': ' and'}
{'answer': ' queue'}
{'answer': ' them'}
{'answer': ' up'}
{'answer': ' for'}
{'answer': ' human'}
{'answer': ' review'}
{'answer': '.'}
{'answer': ' As'}
{'answer': ' a'}
{'answer': ' reviewer'}
{'answer': ','}
{'answer': ' you'}
{'answer': ' can'}
{'answer': ' quickly'}
{'answer': ' step'}
{'answer': ' through'}
{'answer': ' the'}
{'answer': ' runs'}
{'answer': ','}
{'answer': ' view'}
{'answer': ' the'}
{'answer': ' input'}
{'answer': ','}
{'answer': ' output'}
{'answer': ','}
{'answer': ' and'}
{'answer': ' any'}
{'answer': ' existing'}
{'answer': ' tags'}
{'answer': ' before'}
{'answer': ' adding'}
{'answer': ' your'}
{'answer': ' own'}
{'answer': ' feedback'}
{'answer': '.'}
{'answer': ' This'}
{'answer': ' can'}
{'answer': ' be'}
{'answer': ' particularly'}
{'answer': ' useful'}
{'answer': ' for'}
{'answer': ' assessing'}
{'answer': ' subjective'}
{'answer': ' qualities'}
{'answer': ' that'}
{'answer': ' automatic'}
{'answer': ' evalu'}
{'answer': 'ators'}
{'answer': ' struggle'}
{'answer': ' with'}
{'answer': '.'}
{'answer': ''}
```

## ì¶”ê°€ ì½ê¸°

ì´ ê°€ì´ë“œëŠ” ê²€ìƒ‰ ê¸°ìˆ ì˜ ì¼ë¶€ë§Œì„ ë‹¤ë£¹ë‹ˆë‹¤. ê´€ë ¨ ë°ì´í„°ì˜ ìˆ˜ì§‘, ì¤€ë¹„, ê²€ìƒ‰ì— ëŒ€í•œ ë” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì•Œê³  ì‹¶ë‹¤ë©´ [ì´ ì„¹ì…˜](/docs/modules/data_connection/)ì˜ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.