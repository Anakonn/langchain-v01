---
sidebar_position: 5
title: 지침
translated: true
---

추출 결과의 품질은 여러 요인에 따라 달라집니다.

모델의 성능을 최대로 끌어내기 위한 지침은 다음과 같습니다:

* 모델의 온도를 `0`으로 설정합니다.
* 프롬프트를 개선합니다. 프롬프트는 정확하고 간결해야 합니다.
* 스키마를 문서화합니다: 스키마를 문서화하여 LLM에 더 많은 정보를 제공합니다.
* 참조 예제를 제공합니다! 다양한 예제, 특히 추출할 내용이 없는 예제를 포함하는 것이 도움이 됩니다.
* 많은 예제가 있는 경우, 가장 관련성 높은 예제를 검색하는 검색기를 사용합니다.
* 최신 LLM/챗 모델(e.g., gpt-4, claude-3 등)로 벤치마크를 수행합니다 -- 모델 제공업체에 최신 모델을 확인하세요!
* 스키마가 매우 큰 경우, 여러 작은 스키마로 나누어 별도로 추출을 수행하고 결과를 병합합니다.
* 스키마가 모델이 정보를 추출하지 않도록 허용하는지 확인합니다. 그렇지 않으면 모델이 정보를 만들어내야 할 것입니다!
* 검증/수정 단계를 추가합니다 (LLM에게 추출 결과를 수정하거나 검증하도록 요청합니다).

## 벤치마크

* [LangSmith 🦜️🛠️](https://docs.smith.langchain.com/)를 사용하여 사용 사례에 대한 데이터를 생성하고 벤치마크합니다.
* LLM이 충분히 좋은가요? [langchain-benchmarks 🦜💯](https://github.com/langchain-ai/langchain-benchmarks)를 사용하여 기존 데이터셋으로 LLM을 테스트해보세요.

## 유의할 점! 😶‍🌫️

* LLM은 훌륭하지만 모든 경우에 필요한 것은 아닙니다! 단일 구조화된 소스(e.g., LinkedIn)에서 정보를 추출하는 경우, LLM을 사용하는 것은 좋은 생각이 아닙니다 – 전통적인 웹 스크래핑이 훨씬 저렴하고 신뢰할 수 있습니다.
* **인간의 개입** **완벽한 품질**이 필요하다면, 인간의 개입을 계획해야 할 것입니다 -- 복잡한 추출 작업에서 최고의 LLM도 실수를 할 수 있습니다.