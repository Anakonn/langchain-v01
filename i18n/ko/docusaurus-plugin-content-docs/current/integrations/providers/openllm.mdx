---
translated: true
---

# OpenLLM

이 페이지는 [OpenLLM](https://github.com/bentoml/OpenLLM)을 LangChain과 함께 사용하는 방법을 보여줍니다.

`OpenLLM`은 대규모 언어 모델(LLM)을 프로덕션에서 운영하기 위한 오픈 플랫폼입니다. 개발자가 오픈 소스 LLM을 쉽게 실행하고, 클라우드 또는 온-프레미스에 배포하며, 강력한 AI 애플리케이션을 구축할 수 있게 해줍니다.

## 설치 및 설정

PyPI를 통해 OpenLLM 패키지를 설치하세요:

```bash
pip install openllm
```

## LLM

OpenLLM은 다양한 오픈 소스 LLM과 사용자의 fine-tuned LLM을 지원합니다. `openllm model` 명령어를 사용하여 OpenLLM에 최적화된 모든 사용 가능한 모델을 확인하세요.

## Wrappers

OpenLLM Wrapper는 in-process에서 LLM을 로드하거나 원격 OpenLLM 서버에 액세스할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "OpenLLM", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.openllm.OpenLLM.html", "title": "OpenLLM"}]-->
from langchain_community.llms import OpenLLM
```

### OpenLLM 서버를 위한 Wrapper

이 Wrapper는 HTTP 또는 gRPC를 통해 OpenLLM 서버에 연결할 수 있습니다. OpenLLM 서버는 로컬 또는 클라우드에서 실행될 수 있습니다.

로컬에서 테스트해보려면 OpenLLM 서버를 시작하세요:

```bash
openllm start flan-t5
```

Wrapper 사용법:

```python
<!--IMPORTS:[{"imported": "OpenLLM", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.openllm.OpenLLM.html", "title": "OpenLLM"}]-->
from langchain_community.llms import OpenLLM

llm = OpenLLM(server_url='http://localhost:3000')

llm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")
```

### 로컬 추론을 위한 Wrapper

현재 Python 프로세스에서 LLM을 로드하고 추론을 실행하기 위해 OpenLLM Wrapper를 사용할 수도 있습니다.

```python
<!--IMPORTS:[{"imported": "OpenLLM", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.openllm.OpenLLM.html", "title": "OpenLLM"}]-->
from langchain_community.llms import OpenLLM

llm = OpenLLM(model_name="dolly-v2", model_id='databricks/dolly-v2-7b')

llm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")
```

### 사용법

OpenLLM Wrapper에 대한 더 자세한 연습은 [example notebook](/docs/integrations/llms/openllm)을 참고하세요.
