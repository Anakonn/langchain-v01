---
translated: true
---

# Intel

>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel)은 🤗 Transformers와 Diffusers 라이브러리와 Intel이 제공하는 다양한 도구 및 라이브러리 간의 인터페이스로, Intel 아키텍처에서 엔드 투 엔드 파이프라인을 가속화하는 데 사용됩니다.

>[Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX)는 Intel Gaudi2, Intel CPU, Intel GPU를 포함한 다양한 Intel 플랫폼에서 Transformer 기반 모델의 최적의 성능으로 GenAI/LLM을 가속화하기 위해 설계된 혁신적인 툴킷입니다.

이 페이지에서는 LangChain에서 optimum-intel과 ITREX를 사용하는 방법을 다룹니다.

## Optimum-intel

[optimum-intel](https://github.com/huggingface/optimum-intel.git)과 [IPEX](https://github.com/intel/intel-extension-for-pytorch)와 관련된 모든 기능입니다.

### 설치

optimum-intel과 ipex를 다음과 같이 설치하세요:

```bash
pip install optimum[neural-compressor]
pip install intel_extension_for_pytorch
```

아래 설치 지침을 따르세요:

* [여기](https://github.com/huggingface/optimum-intel)에 나와 있는 대로 optimum-intel을 설치하세요.
* [여기](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu)에 나와 있는 대로 IPEX를 설치하세요.

### 임베딩 모델

[사용 예제](/docs/integrations/text_embedding/optimum_intel)를 참고하세요.
또한 cookbook 디렉토리에 있는 "rag_with_quantized_embeddings.ipynb" 튜토리얼 노트북에서 임베더를 RAG 파이프라인에 사용하는 예제를 제공합니다.

```python
<!--IMPORTS:[{"imported": "QuantizedBiEncoderEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.optimum_intel.QuantizedBiEncoderEmbeddings.html", "title": "Intel"}]-->
from langchain_community.embeddings import QuantizedBiEncoderEmbeddings
```

## Intel® Extension for Transformers (ITREX)

(ITREX)는 특히 4세대 Intel Xeon Scalable 프로세서 Sapphire Rapids(코드명 Sapphire Rapids)에서 효과적인 Transformer 기반 모델을 가속화하기 위한 혁신적인 툴킷입니다.

양자화는 가중치를 더 작은 비트 수로 표현하여 이러한 가중치의 정밀도를 줄이는 프로세스입니다. 가중치 전용 양자화는 특히 활성화와 같은 다른 구성 요소를 원래 정밀도로 유지하면서 신경망의 가중치만 양자화하는 데 초점을 맞춥니다.

대규모 언어 모델(LLM)이 더 널리 사용됨에 따라 이러한 현대 아키텍처의 계산 요구 사항을 충족하면서도 정확성을 유지할 수 있는 새로운 개선된 양자화 방법에 대한 수요가 증가하고 있습니다. [일반 양자화](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md)와 비교하여 가중치 전용 양자화는 성능과 정확성의 균형을 맞추는 데 더 나은 절충안일 수 있습니다. 아래에서 볼 수 있듯이 LLM 배포의 병목 현상은 메모리 대역폭이므로 일반적으로 가중치 전용 양자화가 더 나은 정확성을 제공할 수 있습니다.

여기에서는 ITREX를 사용한 임베딩 모델 및 Transformer 대규모 언어 모델에 대한 가중치 전용 양자화를 소개합니다. 가중치 전용 양자화는 신경망의 메모리 및 계산 요구 사항을 줄이기 위해 딥 러닝에서 사용되는 기술입니다. 딥 신경망의 맥락에서 모델 매개변수, 즉 가중치는 일반적으로 부동 소수점 숫자로 표현되며, 이는 상당한 메모리를 소비하고 집약적인 계산 리소스를 필요로 할 수 있습니다.

[intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers)와 관련된 모든 기능입니다.

### 설치

intel-extension-for-transformers를 설치하세요. 시스템 요구 사항 및 기타 설치 팁은 [설치 가이드](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)를 참조하세요.

```bash
pip install intel-extension-for-transformers
```

필요한 다른 패키지를 설치하세요.

```bash
pip install -U torch onnx accelerate datasets
```

### 임베딩 모델

[사용 예제](/docs/integrations/text_embedding/itrex)를 참고하세요.

```python
<!--IMPORTS:[{"imported": "QuantizedBgeEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.itrex.QuantizedBgeEmbeddings.html", "title": "Intel"}]-->
from langchain_community.embeddings import QuantizedBgeEmbeddings
```

### ITREX를 사용한 가중치 전용 양자화

[사용 예제](/docs/integrations/llms/weight_only_quantization)를 참고하세요.

## 구성 매개변수 세부 정보

여기에 `WeightOnlyQuantConfig` 클래스의 세부 정보가 있습니다.

#### weight_dtype (string): 가중치 데이터 유형, 기본값은 "nf4"입니다.

다음 데이터 유형으로 가중치를 양자화할 수 있습니다(WeightOnlyQuantConfig의 weight_dtype):
* **int8**: 8비트 데이터 유형을 사용합니다.
* **int4_fullrange**: 일반 int4 범위 [-7,7]와 비교하여 int4 범위의 -8 값을 사용합니다.
* **int4_clip**: int4 범위 내로 클리핑하고 나머지는 0으로 설정합니다.
* **nf4**: 정규화된 float 4비트 데이터 유형을 사용합니다.
* **fp4_e2m1**: 일반 float 4비트 데이터 유형을 사용합니다. "e2"는 2비트가 지수에 사용되고 "m1"은 1비트가 가수에 사용됨을 의미합니다.

#### compute_dtype (string): 계산 데이터 유형, 기본값은 "fp32"입니다.

이러한 기술은 4비트 또는 8비트로 가중치를 저장하지만, 계산은 여전히 float32, bfloat16 또는 int8(WeightOnlyQuantConfig의 compute_dtype)로 수행됩니다:
* **fp32**: float32 데이터 유형을 사용하여 계산합니다.
* **bf16**: bfloat16 데이터 유형을 사용하여 계산합니다.
* **int8**: 8비트 데이터 유형을 사용하여 계산합니다.

#### llm_int8_skip_modules (module's name 목록): 양자화를 건너뛸 모듈, 기본값은 None입니다.

양자화를 건너뛸 모듈의 목록입니다.

#### scale_dtype (string): 스케일 데이터 유형, 기본값은 "fp32"입니다.

현재 "fp32"(float32)만 지원됩니다.

#### mse_range (boolean): 범위 [0.805, 1.0, 0.005]에서 최적의 클립 범위를 검색할지 여부, 기본값은 False입니다.

#### use_double_quant (boolean): 스케일을 양자화할지 여부, 기본값은 False입니다.

아직 지원되지 않습니다.

#### double_quant_dtype (string): 이중 양자화를 위해 예약되어 있습니다.

#### double_quant_scale_dtype (string): 이중 양자화를 위해 예약되어 있습니다.

#### group_size (int): 양자화 시 그룹 크기.

#### scheme (string): 가중치를 어떤 형식으로 양자화할지 결정합니다. 기본값은 "sym"입니다.

* **sym**: 대칭.
* **asym**: 비대칭.

#### algorithm (string): 정확도를 높이기 위한 알고리즘. 기본값은 "RTN"입니다.

* **RTN**: Round-to-nearest (RTN)은 직관적으로 생각할 수 있는 양자화 방법입니다.
* **AWQ**: 중요한 가중치 채널의 1%만 보호해도 양자화 오류를 크게 줄일 수 있습니다. 중요한 가중치 채널은 활성화와 가중치 분포를 관찰하여 선택됩니다. 중요한 가중치는 큰 스케일 팩터를 곱한 후 양자화됩니다.
* **TEQ**: 가중치 전용 양자화에서 FP32 정밀도를 유지하는 학습 가능한 등가 변환.
