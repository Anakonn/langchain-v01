---
sidebar_class_name: hidden
sidebar_position: 5
translated: true
---

# 콜백

:::info
3rd-party 도구와의 내장 콜백 통합에 대한 문서는 [Integrations](/docs/integrations/callbacks/)를 참고하세요.
:::

LangChain은 LLM 애플리케이션의 다양한 단계에 훅을 걸 수 있는 콜백 시스템을 제공합니다. 이는 로깅, 모니터링, 스트리밍 및 기타 작업에 유용합니다.

API 전반에 걸쳐 사용 가능한 `callbacks` 인수를 사용하여 이러한 이벤트에 가입할 수 있습니다. 이 인수는 아래에 자세히 설명된 메서드 중 하나 이상을 구현해야 하는 핸들러 객체 목록입니다.

## 콜백 핸들러

`CallbackHandlers`는 `CallbackHandler` 인터페이스를 구현하는 객체입니다. 이 인터페이스에는 가입할 수 있는 이벤트에 대한 메서드가 있습니다. `CallbackManager`는 이벤트가 트리거될 때 각 핸들러의 적절한 메서드를 호출합니다.

```python
class BaseCallbackHandler:
    """Base callback handler that can be used to handle callbacks from langchain."""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Run when LLM starts running."""

    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any
    ) -> Any:
        """Run when Chat Model starts running."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """Run on new LLM token. Only available when streaming is enabled."""

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """Run when LLM ends running."""

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when LLM errors."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """Run when chain starts running."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """Run when chain ends running."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when chain errors."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """Run when tool starts running."""

    def on_tool_end(self, output: Any, **kwargs: Any) -> Any:
        """Run when tool ends running."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when tool errors."""

    def on_text(self, text: str, **kwargs: Any) -> Any:
        """Run on arbitrary text."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Run on agent action."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        """Run on agent end."""
```

## 시작하기

LangChain은 시작하는 데 사용할 수 있는 몇 가지 내장 핸들러를 제공합니다. 이들은 `langchain_core/callbacks` 모듈에서 사용할 수 있습니다. 가장 기본적인 핸들러는 `StdOutCallbackHandler`로, 모든 이벤트를 `stdout`에 기록합니다.

**참고**: 객체의 `verbose` 플래그가 true로 설정된 경우 `StdOutCallbackHandler`가 명시적으로 전달되지 않더라도 호출됩니다.

```python
<!--IMPORTS:[{"imported": "StdOutCallbackHandler", "source": "langchain_core.callbacks", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.stdout.StdOutCallbackHandler.html", "title": "Callbacks"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "Callbacks"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Callbacks"}, {"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Callbacks"}]-->
from langchain_core.callbacks import StdOutCallbackHandler
from langchain.chains import LLMChain
from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate

handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])
chain.invoke({"number":2})

# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt, verbose=True)
chain.invoke({"number":2})

# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt)
chain.invoke({"number":2}, {"callbacks":[handler]})

```

```output
> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 =

> Finished chain.


> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 =

> Finished chain.


> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 =

> Finished chain.
```

## 콜백을 어디에 전달할 것인가

`callbacks`는 API 전반(Chains, Models, Tools, Agents 등)에 걸쳐 두 가지 다른 위치에서 사용할 수 있습니다:

- **생성자 콜백**: 생성자에 정의됨, 예: `LLMChain(callbacks=[handler], tags=['a-tag'])`. 이 경우 콜백은 해당 객체에서 이루어진 모든 호출에 사용되며, 해당 객체에만 범위가 지정됩니다. 예를 들어 `LLMChain` 생성자에 핸들러를 전달하면 해당 체인에 연결된 모델에서는 사용되지 않습니다.
- **요청 콜백**: `invoke` 메서드에 정의됨. 이 경우 콜백은 해당 특정 요청에만 사용되며, 해당 요청에 포함된 모든 하위 요청(예: LLMChain 호출이 모델 호출을 트리거하는 경우)에도 사용됩니다. `invoke()` 메서드에서 콜백은 구성 매개변수를 통해 전달됩니다.
`invoke()` 메서드를 사용한 예:

```python
handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

config = {
    'callbacks' : [handler]
}

chain = prompt | chain
chain.invoke({"number":2}, config=config)
```

**참고:** `chain = prompt | chain`은 `chain = LLMChain(llm=llm, prompt=prompt)`와 동일합니다(자세한 내용은 [LangChain Expression Language (LCEL) 문서](/docs/expression_language/)를 참조하세요).

`verbose` 인수는 대부분의 객체(Chains, Models, Tools, Agents 등)에서 생성자 인수로 사용할 수 있으며, 이는 해당 객체와 모든 하위 객체에 `ConsoleCallbackHandler`를 `callbacks` 인수로 전달하는 것과 동일합니다. 이는 디버깅에 유용하며, 모든 이벤트를 콘솔에 기록합니다.

### 각각을 언제 사용할까요?

- 생성자 콜백은 로깅, 모니터링 등과 같이 _단일 요청에 특정되지 않고_ 전체 체인에 적용되는 사용 사례에 가장 유용합니다. 예를 들어 `LLMChain`에 대한 모든 요청을 기록하려면 생성자에 핸들러를 전달하면 됩니다.
- 요청 콜백은 스트리밍과 같이 단일 요청의 출력을 특정 웹소켓 연결로 스트리밍하는 등의 사용 사례에 가장 유용합니다. 예를 들어 단일 요청의 출력을 웹소켓으로 스트리밍하려면 `invoke()` 메서드에 핸들러를 전달하면 됩니다.
