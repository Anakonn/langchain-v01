---
sidebar_class_name: hidden
sidebar_position: 4
title: 검색기
translated: true
---

# 검색기

검색기는 구조화되지 않은 쿼리를 받아 문서를 반환하는 인터페이스입니다. 벡터 저장소보다 더 일반적입니다.
검색기는 문서를 저장할 필요가 없고, 단지 문서를 반환(검색)할 수 있으면 됩니다. 벡터 저장소를 검색기의 기반으로 사용할 수 있지만, 다른 유형의 검색기도 있습니다.

검색기는 문자열 `query`를 입력으로 받아 `Document` 목록을 출력합니다.

## 고급 검색 유형

테이블 열:

- **이름**: 검색 알고리즘의 이름.
- **인덱스 유형**: 이 알고리즘이 의존하는 인덱스 유형(있는 경우).
- **LLM 사용**: 이 검색 방법이 LLM을 사용하는지 여부.
- **사용 시기**: 이 검색 방법을 사용해야 하는 경우에 대한 설명.
- **설명**: 이 검색 알고리즘이 수행하는 작업에 대한 설명.

| 이름                      | 인덱스 유형                   | LLM 사용               | 사용 시기                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                       |
|---------------------------|------------------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Vectorstore](./vectorstore)               | Vectorstore                  | 아니요                        | 막 시작하고 빠르고 쉬운 것을 찾고 있다면.                                                                     | 가장 간단한 방법이며 시작하기 가장 쉽습니다. 각 텍스트 조각에 대한 임베딩을 생성합니다.                                                                                                                                                                        |
| [ParentDocument](./parent_document_retriever)            | Vectorstore + Document Store | 아니요                        | 페이지에 서로 구분되는 많은 작은 정보 조각이 있고, 개별적으로 인덱싱하는 것이 가장 좋지만 전체 문서를 검색하는 것이 가장 좋다면.       | 각 문서에 대해 여러 청크를 인덱싱합니다. 그런 다음 임베딩 공간에서 가장 유사한 청크를 찾지만 개별 청크가 아닌 전체 부모 문서를 반환합니다.                                                                                    |
| [Multi Vector](multi_vector)              | Vectorstore + Document Store | 때때로 인덱싱 중 | 텍스트 자체보다 더 관련성 있다고 생각되는 정보를 문서에서 추출할 수 있다면.                          | 각 문서에 대해 여러 벡터를 생성합니다. 각 벡터는 다양한 방식으로 생성될 수 있습니다. 예를 들어 텍스트 요약 및 가설적 질문 등입니다.                                                                                                                            |
| [Self Query](./self_query)               | Vectorstore                  | 예                       | 사용자 질문이 문서 내용의 유사성보다는 메타데이터를 기반으로 문서를 가져오는 것이 더 나은 경우.          | LLM을 사용하여 사용자 입력을 (1) 의미론적으로 검색할 문자열과 (2) 그에 따른 메타데이터 필터로 변환합니다. 이는 종종 질문이 문서의 내용이 아닌 메타데이터에 대한 것이기 때문에 유용합니다.                                               |
| [Contextual Compression](./contextual_compression)    | 모든 유형                          | 때때로                 | 검색된 문서에 너무 많은 관련 없는 정보가 포함되어 LLM을 방해하는 경우.                         | 다른 검색기 위에 후처리 단계를 두고 검색된 문서에서 가장 관련성 있는 정보만 추출합니다. 이는 임베딩 또는 LLM을 사용하여 수행할 수 있습니다.                                                                                                                |
| [Time-Weighted Vectorstore](./time_weighted_vectorstore) | Vectorstore                  | 아니요                        | 문서에 타임스탬프가 연결되어 있고 가장 최근 문서를 검색하고 싶은 경우                                          | 의미론적 유사성(일반 벡터 검색과 같음)과 최신성(인덱싱된 문서의 타임스탬프 고려)을 조합하여 문서를 가져옵니다.                                                                                                                                     |
| [Multi-Query Retriever](./MultiQueryRetriever)     | 모든 유형                          | 예                       | 사용자 질문이 복잡하고 여러 개별 정보 조각이 필요한 경우                                 | LLM을 사용하여 원래 질문에서 여러 개의 질문을 생성합니다. 이는 원래 질문을 적절히 답변하려면 여러 주제에 대한 정보 조각이 필요할 때 유용합니다. 여러 질문을 생성하면 각각에 대한 문서를 가져올 수 있습니다.                              |
| [Ensemble](./ensemble)                  | 모든 유형                          | 아니요                        | 여러 검색 방법이 있고 이를 결합하려는 경우.                                                                        | 여러 검색기에서 문서를 가져온 다음 결합합니다.                                                                                                                                                                                                                                    |
| [Long-Context Reorder](./long_context_reorder)      | 모든 유형                          | 아니요                        | 긴 문맥 모델을 사용하고 검색된 문서의 중간 정보에 주목하지 않는 것을 발견한 경우. | 기본 검색기에서 문서를 가져온 다음 가장 유사한 문서가 앞뒤에 오도록 재정렬합니다. 이는 긴 문맥 모델의 경우 문맥 창의 중간 정보에 주목하지 않는 것으로 나타났기 때문에 유용합니다. |

## [타사 통합](/docs/integrations/retrievers/)

LangChain은 많은 타사 검색 서비스와도 통합됩니다. 이러한 모든 통합 목록은 [여기](/docs/integrations/retrievers/)에서 확인할 수 있습니다.

## LCEL에서 검색기 사용

검색기가 `Runnable`이므로 다른 `Runnable` 객체와 쉽게 구성할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Retrievers"}, {"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Retrievers"}, {"imported": "StrOutputParser", "source": "langchain_core.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html", "title": "Retrievers"}, {"imported": "RunnablePassthrough", "source": "langchain_core.runnables", "docs": "https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html", "title": "Retrievers"}]-->
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

template = """Answer the question based only on the following context:

{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
model = ChatOpenAI()


def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])


chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

chain.invoke("What did the president say about technology?")

```

## 사용자 정의 검색기

[여기](/docs/modules/data_connection/retrievers/custom_retriever)의 문서를 참조하여 사용자 정의 검색기를 구현하세요.
