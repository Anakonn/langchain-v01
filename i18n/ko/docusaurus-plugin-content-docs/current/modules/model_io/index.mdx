---
sidebar_class_name: hidden
sidebar_custom_props:
  description: 언어 모델과 상호작용하기
sidebar_position: 0
translated: true
---

# 모델 입출력

언어 모델 애플리케이션의 핵심 요소는 모델 자체입니다. LangChain은 모든 언어 모델과 상호작용할 수 있는 구성 요소를 제공합니다.

![모델 입출력 프로세스를 보여주는 순서도. 입력 변수에서 구조화된 출력으로 변환되는 과정을 Format, Predict, Parse 단계로 나타냅니다.](/img/model_io.jpg "모델 입출력 프로세스 다이어그램")

# 빠른 시작하기

아래 빠른 시작 가이드에서는 LangChain의 모델 입출력 구성 요소 사용 기본을 다룹니다. LLM(Large Language Model)과 채팅 모델의 두 가지 모델 유형을 소개하고, Prompt Template을 사용하여 이러한 모델의 입력을 형식화하는 방법과 Output Parser를 사용하여 출력을 처리하는 방법을 설명합니다.

LangChain의 언어 모델은 두 가지 유형으로 구분됩니다:

### [ChatModels](/docs/modules/model_io/chat/)

[채팅 모델](/docs/modules/model_io/chat/)은 대화를 위해 특별히 튜닝된 LLM을 기반으로 합니다. 
중요한 점은 채팅 모델의 API 인터페이스가 순수 텍스트 완성 모델과 다르다는 것입니다. 단일 문자열 대신 채팅 메시지 목록을 입력으로 받고, AI 메시지를 출력합니다. 메시지의 정확한 구성 요소는 아래 섹션에서 자세히 설명합니다. GPT-4와 Anthropic의 Claude-2가 채팅 모델로 구현되어 있습니다.

### [LLMs](/docs/modules/model_io/llms/)

LangChain의 [LLMs](/docs/modules/model_io/llms/)는 순수 텍스트 완성 모델을 나타냅니다.
이들 API는 문자열 프롬프트를 입력으로 받고 문자열 완성을 출력합니다. OpenAI의 GPT-3가 LLM으로 구현되어 있습니다.

이 두 API 유형은 입출력 스키마가 다릅니다.

또한 모든 모델이 동일하지는 않습니다. 각 모델마다 가장 잘 작동하는 프롬프팅 전략이 다릅니다. 예를 들어, Anthropic 모델은 XML을 가장 잘 처리하고 OpenAI 모델은 JSON을 가장 잘 처리합니다. 애플리케이션을 설계할 때 이 점을 고려해야 합니다.

이 시작 가이드에서는 채팅 모델을 사용하며, Anthropic이나 OpenAI와 같은 API 또는 Ollama와 같은 로컬 오픈 소스 모델을 제공할 것입니다.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="openai" label="OpenAI" default>

먼저 OpenAI 파트너 패키지를 설치해야 합니다:

```shell
pip install langchain-openai
```

API에 액세스하려면 API 키가 필요합니다. 계정을 만들고 [여기](https://platform.openai.com/account/api-keys)에서 키를 얻을 수 있습니다. 키를 얻으면 다음과 같이 환경 변수로 설정해야 합니다:

```shell
export OPENAI_API_KEY="..."
```

그런 다음 모델을 초기화할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Model I/O"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Model I/O"}]-->
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI

llm = OpenAI()
chat_model = ChatOpenAI(model="gpt-3.5-turbo-0125")
```

환경 변수를 설정하고 싶지 않다면 OpenAI LLM 클래스를 초기화할 때 `api_key` 매개변수로 직접 키를 전달할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Model I/O"}]-->
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(api_key="...")
```

`llm`과 `chat_model`은 특정 모델에 대한 구성을 나타내는 객체입니다.
`temperature` 등의 매개변수로 이들을 초기화할 수 있으며, 이를 다른 곳에 전달할 수 있습니다.
이들의 주된 차이점은 입출력 스키마입니다.
LLM 객체는 문자열을 입력으로 받고 문자열을 출력합니다.
ChatModel 객체는 메시지 목록을 입력으로 받고 메시지를 출력합니다.

LLM과 ChatModel을 호출하면 이 차이를 확인할 수 있습니다.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Model I/O"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

LLM은 문자열을 반환하고, ChatModel은 메시지를 반환합니다.

  </TabItem>
  <TabItem value="local" label="로컬 (Ollama 사용)">

[Ollama](https://ollama.ai/)를 사용하면 Llama 2와 같은 오픈 소스 대규모 언어 모델을 로컬에서 실행할 수 있습니다.

먼저 [이 지침](https://github.com/jmorganca/ollama)을 따라 로컬 Ollama 인스턴스를 설정하고 실행하세요:

* [다운로드](https://ollama.ai/download)
* `ollama pull llama2`를 통해 모델을 가져오기

그런 다음 Ollama 서버가 실행 중인지 확인하세요. 그 후에 다음과 같이 할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "Ollama", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html", "title": "Model I/O"}, {"imported": "ChatOllama", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html", "title": "Model I/O"}]-->
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

llm = Ollama(model="llama2")
chat_model = ChatOllama()
```

`llm`과 `chat_model`은 특정 모델에 대한 구성을 나타내는 객체입니다.
`temperature` 등의 매개변수로 이들을 초기화할 수 있으며, 이를 다른 곳에 전달할 수 있습니다.
이들의 주된 차이점은 입출력 스키마입니다.
LLM 객체는 문자열을 입력으로 받고 문자열을 출력합니다.
ChatModel 객체는 메시지 목록을 입력으로 받고 메시지를 출력합니다.

LLM과 ChatModel을 호출하면 이 차이를 확인할 수 있습니다.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Model I/O"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

LLM은 문자열을 반환하고, ChatModel은 메시지를 반환합니다.

  </TabItem>
  <TabItem value="anthropic" label="Anthropic (채팅 모델만)">

먼저 LangChain x Anthropic 패키지를 가져와야 합니다.

```shell
pip install langchain-anthropic
```

API에 액세스하려면 API 키가 필요합니다. [여기](https://claude.ai/login)에서 계정을 만들어 키를 얻을 수 있습니다. 키를 얻으면 다음과 같이 환경 변수로 설정해야 합니다:

```shell
export ANTHROPIC_API_KEY="..."
```

그런 다음 모델을 초기화할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Model I/O"}]-->
from langchain_anthropic import ChatAnthropic

chat_model = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0.2, max_tokens=1024)
```

환경 변수를 설정하고 싶지 않다면 Anthropic Chat Model 클래스를 초기화할 때 `api_key` 매개변수로 직접 키를 전달할 수 있습니다:

```python
chat_model = ChatAnthropic(api_key="...")
```

  </TabItem>
  <TabItem value="cohere" label="Cohere (chat model only)">

먼저 파트너 패키지를 설치해야 합니다:

```shell
pip install langchain-cohere
```

API에 액세스하려면 API 키가 필요합니다. 계정을 만들고 [여기](https://dashboard.cohere.com/api-keys)로 이동하여 키를 얻을 수 있습니다. 키를 얻으면 다음을 실행하여 환경 변수로 설정하고자 합니다:

```shell
export COHERE_API_KEY="..."
```

그런 다음 모델을 초기화할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Model I/O"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere()
```

환경 변수를 설정하고 싶지 않다면 Cohere LLM 클래스를 초기화할 때 `cohere_api_key` 매개변수를 통해 키를 직접 전달할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Model I/O"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere(cohere_api_key="...")
```

  </TabItem>
</Tabs>

## 프롬프트 템플릿

대부분의 LLM 애플리케이션은 사용자 입력을 직접 LLM에 전달하지 않습니다. 일반적으로 사용자 입력을 더 큰 텍스트 조각에 추가하는데, 이를 프롬프트 템플릿이라고 합니다. 이 템플릿은 특정 작업에 대한 추가 컨텍스트를 제공합니다.

이전 예에서는 모델에 전달한 텍스트에 회사 이름을 생성하라는 지침이 포함되어 있었습니다. 우리 애플리케이션의 경우 사용자가 회사/제품 설명만 제공하면 모델에 대한 지침을 걱정할 필요가 없으면 좋겠습니다.

PromptTemplates는 이 문제를 해결합니다! 
사용자 입력에서 완전히 포맷된 프롬프트로 가는 모든 로직을 번들링합니다.
이는 매우 간단하게 시작할 수 있습니다. 예를 들어, 위의 문자열을 생성하는 프롬프트는 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Model I/O"}]-->
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```python
What is a good name for a company that makes colorful socks?
```

원시 문자열 포맷팅 대신 이러한 방식을 사용하면 여러 가지 장점이 있습니다.
변수를 "부분"으로 나눌 수 있습니다. 예를 들어 일부 변수만 포맷팅할 수 있습니다.
서로 다른 템플릿을 결합하여 단일 프롬프트로 구성할 수 있습니다.
이러한 기능에 대한 설명은 [프롬프트 섹션](/docs/modules/model_io/prompts)을 참조하세요.

`PromptTemplate`는 메시지 목록을 생성하는 데에도 사용할 수 있습니다.
이 경우 프롬프트에는 내용에 대한 정보뿐만 아니라 각 메시지(역할, 목록 내 위치 등)에 대한 정보도 포함됩니다.
여기서 가장 일반적인 경우는 `ChatPromptTemplate`가 `ChatMessageTemplates` 목록이라는 것입니다.
각 `ChatMessageTemplate`에는 해당 `ChatMessage`의 형식(역할 및 내용)을 지정하는 지침이 포함되어 있습니다.
아래에서 이를 살펴보겠습니다:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts.chat", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Model I/O"}]-->
from langchain_core.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```pycon
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

ChatPromptTemplates는 다른 방식으로도 구성할 수 있습니다. [프롬프트 섹션](/docs/modules/model_io/prompts)을 참조하세요.

## 출력 파서

`OutputParser`는 언어 모델의 원시 출력을 다운스트림에서 사용할 수 있는 형식으로 변환합니다.
주요 유형의 `OutputParser`에는 다음이 포함됩니다:

- `LLM` 출력 텍스트를 구조화된 정보(예: JSON)로 변환
- `ChatMessage`를 문자열로만 변환
- 메시지 외에 호출에서 반환되는 추가 정보(예: OpenAI 함수 호출)를 문자열로 변환

전체 정보는 [출력 파서 섹션](/docs/modules/model_io/output_parsers)을 참조하세요.

이 시작 가이드에서는 쉼표로 구분된 값 목록을 구문 분석하는 간단한 파서를 사용합니다.

```python
<!--IMPORTS:[{"imported": "CommaSeparatedListOutputParser", "source": "langchain.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html", "title": "Model I/O"}]-->
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
output_parser.parse("hi, bye")
# >> ['hi', 'bye']
```

## LCEL로 구성

이제 이 모든 것을 하나의 체인으로 결합할 수 있습니다.
이 체인은 입력 변수를 받아 프롬프트 템플릿에 전달하여 프롬프트를 생성하고, 프롬프트를 언어 모델에 전달한 다음 (선택적) 출력 파서를 통과시킵니다.
이는 모듈식 논리 조각을 번들링하는 편리한 방법입니다.
실제로 작동하는 모습을 살펴보겠습니다!

```python
template = "Generate a list of 5 {text}.\n\n{format_instructions}"

chat_prompt = ChatPromptTemplate.from_template(template)
chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())
chain = chat_prompt | chat_model | output_parser
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

`|` 구문을 사용하여 이 구성 요소를 연결하고 있다는 점에 유의하세요.
이 `|` 구문은 LangChain Expression Language(LCEL)에 의해 구동되며 모든 이러한 개체가 구현하는 universal `Runnable` 인터페이스를 기반으로 합니다.
LCEL에 대해 자세히 알아보려면 [여기](/docs/expression_language)의 문서를 참조하세요.

## 결론

프롬프트, 모델 및 출력 파서 시작하기가 여기까지입니다! 이것은 배울 내용의 표면만 다루었습니다. 자세한 내용은 다음을 참조하세요:

- [프롬프트 섹션](./prompts)에서 프롬프트 템플릿 작업에 대한 정보
- [ChatModel 섹션](./chat)에서 ChatModel 인터페이스에 대한 자세한 정보
- [LLM 섹션](./llms)에서 LLM 인터페이스에 대한 자세한 정보
- [출력 파서 섹션](./output_parsers)에서 다양한 유형의 출력 파서에 대한 정보.
