---
sidebar_position: 0
translated: true
---

# 빠른 시작

이 빠른 시작 가이드에서는 언어 모델 사용의 기본 사항을 다룹니다. LLM(Large Language Model)과 ChatModel의 두 가지 모델 유형을 소개하고, PromptTemplates를 사용하여 이러한 모델의 입력을 형식화하는 방법과 Output Parsers를 사용하여 출력을 처리하는 방법을 다룹니다.

## 모델

이 시작 가이드에서는 다음과 같은 옵션을 제공합니다: Anthropic 또는 OpenAI와 같은 API 사용, 또는 Ollama를 통한 로컬 오픈 소스 모델 사용.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="openai" label="OpenAI" default>

먼저 OpenAI 파트너 패키지를 설치해야 합니다:

```shell
pip install langchain-openai
```

API에 액세스하려면 API 키가 필요합니다. 계정을 만들고 [여기](https://platform.openai.com/account/api-keys)로 이동하여 API 키를 얻을 수 있습니다. 키를 얻으면 다음과 같이 환경 변수로 설정하고자 합니다:

```shell
export OPENAI_API_KEY="..."
```

그런 다음 모델을 초기화할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Quickstart"}]-->
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI

llm = OpenAI()
chat_model = ChatOpenAI(model="gpt-3.5-turbo-0125")
```

환경 변수를 설정하고 싶지 않다면 OpenAI LLM 클래스를 초기화할 때 `api_key` 매개변수를 직접 전달할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}]-->
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(api_key="...")
```

  </TabItem>
  <TabItem value="local" label="Local (using Ollama)">

[Ollama](https://ollama.ai/)를 사용하면 Llama 2와 같은 오픈 소스 대규모 언어 모델을 로컬에서 실행할 수 있습니다.

먼저 [이 지침](https://github.com/jmorganca/ollama)을 따라 Ollama 인스턴스를 설정하고 실행하세요:

* [다운로드](https://ollama.ai/download)
* `ollama pull llama2`를 통해 모델을 가져오세요

그런 다음 Ollama 서버가 실행 중인지 확인하세요. 그 후에 다음과 같이 할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "Ollama", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html", "title": "Quickstart"}, {"imported": "ChatOllama", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html", "title": "Quickstart"}]-->
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

llm = Ollama(model="llama2")
chat_model = ChatOllama()
```

  </TabItem>
  <TabItem value="anthropic" label="Anthropic (chat model only)">

먼저 LangChain x Anthropic 패키지를 가져와야 합니다.

```shell
pip install langchain-anthropic
```

API에 액세스하려면 API 키가 필요합니다. [여기](https://claude.ai/login)에서 계정을 만들어 API 키를 얻을 수 있습니다. 키를 얻으면 다음과 같이 환경 변수로 설정하고자 합니다:

```shell
export ANTHROPIC_API_KEY="..."
```

그런 다음 모델을 초기화할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Quickstart"}]-->
from langchain_anthropic import ChatAnthropic

chat_model = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0.2, max_tokens=1024)
```

환경 변수를 설정하고 싶지 않다면 Anthropic Chat Model 클래스를 초기화할 때 `api_key` 매개변수를 직접 전달할 수 있습니다:

```python
chat_model = ChatAnthropic(api_key="...")
```

  </TabItem>
  <TabItem value="cohere" label="Cohere">

먼저 Cohere 파트너 패키지를 설치해야 합니다:

```shell
pip install langchain-cohere
```

API에 액세스하려면 API 키가 필요합니다. 계정을 만들고 [여기](https://dashboard.cohere.com/api-keys)로 이동하여 API 키를 얻을 수 있습니다. 키를 얻으면 다음과 같이 환경 변수로 설정하고자 합니다:

```shell
export COHERE_API_KEY="..."
```

그런 다음 모델을 초기화할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Quickstart"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere()
```

환경 변수를 설정하고 싶지 않다면 Cohere LLM 클래스를 초기화할 때 `cohere_api_key` 매개변수를 직접 전달할 수 있습니다:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Quickstart"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere(cohere_api_key="...")
```

  </TabItem>
</Tabs>

`llm`과 `chat_model`은 특정 모델에 대한 구성을 나타내는 객체입니다.
`temperature` 등의 매개변수로 초기화할 수 있으며, 이를 전달할 수 있습니다.
이들의 주요 차이점은 입력 및 출력 스키마입니다.
LLM 객체는 문자열을 입력으로 받고 문자열을 출력합니다.
ChatModel 객체는 메시지 목록을 입력으로 받고 메시지를 출력합니다.

LLM과 ChatModel을 호출할 때 이러한 차이를 확인할 수 있습니다.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Quickstart"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

LLM은 문자열을 반환하고 ChatModel은 메시지를 반환합니다.

## Prompt Templates

대부분의 LLM 애플리케이션은 사용자 입력을 직접 LLM에 전달하지 않습니다. 일반적으로 사용자 입력을 더 큰 텍스트 조각, 즉 프롬프트 템플릿에 추가하여 특정 작업에 대한 추가 컨텍스트를 제공합니다.

이전 예에서 모델에 전달한 텍스트에는 회사 이름을 생성하라는 지침이 포함되어 있었습니다. 우리 애플리케이션의 경우 사용자가 회사/제품 설명만 제공하면 모델에 대한 지침을 걱정할 필요가 없으면 좋겠습니다.

PromptTemplates는 이 문제를 해결합니다!
이를 통해 사용자 입력에서 완전히 형식화된 프롬프트로 가는 모든 논리를 번들링할 수 있습니다.
이는 매우 간단하게 시작할 수 있습니다 - 예를 들어, 위의 문자열을 생성하는 프롬프트는 다음과 같습니다:

```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Quickstart"}]-->
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```python
What is a good name for a company that makes colorful socks?
```

그러나 원시 문자열 형식화 대신 이를 사용하면 여러 가지 이점이 있습니다.
변수를 "부분"으로 나눌 수 있습니다 - 즉, 일부 변수만 형식화할 수 있습니다.
서로 다른 템플릿을 결합하여 단일 프롬프트로 구성할 수 있습니다.
이러한 기능에 대한 설명은 [프롬프트 섹션](/docs/modules/model_io/prompts)을 참조하세요.

`PromptTemplate`을 사용하여 메시지 목록을 생성할 수도 있습니다.
이 경우 프롬프트에는 내용에 대한 정보뿐만 아니라 각 메시지(역할, 목록 내 위치 등)에 대한 정보도 포함됩니다.
여기서 가장 일반적인 경우는 `ChatPromptTemplate`이 `ChatMessageTemplate`의 목록이라는 것입니다.
각 `ChatMessageTemplate`에는 해당 `ChatMessage`의 형식 지정 방법, 즉 역할과 내용이 포함되어 있습니다.
아래에서 이를 살펴보겠습니다:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts.chat", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Quickstart"}]-->
from langchain_core.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```pycon
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

`ChatPromptTemplate`은 다른 방식으로도 구축할 수 있습니다. 자세한 내용은 [프롬프트 섹션](/docs/modules/model_io/prompts)을 참조하세요.

## 출력 파서

`OutputParser`는 언어 모델의 원시 출력을 다운스트림에서 사용할 수 있는 형식으로 변환합니다.
주요 유형의 `OutputParser`에는 다음이 포함됩니다:

- `LLM` 텍스트를 구조화된 정보(예: JSON)로 변환
- `ChatMessage`를 문자열로만 변환
- 메시지 외에 호출에서 반환된 추가 정보(예: OpenAI 함수 호출)를 문자열로 변환

이에 대한 전체 정보는 [출력 파서 섹션](/docs/modules/model_io/output_parsers)을 참조하세요.

이 시작 가이드에서는 쉼표로 구분된 값 목록을 구문 분석하는 간단한 파서를 사용합니다.

```python
<!--IMPORTS:[{"imported": "CommaSeparatedListOutputParser", "source": "langchain.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html", "title": "Quickstart"}]-->
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
output_parser.parse("hi, bye")
# >> ['hi', 'bye']
```

## LCEL로 구성하기

이제 이 모든 것을 하나의 체인으로 결합할 수 있습니다.
이 체인은 입력 변수를 받아 프롬프트 템플릿을 통해 프롬프트를 생성하고, 프롬프트를 언어 모델에 전달한 다음, (선택적) 출력 파서를 통해 출력을 전달합니다.
이는 모듈식 논리 조각을 묶는 편리한 방법입니다.
실제로 작동하는 모습을 살펴보겠습니다!

```python
template = "Generate a list of 5 {text}.\n\n{format_instructions}"

chat_prompt = ChatPromptTemplate.from_template(template)
chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())
chain = chat_prompt | chat_model | output_parser
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

`|` 구문을 사용하여 이 구성 요소를 연결하고 있다는 점에 유의하세요.
이 `|` 구문은 LangChain Expression Language(LCEL)에 의해 구동되며 모든 이러한 개체가 구현하는 범용 `Runnable` 인터페이스를 활용합니다.
LCEL에 대해 자세히 알아보려면 [여기](/docs/expression_language/)의 문서를 읽어보세요.

## 결론

프롬프트, 모델 및 출력 파서 시작하기가 여기까지입니다! 이것은 배울 내용의 표면에 불과합니다. 자세한 내용은 다음을 참조하세요:

- [프롬프트 섹션](/docs/modules/model_io/prompts/)에서 프롬프트 템플릿 작업에 대한 정보
- [LLM 섹션](/docs/modules/model_io/llms/)에서 LLM 인터페이스에 대한 자세한 정보
- [ChatModel 섹션](/docs/modules/model_io/chat/)에서 ChatModel 인터페이스에 대한 자세한 정보
- [출력 파서 섹션](/docs/modules/model_io/output_parsers/)에서 다양한 유형의 출력 파서에 대한 정보
