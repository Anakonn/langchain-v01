---
translated: true
---

# 벡터 저장소로 지원되는

`VectorStoreRetrieverMemory`는 벡터 저장소에 메모리를 저장하고 호출될 때마다 가장 "두드러진" 문서 K개를 쿼리합니다.

이는 대부분의 다른 Memory 클래스와 다르게 상호 작용의 순서를 명시적으로 추적하지 않습니다.

이 경우 "문서"는 이전 대화 스니펫입니다. 이는 AI가 대화 초반에 전달받은 관련 정보 조각을 참조하는 데 유용할 수 있습니다.

```python
<!--IMPORTS:[{"imported": "OpenAIEmbeddings", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html", "title": "Backed by a Vector Store"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Backed by a Vector Store"}, {"imported": "VectorStoreRetrieverMemory", "source": "langchain.memory", "docs": "https://api.python.langchain.com/en/latest/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html", "title": "Backed by a Vector Store"}, {"imported": "ConversationChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html", "title": "Backed by a Vector Store"}, {"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Backed by a Vector Store"}]-->
from datetime import datetime
from langchain_openai import OpenAIEmbeddings
from langchain_openai import OpenAI
from langchain.memory import VectorStoreRetrieverMemory
from langchain.chains import ConversationChain
from langchain_core.prompts import PromptTemplate
```

### 벡터 저장소 초기화하기

사용하는 저장소에 따라 이 단계가 다를 수 있습니다. 관련 벡터 저장소 문서를 참조하세요.

```python
<!--IMPORTS:[{"imported": "InMemoryDocstore", "source": "langchain_community.docstore", "docs": "https://api.python.langchain.com/en/latest/docstore/langchain_community.docstore.in_memory.InMemoryDocstore.html", "title": "Backed by a Vector Store"}, {"imported": "FAISS", "source": "langchain_community.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html", "title": "Backed by a Vector Store"}]-->
import faiss

from langchain_community.docstore import InMemoryDocstore
from langchain_community.vectorstores import FAISS


embedding_size = 1536 # Dimensions of the OpenAIEmbeddings
index = faiss.IndexFlatL2(embedding_size)
embedding_fn = OpenAIEmbeddings().embed_query
vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})
```

### `VectorStoreRetrieverMemory` 생성하기

메모리 객체는 벡터 저장소 리트리버에서 인스턴스화됩니다.

```python
# In actual usage, you would set `k` to be a higher value, but we use k=1 to show that
# the vector lookup still returns the semantically relevant information
retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))
memory = VectorStoreRetrieverMemory(retriever=retriever)

# When added to an agent, the memory object can save pertinent information from conversations or used tools
memory.save_context({"input": "My favorite food is pizza"}, {"output": "that's good to know"})
memory.save_context({"input": "My favorite sport is soccer"}, {"output": "..."})
memory.save_context({"input": "I don't the Celtics"}, {"output": "ok"}) #
```

```python
print(memory.load_memory_variables({"prompt": "what sport should i watch?"})["history"])
```

```output
    input: My favorite sport is soccer
    output: ...
```

## 체인에서 사용하기

예를 들어 살펴보겠습니다. 다시 `verbose=True`로 설정하여 프롬프트를 볼 수 있습니다.

```python
llm = OpenAI(temperature=0) # Can be any valid LLM
_DEFAULT_TEMPLATE = """The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
{history}

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: {input}
AI:"""
PROMPT = PromptTemplate(
    input_variables=["history", "input"], template=_DEFAULT_TEMPLATE
)
conversation_with_summary = ConversationChain(
    llm=llm,
    prompt=PROMPT,
    memory=memory,
    verbose=True
)
conversation_with_summary.predict(input="Hi, my name is Perry, what's up?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: My favorite food is pizza
    output: that's good to know

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: Hi, my name is Perry, what's up?
    AI:

    > Finished chain.





    " Hi Perry, I'm doing well. How about you?"
```

```python
# Here, the basketball related content is surfaced
conversation_with_summary.predict(input="what's my favorite sport?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: My favorite sport is soccer
    output: ...

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: what's my favorite sport?
    AI:

    > Finished chain.





    ' You told me earlier that your favorite sport is soccer.'
```

```python
# Even though the language model is stateless, since relevant memory is fetched, it can "reason" about the time.
# Timestamping memories and data is useful in general to let the agent determine temporal relevance
conversation_with_summary.predict(input="Whats my favorite food")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: My favorite food is pizza
    output: that's good to know

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: Whats my favorite food
    AI:

    > Finished chain.





    ' You said your favorite food is pizza.'
```

```python
# The memories from the conversation are automatically stored,
# since this query best matches the introduction chat above,
# the agent is able to 'remember' the user's name.
conversation_with_summary.predict(input="What's my name?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: Hi, my name is Perry, what's up?
    response:  Hi Perry, I'm doing well. How about you?

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: What's my name?
    AI:

    > Finished chain.





    ' Your name is Perry.'
```
