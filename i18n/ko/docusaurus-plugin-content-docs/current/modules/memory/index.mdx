---
sidebar_class_name: hidden
sidebar_position: 3
translated: true
---

# [베타] 메모리

대부분의 LLM 애플리케이션은 대화형 인터페이스를 가지고 있습니다. 대화의 필수적인 구성 요소는 대화 중에 이전에 소개된 정보를 참조할 수 있는 능력입니다.
최소한 대화형 시스템은 직접 과거 메시지의 일정 기간을 액세스할 수 있어야 합니다.
더 복잡한 시스템에는 지속적으로 업데이트되는 세계 모델이 필요하며, 이를 통해 엔티티와 그들의 관계에 대한 정보를 유지할 수 있습니다.

우리는 이전 상호 작용에 대한 정보를 저장할 수 있는 능력을 "메모리"라고 합니다.
LangChain은 시스템에 메모리를 추가하기 위한 많은 유틸리티를 제공합니다.
이러한 유틸리티는 단독으로 사용하거나 체인에 seamlessly 통합될 수 있습니다.

LangChain의 대부분의 메모리 관련 기능은 베타 버전으로 표시됩니다. 이는 두 가지 이유 때문입니다:

1. 대부분의 기능(일부 예외 사항 제외)은 프로덕션 준비가 되지 않았습니다.

2. 대부분의 기능(일부 예외 사항 제외)은 레거시 체인에서 작동하며 새로운 LCEL 구문에서는 작동하지 않습니다.

이에 대한 주요 예외는 `ChatMessageHistory` 기능입니다. 이 기능은 대체로 프로덕션 준비가 되어 있으며 LCEL 실행 가능 파일과 통합됩니다.

- [LCEL 실행 가능 파일](/docs/expression_language/how_to/message_history): LCEL 실행 가능 파일에서 `ChatMessageHistory`를 사용하는 방법에 대한 개요는 이 문서를 참조하십시오.

- [통합](/docs/integrations/memory): `ChatMessageHistory` 통합에 대한 소개는 이 문서를 참조하십시오.

## 소개

메모리 시스템은 두 가지 기본 작업을 지원해야 합니다: 읽기와 쓰기.
모든 체인은 특정 입력을 기대하는 핵심 실행 논리를 정의한다는 점을 기억하십시오.
이러한 입력 중 일부는 사용자로부터 직접 오지만 일부는 메모리에서 올 수 있습니다.
체인은 주어진 실행에서 두 번 메모리 시스템과 상호 작용합니다.
1. 초기 사용자 입력을 받은 후 그러나 핵심 논리를 실행하기 전에 체인은 메모리 시스템에서 읽고 사용자 입력을 보강합니다.
2. 핵심 논리를 실행한 후 그러나 답변을 반환하기 전에 체인은 현재 실행의 입력과 출력을 메모리에 기록하여 향후 실행에서 참조할 수 있게 합니다.

![대화형 인터페이스의 메모리 시스템 READ 및 WRITE 작업을 설명하는 다이어그램.](/img/memory_diagram.png "메모리 시스템 다이어그램")

## 시스템에 메모리 구축하기

메모리 시스템의 두 가지 핵심 설계 결정은 다음과 같습니다:
- 상태가 어떻게 저장되는지
- 상태가 어떻게 쿼리되는지

### 저장: 채팅 메시지 목록

모든 메모리의 기반은 모든 채팅 상호 작용의 기록입니다.
이 모든 것이 직접 사용되지는 않더라도 어떤 형태로든 저장되어야 합니다.
LangChain 메모리 모듈의 핵심 부분은 메모리 내 목록에서 지속적인 데이터베이스에 이르는 이러한 채팅 메시지 저장을 위한 일련의 통합입니다.

- [채팅 메시지 저장](/docs/modules/memory/chat_messages/): 채팅 메시지를 다루는 방법과 제공되는 다양한 통합 방법.

### 쿼리: 채팅 메시지 위의 데이터 구조 및 알고리즘

채팅 메시지 목록을 유지하는 것은 비교적 간단합니다.
그보다 덜 간단한 것은 가장 유용한 메시지 보기를 제공하는 데이터 구조와 알고리즘입니다.

매우 단순한 메모리 시스템은 매 실행마다 가장 최근의 메시지만 반환할 수 있습니다. 약간 더 복잡한 메모리 시스템은 지난 K개의 메시지에 대한 간단한 요약을 반환할 수 있습니다.
더 정교한 시스템은 저장된 메시지에서 엔티티를 추출하고 현재 실행에서 참조된 엔티티에 대한 정보만 반환할 수 있습니다.

각 애플리케이션은 메모리가 쿼리되는 방식에 대한 다른 요구 사항을 가질 수 있습니다. 메모리 모듈은 간단한 메모리 시스템으로 시작하고 필요한 경우 사용자 정의 시스템을 작성할 수 있도록 해야 합니다.

- [메모리 유형](/docs/modules/memory/types/): LangChain이 지원하는 메모리 유형을 구성하는 다양한 데이터 구조와 알고리즘

## 시작하기

LangChain에서 메모리가 실제로 어떻게 작동하는지 살펴보겠습니다.
여기서는 임의의 메모리 클래스와 상호 작용하는 기본 사항을 다룹니다.

`ConversationBufferMemory`를 체인에서 사용하는 방법을 살펴보겠습니다.
`ConversationBufferMemory`는 채팅 메시지 버퍼에 목록을 유지하고 이를 프롬프트 템플릿에 전달하는 매우 간단한 형태의 메모리입니다.

```python
<!--IMPORTS:[{"imported": "ConversationBufferMemory", "source": "langchain.memory", "docs": "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html", "title": "[Beta] Memory"}]-->
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()
memory.chat_memory.add_user_message("hi!")
memory.chat_memory.add_ai_message("what's up?")
```

메모리를 체인에서 사용할 때 이해해야 할 몇 가지 핵심 개념이 있습니다.
여기서는 대부분의 메모리 유형에 유용한 일반적인 개념을 다룹니다.
각 개별 메모리 유형에는 이해해야 할 고유한 매개변수와 개념이 있을 수 있습니다.

### 메모리에서 반환되는 변수

체인에 들어가기 전에 메모리에서 다양한 변수가 읽힙니다.
이러한 변수에는 특정 이름이 있으며 체인이 기대하는 변수와 일치해야 합니다.
`memory.load_memory_variables({})`를 호출하여 이러한 변수를 확인할 수 있습니다.
여기서 우리가 전달하는 빈 사전은 실제 변수에 대한 자리 표시자일 뿐입니다.
사용 중인 메모리 유형이 입력 변수에 따라 달라지는 경우 일부를 전달해야 할 수 있습니다.

```python
memory.load_memory_variables({})
```

```output
    {'history': "Human: hi!\nAI: what's up?"}
```

이 경우 `load_memory_variables`가 단일 키 `history`를 반환하는 것을 볼 수 있습니다.
이는 체인(그리고 아마도 프롬프트)이 `history`라는 이름의 입력을 기대해야 함을 의미합니다.
일반적으로 메모리 클래스의 매개변수를 통해 이 변수를 제어할 수 있습니다.
예를 들어 메모리 변수가 `chat_history` 키에 반환되도록 하려면 다음과 같이 할 수 있습니다:

```python
memory = ConversationBufferMemory(memory_key="chat_history")
memory.chat_memory.add_user_message("hi!")
memory.chat_memory.add_ai_message("what's up?")
```

```output
    {'chat_history': "Human: hi!\nAI: what's up?"}
```

이러한 키를 제어하는 매개변수 이름은 메모리 유형마다 다를 수 있지만 (1) 이것이 제어 가능하다는 점과 (2) 어떻게 제어하는지 이해하는 것이 중요합니다.

### 메모리가 문자열인지 메시지 목록인지 여부

채팅 메시지 목록을 반환하는 가장 일반적인 메모리 유형 중 하나입니다.
이는 단일 문자열로 반환되거나 모두 연결되어 있을 수 있습니다(LLM에 전달될 때 유용함)
또는 ChatMessages 목록(ChatModels에 전달될 때 유용함).

기본적으로 단일 문자열로 반환됩니다.
메시지 목록으로 반환하려면 `return_messages=True`를 설정할 수 있습니다.

```python
memory = ConversationBufferMemory(return_messages=True)
memory.chat_memory.add_user_message("hi!")
memory.chat_memory.add_ai_message("what's up?")
```

```output
    {'history': [HumanMessage(content='hi!', additional_kwargs={}, example=False),
  AIMessage(content='what's up?', additional_kwargs={}, example=False)]}
```

### 메모리에 저장되는 키

체인은 종종 여러 입력/출력 키를 가져오거나 반환합니다.
이러한 경우 채팅 메시지 기록에 저장할 키를 어떻게 알 수 있습니까?
이는 일반적으로 메모리 유형의 `input_key` 및 `output_key` 매개변수로 제어할 수 있습니다.
이들은 기본적으로 `None`이며, 입력/출력 키가 하나만 있는 경우 그것을 사용하는 것으로 알려져 있습니다.
그러나 여러 입력/출력 키가 있는 경우 사용할 이름을 반드시 지정해야 합니다.

### 엔드 투 엔드 예제

마지막으로 체인에서 이를 사용하는 방법을 살펴보겠습니다.
`LLMChain`을 사용하고 LLM과 ChatModel로 작업하는 방법을 보여드리겠습니다.

#### LLM 사용

```python
<!--IMPORTS:[{"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "[Beta] Memory"}, {"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "[Beta] Memory"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "[Beta] Memory"}, {"imported": "ConversationBufferMemory", "source": "langchain.memory", "docs": "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html", "title": "[Beta] Memory"}]-->
from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory


llm = OpenAI(temperature=0)
# Notice that "chat_history" is present in the prompt template
template = """You are a nice chatbot having a conversation with a human.

Previous conversation:
{chat_history}

New human question: {question}
Response:"""
prompt = PromptTemplate.from_template(template)
# Notice that we need to align the `memory_key`
memory = ConversationBufferMemory(memory_key="chat_history")
conversation = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True,
    memory=memory
)
```

```python
# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory
conversation({"question": "hi"})
```

#### ChatModel 사용

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "[Beta] Memory"}, {"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "[Beta] Memory"}, {"imported": "MessagesPlaceholder", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html", "title": "[Beta] Memory"}, {"imported": "SystemMessagePromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.SystemMessagePromptTemplate.html", "title": "[Beta] Memory"}, {"imported": "HumanMessagePromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.HumanMessagePromptTemplate.html", "title": "[Beta] Memory"}, {"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "[Beta] Memory"}, {"imported": "ConversationBufferMemory", "source": "langchain.memory", "docs": "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html", "title": "[Beta] Memory"}]-->
from langchain_openai import ChatOpenAI
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory


llm = ChatOpenAI()
prompt = ChatPromptTemplate(
    messages=[
        SystemMessagePromptTemplate.from_template(
            "You are a nice chatbot having a conversation with a human."
        ),
        # The `variable_name` here is what must align with memory
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{question}")
    ]
)
# Notice that we `return_messages=True` to fit into the MessagesPlaceholder
# Notice that `"chat_history"` aligns with the MessagesPlaceholder name.
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
conversation = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True,
    memory=memory
)
```

```python
# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory
conversation({"question": "hi"})
```

## 다음 단계

이것으로 시작하는 데 필요한 내용이 모두 포함되어 있습니다!
더 고급 주제인 사용자 정의 메모리, 다중 메모리 등에 대한 연습은 다른 섹션을 참조하세요.
