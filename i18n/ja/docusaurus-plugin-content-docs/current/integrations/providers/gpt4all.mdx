---
translated: true
---

# GPT4All

このページでは、LangChainの中の`GPT4All`ラッパーの使い方を説明します。このチュートリアルは、インストールとセットアップ、そして使用例の2つの部分に分かれています。

## インストールとセットアップ

- `pip install gpt4all`でPythonパッケージをインストールします
- [GPT4Allモデル](https://gpt4all.io/index.html)をダウンロードし、お好きなディレクトリに置きます

この例では、`mistral-7b-openorca.Q4_0.gguf`(最高の高速チャットモデル)を使用します:

```bash
mkdir models
wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf
```

## 使用方法

### GPT4All

GPT4Allラッパーを使うには、事前学習済みのモデルファイルのパスとモデルの設定を指定する必要があります。

```python
<!--IMPORTS:[{"imported": "GPT4All", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.gpt4all.GPT4All.html", "title": "GPT4All"}]-->
from langchain_community.llms import GPT4All

# Instantiate the model. Callbacks support token-wise streaming
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text
response = model.invoke("Once upon a time, ")
```

生成パラメータ、例えば n_predict、temp、top_p、top_kなどをカスタマイズすることもできます。

モデルの予測をストリーミングするには、CallbackManagerを追加します。

```python
<!--IMPORTS:[{"imported": "GPT4All", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.gpt4all.GPT4All.html", "title": "GPT4All"}, {"imported": "StreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html", "title": "GPT4All"}, {"imported": "StreamlitCallbackHandler", "source": "langchain.callbacks.streamlit", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streamlit.StreamlitCallbackHandler.html", "title": "GPT4All"}]-->
from langchain_community.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# There are many CallbackHandlers supported, such as
# from langchain.callbacks.streamlit import StreamlitCallbackHandler

callbacks = [StreamingStdOutCallbackHandler()]
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text. Tokens are streamed through the callback manager.
model("Once upon a time, ", callbacks=callbacks)
```

## モデルファイル

モデルファイルのダウンロードリンクは[https://gpt4all.io/](https://gpt4all.io/index.html)にあります。

詳細なチュートリアルは[このノートブック](/docs/integrations/llms/gpt4all)をご覧ください。
