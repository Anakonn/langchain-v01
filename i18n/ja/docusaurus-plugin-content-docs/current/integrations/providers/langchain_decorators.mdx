---
translated: true
---

# LangChain ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ âœ¨

~~~
å…è²¬äº‹é …: `LangChain ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼` ã¯ LangChain ãƒãƒ¼ãƒ ã«ã‚ˆã£ã¦ä½œæˆã•ã‚ŒãŸã‚‚ã®ã§ã¯ãªãã€LangChain ãƒãƒ¼ãƒ ã«ã‚ˆã£ã¦ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã§ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚
~~~

>`LangChain ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼` ã¯ LangChain ã®ä¸Šã«æ§‹ç¯‰ã•ã‚ŒãŸãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã€ã‚«ã‚¹ã‚¿ãƒ ã® LangChain ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒã‚§ãƒ¼ãƒ³ã‚’æ›¸ããŸã‚ã®æ§‹æ–‡ç³–è¡£ ğŸ­ ã‚’æä¾›ã—ã¾ã™ã€‚
>
>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€å•é¡Œã€ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã¯ã€ã“ã¡ã‚‰ã§ issue ã‚’ä½œæˆã—ã¦ãã ã•ã„:
>[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)

ä¸»ãªåŸå‰‡ã¨åˆ©ç‚¹:

- ã‚ˆã‚Š `Pythonic` ãªæ›¸ãæ–¹
- ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã§ä¸­æ–­ã•ã‚Œã‚‹ã“ã¨ã®ãªã„ã€è¤‡æ•°è¡Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ãã“ã¨ãŒã§ãã‚‹
- IDE ã®çµ„ã¿è¾¼ã¿ã‚µãƒãƒ¼ãƒˆã‚’æ´»ç”¨ã—ã¦ã€**ãƒ’ãƒ³ãƒˆè¡¨ç¤º**ã€**å‹ãƒã‚§ãƒƒã‚¯**ã€**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—**ã‚’ä½¿ã£ã¦ã€ç´ æ—©ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ç¢ºèªã§ãã‚‹
- ğŸ¦œğŸ”— LangChain ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ã¦ã®æ©Ÿèƒ½ã‚’æ´»ç”¨ã§ãã‚‹
- **ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼**ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–“ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã‚¯ãƒ©ã‚¹ã«ãƒã‚¤ãƒ³ãƒ‰ã§ãã‚‹

**LangChain ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ âœ¨** ã‚’ä½¿ã£ãŸã‚·ãƒ³ãƒ—ãƒ«ãªä¾‹ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

```python

@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:
    """
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    """
    return

# run it naturally
write_me_short_post(topic="starwars")
# or
write_me_short_post(topic="starwars", platform="redit")
```

# ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install langchain_decorators
```

## ä¾‹

è‰¯ã„å§‹ã¾ã‚Šæ–¹ã¯ã€ä»¥ä¸‹ã®ä¾‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã™:
 - [Jupyter ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)
 - [Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)

# ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å®šç¾©

ã“ã“ã§ã¯ã€é–¢æ•°ã« `llm_prompt` ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä»˜ã‘ã‚‹ã“ã¨ã§ã€ãã‚Œã‚’ LLMChain ã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚å®Ÿè¡Œã™ã‚‹ã«ã¯

æ¨™æº–ã® LLMChain ã¯ã€å˜ã« inputs_variables ã¨ prompt ã ã‘ã§ã¯ãªãã€ã‚‚ã£ã¨å¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚ã“ã®å®Ÿè£…ã®è©³ç´°ã¯ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«éš ã•ã‚Œã¦ã„ã¾ã™ã€‚
å‹•ä½œã®ä»•çµ„ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

1. **ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š**ã‚’ä½¿ã†:

```python
# define global settings for all prompty (if not set - chatGPT is the current default)
from langchain_decorators import GlobalSettings

GlobalSettings.define_settings(
    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally
    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming
)
```

2. äº‹å‰å®šç¾©ã•ã‚ŒãŸ **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—**ã‚’ä½¿ã†

```python
#You can change the default prompt types
from langchain_decorators import PromptTypes, PromptTypeSettings

PromptTypes.AGENT_REASONING.llm = ChatOpenAI()

# Or you can just define your own ones:
class MyCustomPromptTypes(PromptTypes):
    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))

@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4)
def write_a_complicated_code(app_idea:str)->str:
    ...

```

3. ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ä¸­ã§ **è¨­å®šã‚’ç›´æ¥å®šç¾©ã™ã‚‹**

```python
<!--IMPORTS:[{"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "LangChain Decorators \u2728"}]-->
from langchain_openai import OpenAI

@llm_prompt(
    llm=OpenAI(temperature=0.7),
    stop_tokens=["\nObservation"],
    ...
    )
def creative_writer(book_title:str)->str:
    ...
```

## ãƒ¡ãƒ¢ãƒªã‚„ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ¸¡ã™:

ã“ã‚Œã‚‰ã‚’æ¸¡ã™ã«ã¯ã€é–¢æ•°ã®ä¸­ã§å®£è¨€ã™ã‚‹ã ã‘ã§ã™ (ã¾ãŸã¯kwargsã‚’ä½¿ã£ã¦ä½•ã§ã‚‚æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™)

```python

@llm_prompt()
async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):
    """
    {history_key}
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass

await write_me_short_post(topic="old movies")

```

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®ç°¡ç•¥åŒ–

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æ´»ç”¨ã—ãŸã„å ´åˆ:
 - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’éåŒæœŸé–¢æ•°ã¨ã—ã¦å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
 - ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚ªãƒ³ã«ã™ã‚‹ã‹ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®PromptTypeã‚’å®šç¾©ã™ã‚‹
 - StreamingContextã‚’ä½¿ã£ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹

ã“ã®ã‚ˆã†ã«ã—ã¦ã€ã©ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã™ã¹ãã‹ã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§ã€LLMã®é¸æŠã‚„ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ä½œæˆãƒ»é…å¸ƒã‚’æ°—ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®ã‚ªãƒ³/ã‚ªãƒ•ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã ã‘ã§ã™ã€‚

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§å‘¼ã³å‡ºã•ã‚ŒãŸå ´åˆã«ã®ã¿ç™ºç”Ÿã—ã¾ã™ã€‚ãã“ã§ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹ç°¡å˜ãªé–¢æ•°ã‚’å®šç¾©ã§ãã¾ã™ã€‚

```python
# this code example is complete and should run as it is

from langchain_decorators import StreamingContext, llm_prompt

# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)
# note that only async functions can be streamed (will get an error if it's not)
@llm_prompt(capture_stream=True)
async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass



# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real world
tokens=[]
def capture_stream_func(new_token:str):
    tokens.append(new_token)

# if we want to capture the stream, we need to wrap the execution into StreamingContext...
# this will allow us to capture the stream even if the prompt call is hidden inside higher level method
# only the prompts marked with capture_stream will be captured here
with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):
    result = await run_prompt()
    print("Stream finished ... we can distinguish tokens thanks to alternating colors")


print("\nWe've captured",len(tokens),"tokensğŸ‰\n")
print("Here is the result:")
print(result)
```

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å®£è¨€

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯é–¢æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã«ãªã‚Šã¾ã™ãŒã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–

ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸€éƒ¨ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©ã§ã‚ã‚‹ã“ã¨ã‚’æŒ‡å®šã™ã‚‹ã«ã¯ã€`<prompt>` è¨€èªã‚¿ã‚°ã‚’ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ã„ã¾ã™ã€‚

```python
@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.

    It needs to be a code block, marked as a `<prompt>` language
    ```<prompt>
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    return
```

## ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚»ãƒƒãƒˆã¨ã—ã¦å®šç¾©ã™ã‚‹ã®ãŒéå¸¸ã«ä¾¿åˆ©ã§ã™ã€‚ã“ã†ã™ã‚‹ã«ã¯:

```python
@llm_prompt
def simulate_conversation(human_input:str, agent_role:str="a pirate"):
    """
    ## System message
     - note the `:system` sufix inside the <prompt:_role_> tag


    ```<prompt:system>
    You are a {agent_role} hacker. You mus act like one.
    You reply always in code, using python or javascript code block...
    for example:

    ... do not reply with anything else.. just with code - respecting your role.
    ```

    # human message
    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)
    ``` <prompt:user>
    Helo, who are you
    ```
    a reply:


    ``` <prompt:assistant>
    \``` python <<- escaping inner code block with \ that should be part of the prompt
    def hello():
        print("Argh... hello you pesky pirate")
    \```
    ```

    we can also add some history using placeholder
    ```<prompt:placeholder>
    {history}
    ```
    ```<prompt:user>
    {human_input}
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    pass

```

ã“ã“ã§ã®ãƒ­ãƒ¼ãƒ«ã¯ã€chatGPTãªã©ã®ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ãƒ†ã‚£ãƒ–ã®ãƒ­ãƒ¼ãƒ« (assistantã€userã€system) ã§ã™ã€‚

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³

- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’å®Œå…¨ã«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦å®šç¾©ã§ãã¾ã™
- ãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å…¥åŠ›ãŒãªã„å ´åˆã€ãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ãŒãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã¾ã›ã‚“

æ§‹æ–‡ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

```python
@llm_prompt
def prompt_with_optional_partials():
    """
    this text will be rendered always, but

    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}

    you can also place it in between the words
    this too will be rendered{? , but
        this  block will be rendered only if {this_value} and {this_value}
        is not empty?} !
    """
```

# å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼

- `llm_prompt` ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯ã€å‡ºåŠ›ã®å‹ã«åŸºã¥ã„ã¦æœ€é©ãªå‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’è‡ªå‹•çš„ã«æ¤œå‡ºã—ã¾ã™ (è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ç”Ÿã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¾ã™)
- ãƒªã‚¹ãƒˆã€è¾æ›¸ã€Pydanticå‡ºåŠ›ã‚‚ãƒã‚¤ãƒ†ã‚£ãƒ–ã«ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ (è‡ªå‹•çš„ã«)

```python
# this code example is complete and should run as it is

from langchain_decorators import llm_prompt

@llm_prompt
def write_name_suggestions(company_business:str, count:int)->list:
    """ Write me {count} good name suggestions for company that {company_business}
    """
    pass

write_name_suggestions(company_business="sells cookies", count=5)
```

## ã‚ˆã‚Šè¤‡é›‘ãªæ§‹é€ 

è¾æ›¸/Pydantic ã®å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒ†ã‚£ãƒ³ã‚°å‘½ä»¤ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™...
ã“ã‚Œã¯é¢å€’ãªä½œæ¥­ã«ãªã‚‹ãŸã‚ã€å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã«ãƒ¢ãƒ‡ãƒ« (Pydantic) ã«åŸºã¥ã„ã¦å‘½ä»¤ã‚’ç”Ÿæˆã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from langchain_decorators import llm_prompt
from pydantic import BaseModel, Field


class TheOutputStructureWeExpect(BaseModel):
    name:str = Field (description="The name of the company")
    headline:str = Field( description="The description of the company (for landing page)")
    employees:list[str] = Field(description="5-8 fake employee names with their positions")

@llm_prompt()
def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:
    """ Generate a fake company that {company_business}
    {FORMAT_INSTRUCTIONS}
    """
    return

company = fake_company_generator(company_business="sells cookies")

# print the result nicely formatted
print("Company name: ",company.name)
print("company headline: ",company.headline)
print("company employees: ",company.employees)

```

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãƒã‚¤ãƒ³ãƒ‰ã™ã‚‹

```python
from pydantic import BaseModel
from langchain_decorators import llm_prompt

class AssistantPersonality(BaseModel):
    assistant_name:str
    assistant_role:str
    field:str

    @property
    def a_property(self):
        return "whatever"

    def hello_world(self, function_kwarg:str=None):
        """
        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method
        """


    @llm_prompt
    def introduce_your_self(self)->str:
        """
        ```Â <prompt:system>
        You are an assistant named {assistant_name}.
        Your role is to act as {assistant_role}
        ```
        ```<prompt:user>
        Introduce your self (in less than 20 words)
        ```
        """



personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")

print(personality.introduce_your_self(personality))
```

# ãã®ä»–ã®ä¾‹:

- ã“ã‚Œã‚‰ã®ä¾‹ã¨ã€ã„ãã¤ã‹ã®è¿½åŠ ã®ä¾‹ã¯ [ã“ã¡ã‚‰ã®Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)ã«ã‚‚ç”¨æ„ã•ã‚Œã¦ã„ã¾ã™
- [ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å†å®Ÿè£…](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp)ã‚‚ã€ç´”ç²‹ãªLangChainãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã£ã¦è¡Œã£ã¦ã„ã¾ã™
