---
sidebar_position: 0
title: ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ
translated: true
---

ã“ã®ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã§ã¯ã€**é–¢æ•°/ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—**ãŒå¯èƒ½ãª[ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«](/docs/modules/model_io/chat/)ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

:::important
**é–¢æ•°/ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—**ã‚’ä½¿ã£ãŸæŠ½å‡ºã¯ã€[**é–¢æ•°/ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«**](/docs/modules/model_io/chat/function_calling)ã§ã®ã¿å‹•ä½œã—ã¾ã™ã€‚
:::

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

[æ§‹é€ åŒ–å‡ºåŠ›](/docs/modules/model_io/chat/structured_output)ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã€**é–¢æ•°/ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—**ãŒå¯èƒ½ãªLLMã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã€ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¾ã—ã‚‡ã†!

```python
!pip install langchain

# Install a model capable of tool calling
# pip install langchain-openai
# pip install langchain-mistralai
# pip install langchain-fireworks

# Set env vars for the relevant model or load from a .env file:
# import dotenv
# dotenv.load_dotenv()
```

## ã‚¹ã‚­ãƒ¼ãƒ

ã¾ãšã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸã„æƒ…å ±ã‚’è¨˜è¿°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Pydanticã‚’ä½¿ç”¨ã—ã¦ã€å€‹äººæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
from typing import Optional

from langchain_core.pydantic_v1 import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    # ^ Doc-string for the entity Person.
    # This doc-string is sent to the LLM as the description of the schema Person,
    # and it can help to improve extraction results.

    # Note that:
    # 1. Each field is an `optional` -- this allows the model to decline to extract it!
    # 2. Each field has a `description` -- this description is used by the LLM.
    # Having a good description can help improve extraction results.
    name: Optional[str] = Field(default=None, description="The name of the person")
    hair_color: Optional[str] = Field(
        default=None, description="The color of the peron's hair if known"
    )
    height_in_meters: Optional[str] = Field(
        default=None, description="Height measured in meters"
    )
```

ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã™ã‚‹éš›ã®2ã¤ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

1. **å±æ€§**ã¨**ã‚¹ã‚­ãƒ¼ãƒ**è‡ªä½“ã‚’æ–‡æ›¸åŒ–ã™ã‚‹ã“ã¨: ã“ã®æƒ…å ±ã¯LLMã«é€ä¿¡ã•ã‚Œã€æƒ…å ±æŠ½å‡ºã®å“è³ªå‘ä¸Šã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
2. LLMã«æƒ…å ±ã‚’ä½œã‚Šå‡ºã•ã›ãªã„ã“ã¨! ä¸Šè¨˜ã§ã¯å±æ€§ã«`Optional`ã‚’ä½¿ç”¨ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã«æŠ½å‡ºã™ã¹ãæƒ…å ±ãŒãªã„å ´åˆã«LLMãŒ`None`ã‚’å‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚

:::important
æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã«ã¯ã€ã‚¹ã‚­ãƒ¼ãƒã‚’ååˆ†ã«æ–‡æ›¸åŒ–ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã«æŠ½å‡ºã™ã¹ãæƒ…å ±ãŒãªã„å ´åˆã«LLMã«çµæœã‚’è¿”ã•ã›ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
:::

## æŠ½å‡ºå™¨

ä¸Šã§å®šç¾©ã—ãŸã‚¹ã‚­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¦ã€æƒ…å ±æŠ½å‡ºå™¨ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚

```python
from typing import Optional

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI

# Define a custom prompt to provide instructions and any additional context.
# 1) You can add examples into the prompt template to improve extraction quality
# 2) Introduce additional parameters to take context into account (e.g., include metadata
#    about the document from which the text was extracted.)
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert extraction algorithm. "
            "Only extract relevant information from the text. "
            "If you do not know the value of an attribute asked to extract, "
            "return null for the attribute's value.",
        ),
        # Please see the how-to about improving performance with
        # reference examples.
        # MessagesPlaceholder('examples'),
        ("human", "{text}"),
    ]
)
```

é–¢æ•°/ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

[æ§‹é€ åŒ–å‡ºåŠ›](/docs/modules/model_io/chat/structured_output)ã®ãƒšãƒ¼ã‚¸ã§ã€ã“ã®APIã§ä½¿ç”¨ã§ãã‚‹ã„ãã¤ã‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

```python
from langchain_mistralai import ChatMistralAI

llm = ChatMistralAI(model="mistral-large-latest", temperature=0)

runnable = prompt | llm.with_structured_output(schema=Person)
```

è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†

```python
text = "Alan Smith is 6 feet tall and has blond hair."
runnable.invoke({"text": text})
```

```output
Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288')
```

:::important

æŠ½å‡ºã¯ç”Ÿæˆçš„ã§ã™ ğŸ¤¯

LLMã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãªã®ã§ã€ãƒ•ã‚£ãƒ¼ãƒˆã§æä¾›ã•ã‚ŒãŸèº«é•·ã‚’ãƒ¡ãƒ¼ãƒˆãƒ«ã§æ­£ã—ãæŠ½å‡ºã™ã‚‹ãªã©ã€ã‹ãªã‚Šã‚¯ãƒ¼ãƒ«ãªã“ã¨ãŒã§ãã¾ã™!
:::

## è¤‡æ•°ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£

**ã»ã¨ã‚“ã©ã®å ´åˆ**ã€å˜ä¸€ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã§ã¯ãªãã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Pydanticã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚¹ãƒˆã™ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚’ç°¡å˜ã«å®Ÿç¾ã§ãã¾ã™ã€‚

```python
from typing import List, Optional

from langchain_core.pydantic_v1 import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    # ^ Doc-string for the entity Person.
    # This doc-string is sent to the LLM as the description of the schema Person,
    # and it can help to improve extraction results.

    # Note that:
    # 1. Each field is an `optional` -- this allows the model to decline to extract it!
    # 2. Each field has a `description` -- this description is used by the LLM.
    # Having a good description can help improve extraction results.
    name: Optional[str] = Field(default=None, description="The name of the person")
    hair_color: Optional[str] = Field(
        default=None, description="The color of the peron's hair if known"
    )
    height_in_meters: Optional[str] = Field(
        default=None, description="Height measured in meters"
    )


class Data(BaseModel):
    """Extracted data about people."""

    # Creates a model so that we can extract multiple entities.
    people: List[Person]
```

:::important
æŠ½å‡ºãŒå®Œç’§ã§ã¯ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**å‚ç…§ä¾‹**ã‚’ä½¿ç”¨ã—ã¦æŠ½å‡ºã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã¨ã€**ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”è¦§ãã ã•ã„ã€‚
:::

```python
runnable = prompt | llm.with_structured_output(schema=Data)
text = "My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me."
runnable.invoke({"text": text})
```

```output
Data(people=[Person(name='Jeff', hair_color=None, height_in_meters=None), Person(name='Anna', hair_color=None, height_in_meters=None)])
```

:::tip
ã‚¹ã‚­ãƒ¼ãƒãŒ**è¤‡æ•°ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£**ã®æŠ½å‡ºã«å¯¾å¿œã—ã¦ã„ã‚‹å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã§ã‚‚ã€ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã“ã¨ã§ã€**ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒå­˜åœ¨ã—ãªã„**ã“ã¨ã‚’ç¤ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚

ã“ã‚Œã¯é€šå¸¸**è‰¯ã„ã“ã¨**ã§ã™! å¿…é ˆã®å±æ€§ã‚’æŒã¤ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŒ‡å®šã§ãã¾ã™ãŒã€å¿…ãšã—ã‚‚ã“ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¤œå‡ºã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
:::

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

LangChainã«ã‚ˆã‚‹æŠ½å‡ºã®åŸºæœ¬ã‚’ç†è§£ã—ãŸã‚‰ã€ã‚¬ã‚¤ãƒ‰ã®æ®‹ã‚Šã®éƒ¨åˆ†ã«é€²ã‚“ã§ã„ãæº–å‚™ãŒã§ãã¾ã—ãŸ:

- [ä¾‹ã‚’è¿½åŠ ã™ã‚‹](/docs/use_cases/extraction/how_to/examples): **å‚ç…§ä¾‹**ã‚’ä½¿ç”¨ã—ã¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚
- [é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹](/docs/use_cases/extraction/how_to/handle_long_text): LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«åã¾ã‚‰ãªã„ãƒ†ã‚­ã‚¹ãƒˆã«ã©ã®ã‚ˆã†ã«å¯¾å‡¦ã™ã¹ãã§ã™ã‹?
- [ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹](/docs/use_cases/extraction/how_to/handle_files): PDFãªã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã™ã‚‹éš›ã®LangChainãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã¨ãƒ‘ãƒ¼ã‚µãƒ¼ã®ä½¿ç”¨ä¾‹ã€‚
- [ãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã™ã‚‹](/docs/use_cases/extraction/how_to/parse): **ãƒ„ãƒ¼ãƒ«/é–¢æ•°ã®å‘¼ã³å‡ºã—**ã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãƒ¢ãƒ‡ãƒ«ã§æŠ½å‡ºã™ã‚‹éš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚
- [ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³](/docs/use_cases/extraction/guidelines): æŠ½å‡ºã‚¿ã‚¹ã‚¯ã®è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‚
