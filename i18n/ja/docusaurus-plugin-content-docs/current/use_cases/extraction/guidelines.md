---
sidebar_position: 5
title: ガイドライン
translated: true
---

抽出結果の品質は多くの要因に依存します。

最高のパフォーマンスを引き出すためのガイドラインは以下の通りです:

* モデルの温度を `0` に設定してください。
* プロンプトを改善してください。プロンプトは正確かつ簡潔である必要があります。
* スキーマを文書化してください: LLMに詳細な情報を提供するためにスキーマを文書化してください。
* 参照例を提供してください! 多様な例は役立ちます。何も抽出されるべきでない例も含めてください。
* 多くの例がある場合は、リトリーバーを使用して最も関連性の高い例を取得してください。
* 利用可能な最高のLLM/Chatモデル(例: gpt-4、claude-3など)でベンチマークを行ってください - 最新のものを使用しているかどうかはモデル提供者に確認してください。
* スキーマが非常に大きい場合は、複数の小さなスキーマに分割し、個別の抽出を行ってから結果をマージしてください。
* スキーマでモデルが情報を拒否できるようにしてください。そうしないと、モデルは情報を作り出さざるを得なくなります。
* 検証/修正ステップを追加してください(LLMに結果を修正または検証させる)。

## ベンチマーク

* [LangSmith 🦜️🛠️](https://docs.smith.langchain.com/)を使用して、ユースケース用のデータを作成してベンチマークを行ってください。
* LLMは十分ですか? [langchain-benchmarks 🦜💯 ](https://github.com/langchain-ai/langchain-benchmarks)を使用して既存のデータセットでLLMをテストしてください。

## 覚えておいてください! 😶‍🌫️

* LLMは素晴らしいですが、すべてのケースに必要というわけではありません。単一の構造化されたソース(例: LinkedIn)から情報を抽出する場合、LLMを使用するのは適切ではありません - 従来のWebスクレイピングの方が安価で信頼性が高くなります。

* **人間によるループ** 完璧な品質が必要な場合は、人間によるループを計画する必要があります。最高のLLMでも、複雑な抽出タスクでは間違いを犯す可能性があります。
