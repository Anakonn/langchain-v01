---
sidebar_position: 2
translated: true
---

# ステップバック プロンプティング

時には、質問の具体的な内容によって検索品質やモデル生成が妨げられることがあります。これに対処する1つの方法は、まず抽象的な「ステップバック」の質問を生成し、元の質問とステップバック質問の両方に基づいて検索することです。

例えば、「なぜ私の LangGraph エージェントの astream_events が {DESIRED_OUTPUT} ではなく {LONG_TRACE} を返すのですか」という形式の質問をすると、「LangGraph エージェントの astream_events がどのように機能するか」といった、より一般的な質問で検索すると、より関連性の高い文書が検索できる可能性があります。

LangChain YouTube ビデオの Q&A ボットのコンテキストでステップバック プロンプティングを使う方法を見ていきましょう。

## セットアップ

#### 依存関係のインストール

```python
# %pip install -qU langchain-core langchain-openai
```

#### 環境変数の設定

この例では OpenAI を使用します:

```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

## ステップバック質問の生成

良いステップバック質問を生成するには、良いプロンプトを書くことが重要です:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

system = """You are an expert at taking a specific question and extracting a more generic question that gets at \
the underlying principles needed to answer the specific question.

You will be asked about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.

LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.
LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.
LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.
LangSmith is a platform that makes it easy to trace and test LLM applications.

Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question. \

If you don't recognize a word or acronym to not try to rewrite it.

Write concise questions."""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
step_back = prompt | llm | StrOutputParser()
```

```python
question = (
    "I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search. "
    "How do I get just the LLM calls from the event stream"
)
result = step_back.invoke({"question": question})
print(result)
```

```output
What are the specific methods or functions provided by LangGraph for extracting LLM calls from an event stream that includes various types of interactions and data sources?
```

## ステップバック質問と元の質問を返す

再現率を高めるために、ステップバック質問と元の質問の両方に基づいて文書を検索したいと思います。両方を簡単に返すことができます:

```python
from langchain_core.runnables import RunnablePassthrough

step_back_and_original = RunnablePassthrough.assign(step_back=step_back)

step_back_and_original.invoke({"question": question})
```

```output
{'question': 'I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search. How do I get just the LLM calls from the event stream',
 'step_back': 'What are the specific methods or functions provided by LangGraph for extracting LLM calls from an event stream generated by an agent built using external tools like Gemini Pro, vectorstores, and DuckDuckGo search?'}
```

## 関数呼び出しを使って構造化された出力を得る

この手法を他のクエリ分析手法と組み合わせる場合は、関数呼び出しを使って構造化されたクエリオブジェクトを取得することになるでしょう。ステップバック プロンプティングでも関数呼び出しを使うことができます:

```python
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.pydantic_v1 import BaseModel, Field


class StepBackQuery(BaseModel):
    step_back_question: str = Field(
        ...,
        description="Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question.",
    )


llm_with_tools = llm.bind_tools([StepBackQuery])
hyde_chain = prompt | llm_with_tools | PydanticToolsParser(tools=[StepBackQuery])
hyde_chain.invoke({"question": question})
```

```output
[StepBackQuery(step_back_question='What are the steps to filter and extract specific types of calls from an event stream in a Python framework like LangGraph?')]
```
