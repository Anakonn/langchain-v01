---
translated: true
---

# ベクトルストアでバックアップ

`VectorStoreRetrieverMemory`はベクトルストアにメモリを保存し、呼び出されるたびに最も"重要な"ドキュメントのトップKを照会します。

これは、他のほとんどのMemoryクラスとは異なり、相互作用の順序を明示的に追跡しません。

この場合、"ドキュメント"は以前の会話スニペットです。これにより、会話の早い段階で AI に伝えられた関連情報を参照することができます。

```python
<!--IMPORTS:[{"imported": "OpenAIEmbeddings", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html", "title": "Backed by a Vector Store"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Backed by a Vector Store"}, {"imported": "VectorStoreRetrieverMemory", "source": "langchain.memory", "docs": "https://api.python.langchain.com/en/latest/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html", "title": "Backed by a Vector Store"}, {"imported": "ConversationChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html", "title": "Backed by a Vector Store"}, {"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Backed by a Vector Store"}]-->
from datetime import datetime
from langchain_openai import OpenAIEmbeddings
from langchain_openai import OpenAI
from langchain.memory import VectorStoreRetrieverMemory
from langchain.chains import ConversationChain
from langchain_core.prompts import PromptTemplate
```

### ベクトルストアを初期化する

使用するストアによって、この手順は異なる場合があります。詳細については、関連するベクトルストアのドキュメントを参照してください。

```python
<!--IMPORTS:[{"imported": "InMemoryDocstore", "source": "langchain_community.docstore", "docs": "https://api.python.langchain.com/en/latest/docstore/langchain_community.docstore.in_memory.InMemoryDocstore.html", "title": "Backed by a Vector Store"}, {"imported": "FAISS", "source": "langchain_community.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html", "title": "Backed by a Vector Store"}]-->
import faiss

from langchain_community.docstore import InMemoryDocstore
from langchain_community.vectorstores import FAISS


embedding_size = 1536 # Dimensions of the OpenAIEmbeddings
index = faiss.IndexFlatL2(embedding_size)
embedding_fn = OpenAIEmbeddings().embed_query
vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})
```

### `VectorStoreRetrieverMemory`を作成する

メモリオブジェクトは、任意のベクトルストアリトリーバーから初期化されます。

```python
# In actual usage, you would set `k` to be a higher value, but we use k=1 to show that
# the vector lookup still returns the semantically relevant information
retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))
memory = VectorStoreRetrieverMemory(retriever=retriever)

# When added to an agent, the memory object can save pertinent information from conversations or used tools
memory.save_context({"input": "My favorite food is pizza"}, {"output": "that's good to know"})
memory.save_context({"input": "My favorite sport is soccer"}, {"output": "..."})
memory.save_context({"input": "I don't the Celtics"}, {"output": "ok"}) #
```

```python
print(memory.load_memory_variables({"prompt": "what sport should i watch?"})["history"])
```

```output
    input: My favorite sport is soccer
    output: ...
```

## チェーンで使用する

例を見ていきましょう。ここでも `verbose=True` を設定して、プロンプトを確認できるようにします。

```python
llm = OpenAI(temperature=0) # Can be any valid LLM
_DEFAULT_TEMPLATE = """The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
{history}

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: {input}
AI:"""
PROMPT = PromptTemplate(
    input_variables=["history", "input"], template=_DEFAULT_TEMPLATE
)
conversation_with_summary = ConversationChain(
    llm=llm,
    prompt=PROMPT,
    memory=memory,
    verbose=True
)
conversation_with_summary.predict(input="Hi, my name is Perry, what's up?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: My favorite food is pizza
    output: that's good to know

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: Hi, my name is Perry, what's up?
    AI:

    > Finished chain.





    " Hi Perry, I'm doing well. How about you?"
```

```python
# Here, the basketball related content is surfaced
conversation_with_summary.predict(input="what's my favorite sport?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: My favorite sport is soccer
    output: ...

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: what's my favorite sport?
    AI:

    > Finished chain.





    ' You told me earlier that your favorite sport is soccer.'
```

```python
# Even though the language model is stateless, since relevant memory is fetched, it can "reason" about the time.
# Timestamping memories and data is useful in general to let the agent determine temporal relevance
conversation_with_summary.predict(input="Whats my favorite food")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: My favorite food is pizza
    output: that's good to know

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: Whats my favorite food
    AI:

    > Finished chain.





    ' You said your favorite food is pizza.'
```

```python
# The memories from the conversation are automatically stored,
# since this query best matches the introduction chat above,
# the agent is able to 'remember' the user's name.
conversation_with_summary.predict(input="What's my name?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Relevant pieces of previous conversation:
    input: Hi, my name is Perry, what's up?
    response:  Hi Perry, I'm doing well. How about you?

    (You do not need to use these pieces of information if not relevant)

    Current conversation:
    Human: What's my name?
    AI:

    > Finished chain.





    ' Your name is Perry.'
```
