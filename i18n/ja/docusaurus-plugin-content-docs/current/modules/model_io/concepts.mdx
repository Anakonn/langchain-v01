---
sidebar_class_name: hidden
sidebar_position: 1
translated: true
---

# 概念

言語モデルアプリケーションの中核となる要素は、モデルです。LangChainでは、任意の言語モデルとインターフェースを持つためのビルディングブロックを提供しています。このセクションの内容は、モデルの使用を容易にすることが主な目的です。これは主に、モデルの明確なインターフェース、モデルへの入力を構築するためのヘルパーユーティリティ、およびモデルの出力を扱うためのヘルパーユーティリティの提供によって実現されます。

## モデル

LangChainが統合するモデルには主に2つのタイプがあります。LLMとChatモデルです。これらは、入出力の型によって定義されます。

### LLM

LangChainのLLMは、純粋なテキスト補完モデルを指します。
これらのAPIは、文字列プロンプトを入力として受け取り、文字列の補完を出力します。OpenAIのGPT-3はLLMとして実装されています。

### Chatモデル

Chatモデルは、多くの場合LLMをベースにしつつ、会話のために特別にチューニングされています。
重要なのは、これらのプロバイダーAPIが、純粋なテキスト補完モデルとは異なるインターフェースを使用することです。単一の文字列ではなく、チャットメッセージのリストを入力として受け取り、AIメッセージを出力します。メッセージの正確な構成については、以下のセクションをご覧ください。GPT-4やAnthropic社のClaude-2は、両方ともChatモデルとして実装されています。

### 考慮事項

これら2つのAPI型は、入出力スキーマが大きく異なります。そのため、それぞれに最適な方法で対話することが重要です。LangChainでは、これらを互換的に扱うことができますが、必ずしもそうすべきではありません。特に、LLMとChatモデルのプロンプティング戦略は大きく異なる可能性があります。つまり、使用しているモデルのタイプに合わせて、プロンプトを適切に設計する必要があります。

さらに、すべてのモデルが同じではありません。異なるモデルには、それぞれ最適なプロンプティング戦略があります。例えば、AnthropicのモデルはXMLが最適ですが、OpenAIのモデルはJSONが最適です。つまり、ある1つのモデルに適したプロンプトが、別のモデルには適さない可能性があります。LangChainでは多くのデフォルトプロンプトを提供していますが、使用しているモデルに最適に機能するとは限りません。これまでのところ、ほとんどのプロンプトはOpenAIで良好に機能しますが、他のモデルでは十分にテストされていません。これは現在取り組んでいる課題ですが、ご留意ください。

## メッセージ

Chatモデルでは、メッセージのリストを入力として受け取り、1つのメッセージを返します。メッセージには、いくつかの異なるタイプがあります。すべてのメッセージには `role` と `content` プロパティがあります。 `role` は、誰がそのメッセージを発しているかを示します。LangChainには、さまざまな役割に対応したメッセージクラスがあります。 `content` プロパティは、メッセージの内容を表します。これには以下のようなものがあります:

- 文字列(ほとんどのモデルがこの形式)
- 辞書のリスト(マルチモーダル入力の場合、辞書にはその入力タイプと入力位置に関する情報が含まれます)

さらに、メッセージには `additional_kwargs` プロパティがあります。ここには、プロバイダー固有の入力パラメータなど、一般的ではない追加情報を格納できます。最も有名な例は、OpenAIの `function_call` です。

### HumanMessage

ユーザーからのメッセージを表します。通常、コンテンツのみで構成されます。

### AIMessage

モデルからのメッセージを表します。 `additional_kwargs` (例: OpenAIの `functional_call`) を含む可能性があります。

### SystemMessage

システムメッセージを表します。一部のモデルでのみサポートされています。これは、モデルの動作方法を指示します。通常、コンテンツのみで構成されます。

### FunctionMessage

関数呼び出しの結果を表します。 `role` と `content` に加えて、この呼び出しに使用された関数の名前を示す `name` パラメータがあります。

### ToolMessage

ツール呼び出しの結果を表します。これは、OpenAIの `function` と `tool` メッセージタイプに合わせるために、FunctionMessageとは区別されています。 `role` と `content` に加えて、この呼び出しに使用されたツールのIDを示す `tool_call_id` パラメータがあります。

## プロンプト

言語モデルへの入力は、しばしばプロンプトと呼ばれます。ユーザーからのアプリ入力が、そのままモデルへの入力にはならないことがよくあります。代わりに、何らかの変換を経て、文字列やメッセージのリストがモデルに入力されます。ユーザー入力を最終的な文字列やメッセージに変換するオブジェクトは、「プロンプトテンプレート」として知られています。LangChainでは、プロンプトの操作を容易にするためのいくつかの抽象化を提供しています。

### PromptValue

ChatモデルとLLMは異なる入力タイプを取ります。PromptValueは、これら2つの間の相互運用性を持つクラスです。文字列に変換する(LLMと連携する)メソッドと、メッセージのリストに変換する(Chatモデルと連携する)メソッドを公開しています。

### PromptTemplate

[このドキュメント](/docs/modules/model_io/prompts/quick_start#prompttemplate)に例があります。これは、テンプレート文字列で構成されています。このテンプレートは、ユーザー入力でフォーマットされて、最終的な文字列が生成されます。

### MessagePromptTemplate

このタイプのテンプレートは、テンプレートメッセージ(特定の役割とPromptTemplateで構成)で構成されています。このPromptTemplateは、ユーザー入力でフォーマットされ、最終的な文字列がメッセージの `content` となります。

#### HumanMessagePromptTemplate

これは、HumanMessageを生成するMessagePromptTemplateです。

#### AIMessagePromptTemplate

これは、AIMessageを生成するMessagePromptTemplateです。

#### SystemMessagePromptTemplate

これは、SystemMessageを生成するMessagePromptTemplateです。

### メッセージプレースホルダー

多くの場合、プロンプトへの入力はメッセージのリストになります。これは `MessagesPlaceholder` を使用する場合です。これらのオブジェクトは `variable_name` 引数によってパラメータ化されます。同じ値を持つ入力は、メッセージのリストになります。

### ChatPromptTemplate

[これ](/docs/modules/model_io/prompts/quick_start#chatprompttemplate)は、プロンプトテンプレートの例です。これは `MessagePromptTemplates` または `MessagePlaceholders` のリストで構成されています。これらはユーザー入力でフォーマットされ、最終的なメッセージのリストが生成されます。

## 出力パーサー

モデルの出力は文字列またはメッセージです。多くの場合、文字列やメッセージには、後続の処理で使用できるように特定の形式で情報がフォーマットされています(例: コンマ区切りのリスト、JSONブロブ)。出力パーサーは、モデルの出力を、より使いやすい形式に変換する役割を担います。これらは一般的に出力メッセージの `content` で動作しますが、時には `additional_kwargs` フィールドの値でも動作します。

### StrOutputParser

これは単純な出力パーサーで、言語モデル(LLMまたはChatModel)の出力を文字列に変換します。出力がLLMの場合(したがって文字列を出力する)、その文字列をそのまま渡します。出力がChatModelの場合(したがってメッセージを出力する)、メッセージの `.content` 属性を渡します。

### OpenAI Functions Parsers

OpenAI関数呼び出しに特化したパーサーがいくつかあります。これらは `function_call` と `arguments` パラメータ(これらは `additional_kwargs` 内にある)を取り扱い、コンテンツはほとんど無視します。

### エージェント出力パーサー

[エージェント](/docs/modules/agents/)は、言語モデルを使ってどのようなステップを取るべきかを決定するシステムです。言語モデルの出力は、取るべきアクション(もしあれば)を表すスキーマに変換される必要があります。エージェント出力パーサーは、生のLLMまたはChatModel出力をそのようなスキーマに変換する役割を担います。これらの出力パーサー内のロジックは、使用されているモデルやプロンプト戦略によって異なる場合があります。
