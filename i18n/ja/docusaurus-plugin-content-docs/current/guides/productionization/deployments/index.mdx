---
translated: true
---

# デプロイ

現代の急速な技術の進歩に伴い、大規模言語モデル(LLM)の使用が急速に拡大しています。そのため、開発者がこれらのモデルを本番環境でどのように効果的にデプロイするかを理解することが重要です。LLMインターフェイスは通常以下の2つのカテゴリに分類されます:

- **ケース1: 外部LLMプロバイダー(OpenAI、Anthropic等)の利用**
    このシナリオでは、計算負荷の大部分がLLMプロバイダーによって処理されますが、LangChainはこれらのサービスに関するビジネスロジックの実装を簡素化します。このアプローチには、プロンプトテンプレート、チャットメッセージ生成、キャッシング、ベクトル埋め込みデータベースの作成、前処理などの機能が含まれます。

- **ケース2: セルフホスティングのオープンソースモデル**
    あるいは、開発者はより小規模ながら同等の能力を持つセルフホスティングのオープンソースLLMモデルを選択することもできます。このアプローチにより、外部LLMプロバイダーにデータを転送することに伴うコスト、レイテンシ、プライバシーの問題を大幅に削減できます。

製品の基盤となるフレームワークに関わらず、LLMアプリケーションのデプロイには独自の課題があります。サービングフレームワークを評価する際は、トレードオフと重要な考慮事項を理解することが不可欠です。

## 概要

このガイドでは、本番環境でのLLMデプロイに必要な要件について包括的な概要を提供することを目的としています。重点は以下の点にあります:

- **堅牢なLLMアプリケーションサービスの設計**
- **コスト効率の維持**
- **迅速な反復の確保**

これらのコンポーネントを理解することは、サービングシステムを評価する際に不可欠です。LangChainは、これらの課題に取り組むためのいくつかのオープンソースプロジェクトと統合されており、LLMアプリケーションの本番化に堅牢なフレームワークを提供します。主なフレームワークには以下のようなものがあります:

- [Ray Serve](/docs/integrations/providers/ray_serve)
- [BentoML](https://github.com/bentoml/BentoML)
- [OpenLLM](/docs/integrations/providers/openllm)
- [Modal](/docs/integrations/providers/modal)
- [Jina](/docs/integrations/providers/jina)

これらのリンクでは、LLMデプロイメントのニーズに最適なものを見つけるのに役立つ、各エコシステムの詳細情報を提供しています。

## 堅牢なLLMアプリケーションサービスの設計

本番環境でLLMサービスをデプロイする際は、停止のない滑らかなユーザー体験を提供することが不可欠です。24時間365日の稼働を実現するには、アプリケーションを取り囲むいくつかのサブシステムを構築し、維持する必要があります。

### モニタリング

本番環境で稼働するシステムにとって、モニタリングは不可欠な要素です。LLMの文脈では、パフォーマンスと品質の両方のメトリクスを監視することが重要です。

**パフォーマンスメトリクス:** これらのメトリクスは、モデルの効率性と容量に関する洞察を提供します。主な例は以下の通りです:

- クエリ毎秒(QPS): モデルが1秒間に処理するクエリ数を測定し、その利用状況を把握できます。
- レイテンシ: クライアントがリクエストを送信してから応答を受け取るまでの遅延を示します。
- トークン毎秒(TPS): モデルが1秒間に生成できるトークン数を表します。

**品質メトリクス:** これらのメトリクスは、ビジネスユースケースに合わせてカスタマイズされます。例えば、システムの出力が以前のバージョンなどのベースラインと比べてどのように比較されるかなどです。これらのメトリクスはオフラインで計算できますが、必要なデータをログに記録しておく必要があります。

### 障害耐性

モデルの推論やビジネスロジックのコードでの例外など、アプリケーションでエラーが発生し、トラフィックが中断する可能性があります。また、アプリケーションを実行しているマシンでの予期せぬハードウェアの故障や、需要の高い期間でのスポットインスタンスの損失など、その他の問題も起こり得ます。これらのリスクを軽減するには、レプリカのスケーリングによる冗長性の向上や、失敗したレプリカの回復メカニズムの実装などが考えられます。ただし、モデルのレプリカだけが潜在的な障害点ではありません。スタック上のあらゆる障害に対する耐性を構築することが不可欠です。

### ゼロダウンタイムアップグレード

システムのアップグレードは必要不可欠ですが、適切に処理されないと、サービスの中断を招く可能性があります。ダウンタイムを防ぐ1つの方法は、古いバージョンから新しいバージョンへの滑らかな移行プロセスを実装することです。理想的には、LLMサービスの新しいバージョンがデプロイされ、古いバージョンからの traffic が徐々に新しいバージョンに移行し、全体としてQPSが一定に保たれるようにします。

### ロードバランシング

ロードバランシングとは、複数のコンピューター、サーバー、またはその他のリソースにワークを均等に分散させ、システムの利用率を最適化し、スループットを最大化し、応答時間を最小化し、単一のリソースの過負荷を回避する技術です。これを交通整理に例えると、リクエスト(車)を異なるサーバー(道路)に振り分けて、どの道路も混雑しすぎないようにするイメージです。

ロードバランシングには様々な戦略があります。例えば、一般的な方法として *ラウンドロビン* 戦略があります。これは、各リクエストを順番にサーバーに送り、最後のサーバーに到達したら最初のサーバーに戻るというものです。すべてのサーバーが同等の能力の場合に適しています。しかし、サーバーの性能に差がある場合は、*重み付きラウンドロビン* や *最少接続* 戦略を使い、より強力なサーバーやより少ない接続を持つサーバーにより多くのリクエストを送るのが良いでしょう。LLMチェーンを実行しているアプリケーションを考えてみましょう。人気が出てきて、同時に何百、何千ものユーザーが質問をする場合、1つのサーバーが過負荷になると、ロードバランサーは新しいリクエストを別のサーバーに振り分けます。これにより、すべてのユーザーが適時の応答を受け、システムの安定性が維持されます。

## コスト効率と拡張性の維持

LLMサービスの導入は、ユーザーとのやり取りが多い場合、特に高額になる可能性があります。LLMプロバイダーの料金は通常、使用したトークン数に基づいているため、これらのモデルを使ったチャットシステムの推論は高コストになる可能性があります。しかし、サービスの品質を損なうことなく、これらのコストを管理するための複数の戦略があります。

### モデルのセルフホスティング

より小規模で、オープンソースのLLMが、LLMプロバイダーへの依存性の問題に取り組むために登場しています。セルフホスティングにより、LLMプロバイダーのモデルと同様の品質を維持しつつ、コストを管理することができます。課題は、自社のマシンで信頼性の高い高性能なLLMサービングシステムを構築することです。

### リソース管理とオートスケーリング

アプリケーション内の計算ロジックには、適切なリソース割り当てが必要です。例えば、トラフィックの一部をOpenAIのエンドポイントで、他の部分をセルフホストのモデルで処理する場合、それぞれに適切なリソースを割り当てることが重要です。トラフィックに応じてリソース割り当てを調整するオートスケーリングは、アプリケーション運用コストに大きな影響を与えます。このストラテジーでは、コストと応答性のバランスを取ることが重要で、リソースの過剰プロビジョニングや、アプリケーションの応答性の低下を避ける必要があります。

### スポットインスタンスの活用

AWSなどのプラットフォームでは、スポットインスタンスが大幅なコスト削減を提供します。通常の料金の約3分の1程度です。ただし、クラッシュ率が高いため、効果的に活用するには堅牢な障害許容メカニズムが必要です。

### 独立したスケーリング

自社でモデルをホストする場合、独立したスケーリングを検討すべきです。例えば、フランス語とスペイン語の2つの翻訳モデルがある場合、それぞれの要求に応じて異なるスケーリング要件が必要になる可能性があります。

### リクエストのバッチ処理

大規模言語モデルの文脈では、リクエストのバッチ処理によって効率が向上します。GPUは本質的に並列処理に適したプロセッサーであり、複数のタスクを同時に処理できます。個別のリクエストを送信すると、GPUは1つのタスクしか処理できないため、十分に活用されません。一方、リクエストをバッチ処理すれば、GPUが複数のタスクを同時に処理できるため、その活用が最大化され、推論速度も向上します。これにより、コスト削減だけでなく、LLMサービスの全体的なレイテンシーの改善にもつながります。

要約すると、LLMサービスの拡張性を維持しつつコストを管理するには、戦略的なアプローチが必要です。セルフホストモデルの活用、リソース管理の最適化、オートスケーリングの活用、スポットインスタンスの利用、モデルの独立したスケーリング、リクエストのバッチ処理などが、主要な戦略として考えられます。Ray ServeやBentoMLなどのオープンソースライブラリは、これらの複雑性に対処するよう設計されています。

## 迅速な反復の確保

LLMの分野は、新しいライブラリやモデルアーキテクチャが絶え間なく登場するなど、前例のないペースで進化しています。そのため、特定のフレームワークに縛られるソリューションを避けることが重要です。これは特に、サービングの分野で重要で、インフラストラクチャの変更は時間がかかり、コストがかかり、リスクが高くなる可能性があります。特定の機械学習ライブラリやフレームワークに固定されることなく、汎用的で拡張性のあるサービング層を提供することを目指すべきです。柔軟性が重要な側面は以下のとおりです。

### モデルの組み合わせ

LangChainのようなシステムを展開するには、さまざまなモデルを組み合わせ、ロジックで接続する能力が必要です。自然言語入力SQLクエリエンジンの構築を例に取ると、LLMによるクエリの取得は一部にすぎません。接続されたデータベースからメタデータを抽出し、LLMに対するプロンプトを構築し、SQLエンジンでクエリを実行し、クエリの実行中にLLMにフィードバックを提供し、ユーザーに結果を提示するなど、Python で構築された複雑なコンポーネントを、動的なロジカルブロックのチェーンとして統合する必要があります。

### クラウドプロバイダー

多くのホスティングソリューションは特定のクラウドプロバイダーに制限されており、マルチクラウドの時代にはオプションが限られる可能性があります。他のインフラストラクチャコンポーネントがどこに構築されているかによって、選択したクラウドプロバイダーにこだわる必要があるかもしれません。

### インフラストラクチャ as コード (IaC)

迅速な反復には、インフラストラクチャを迅速かつ確実に再現する能力が不可欠です。ここで、Terraform、CloudFormation、Kubernetes YAMLファイルなどのInfrastructure as Code (IaC)ツールが役立ちます。これらのツールを使えば、コードファイルでインフラストラクチャを定義でき、バージョン管理も可能で、迅速かつ確実な展開が可能になります。

### CI/CD

急速に変化する環境では、CI/CDパイプラインの実装が、反復プロセスを大幅に高速化できます。これにより、LLMアプリケーションのテストと展開を自動化でき、エラーのリスクを減らし、迅速なフィードバックと反復が可能になります。
