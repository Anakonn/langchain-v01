---
translated: true
---

# Exemple RAG sur Intel Xeon

Ce mod√®le effectue le RAG √† l'aide de Chroma et de Text Generation Inference sur les processeurs Intel¬Æ Xeon¬Æ Scalable.
Les processeurs Intel¬Æ Xeon¬Æ Scalable sont dot√©s d'acc√©l√©rateurs int√©gr√©s pour une performance par c≈ìur plus √©lev√©e et des performances IA in√©gal√©es, avec des technologies de s√©curit√© avanc√©es pour r√©pondre aux exigences des charges de travail les plus demand√©es - tout en offrant le plus grand choix de cloud et la portabilit√© des applications, veuillez v√©rifier [Intel¬Æ Xeon¬Æ Scalable Processors](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html).

## Configuration de l'environnement

Pour utiliser [ü§ó text-generation-inference](https://github.com/huggingface/text-generation-inference) sur les processeurs Intel¬Æ Xeon¬Æ Scalable, veuillez suivre ces √©tapes :

### Lancer une instance de serveur local sur un serveur Intel Xeon :

```bash
model=Intel/neural-chat-7b-v3-3
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4 --model-id $model
```

Pour les mod√®les gated comme `LLAMA-2`, vous devrez passer -e HUGGING_FACE_HUB_TOKEN=\<token\> √† la commande docker run ci-dessus avec un jeton de lecture valide Hugging Face Hub.

Veuillez suivre ce lien [jeton huggingface](https://huggingface.co/docs/hub/security-tokens) pour obtenir le jeton d'acc√®s et exporter `HUGGINGFACEHUB_API_TOKEN` avec le jeton.

```bash
export HUGGINGFACEHUB_API_TOKEN=<token>
```

Envoyez une requ√™te pour v√©rifier si le point de terminaison fonctionne :

```bash
curl localhost:8080/generate -X POST -d '{"inputs":"Which NFL team won the Super Bowl in the 2010 season?","parameters":{"max_new_tokens":128, "do_sample": true}}'   -H 'Content-Type: application/json'
```

Pour plus de d√©tails, veuillez vous r√©f√©rer √† [text-generation-inference](https://github.com/huggingface/text-generation-inference).

## Remplissage avec des donn√©es

Si vous voulez remplir la base de donn√©es avec quelques donn√©es d'exemple, vous pouvez ex√©cuter les commandes ci-dessous :

```shell
poetry install
poetry run python ingest.py
```

Le script traite et stocke les sections des donn√©es des rapports 10k d'Edgar pour Nike `nke-10k-2023.pdf` dans une base de donn√©es Chroma.

## Utilisation

Pour utiliser ce package, vous devez d'abord avoir install√© le CLI LangChain :

```shell
pip install -U langchain-cli
```

Pour cr√©er un nouveau projet LangChain et installer celui-ci comme seul package, vous pouvez faire :

```shell
langchain app new my-app --package intel-rag-xeon
```

Si vous voulez l'ajouter √† un projet existant, vous pouvez simplement ex√©cuter :

```shell
langchain app add intel-rag-xeon
```

Et ajoutez le code suivant √† votre fichier `server.py` :

```python
from intel_rag_xeon import chain as xeon_rag_chain

add_routes(app, xeon_rag_chain, path="/intel-rag-xeon")
```

(Facultatif) Configurons maintenant LangSmith. LangSmith nous aidera √† tracer, surveiller et d√©boguer les applications LangChain. Vous pouvez vous inscrire √† LangSmith [ici](https://smith.langchain.com/). Si vous n'avez pas acc√®s, vous pouvez sauter cette section.

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>
export LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to "default"
```

Si vous √™tes dans ce r√©pertoire, vous pouvez alors d√©marrer une instance LangServe directement en :

```shell
langchain serve
```

Cela d√©marrera l'application FastAPI avec un serveur qui tourne localement sur
[http://localhost:8000](http://localhost:8000)

Nous pouvons voir tous les mod√®les sur [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
Nous pouvons acc√©der au playground sur [http://127.0.0.1:8000/intel-rag-xeon/playground](http://127.0.0.1:8000/intel-rag-xeon/playground)

Nous pouvons acc√©der au mod√®le √† partir du code avec :

```python
from langserve.client import RemoteRunnable

runnable = RemoteRunnable("http://localhost:8000/intel-rag-xeon")
```
