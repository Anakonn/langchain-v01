---
sidebar_class_name: hidden
sidebar_position: 3
translated: true
---

# Modèles de discussion

Les modèles de discussion sont un composant essentiel de LangChain.

Un modèle de discussion est un modèle de langage qui utilise des messages de discussion comme entrées et renvoie des messages de discussion comme sorties (par opposition à l'utilisation d'un texte brut).

LangChain a des intégrations avec de nombreux fournisseurs de modèles (OpenAI, Cohere, Hugging Face, etc.) et expose une interface standard pour interagir avec tous ces modèles.

LangChain vous permet d'utiliser des modèles en mode synchrone, asynchrone, par lots et en flux et fournit d'autres fonctionnalités (par exemple, la mise en cache) et plus encore.

## [Démarrage rapide](./quick_start)

Consultez [ce démarrage rapide](./quick_start) pour avoir un aperçu du fonctionnement des modèles de discussion, y compris toutes les différentes méthodes qu'ils exposent.

## [Intégrations](/docs/integrations/chat/)

Pour obtenir la liste complète de toutes les intégrations de LLM que LangChain fournit, veuillez consulter la [page des intégrations](/docs/integrations/chat/).

## Guides pratiques

Nous avons plusieurs guides pratiques pour une utilisation plus avancée des LLM.
Cela inclut :

- [Comment mettre en cache les réponses du modèle de discussion](./chat_model_caching)
- [Comment utiliser les modèles de discussion qui prennent en charge l'appel de fonction](./function_calling)
- [Comment diffuser les réponses d'un modèle de discussion](./streaming)
- [Comment suivre l'utilisation des jetons dans un appel de modèle de discussion](./token_usage_tracking)
- [Comment créer un modèle de discussion personnalisé](./custom_chat_model)
