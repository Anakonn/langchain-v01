---
sidebar_position: 0
translated: true
---

# Démarrage rapide

Le démarrage rapide couvrira les bases du travail avec les modèles de langage. Il présentera les deux types de modèles différents - les LLM et les ChatModels. Il couvrira ensuite comment utiliser les PromptTemplates pour formater les entrées de ces modèles et comment utiliser les Output Parsers pour travailler avec les sorties.

## Modèles

Pour ce guide de démarrage, nous fournirons quelques options : utiliser une API comme Anthropic ou OpenAI, ou utiliser un modèle open source local via Ollama.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="openai" label="OpenAI" default>

Tout d'abord, nous devrons installer leur package partenaire :

```shell
pip install langchain-openai
```

L'accès à l'API nécessite une clé API, que vous pouvez obtenir en créant un compte et en vous rendant [ici](https://platform.openai.com/account/api-keys). Une fois que nous avons une clé, nous voudrons la définir en tant que variable d'environnement en exécutant :

```shell
export OPENAI_API_KEY="..."
```

Nous pouvons ensuite initialiser le modèle :

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Quickstart"}]-->
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI

llm = OpenAI()
chat_model = ChatOpenAI(model="gpt-3.5-turbo-0125")
```

Si vous préférez ne pas définir de variable d'environnement, vous pouvez passer la clé directement via le paramètre nommé `api_key` lors de l'initialisation de la classe OpenAI LLM :

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}]-->
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(api_key="...")
```

  </TabItem>
  <TabItem value="local" label="Local (en utilisant Ollama)">

[Ollama](https://ollama.ai/) vous permet d'exécuter des modèles de langage open source, comme Llama 2, localement.

Suivez d'abord [ces instructions](https://github.com/jmorganca/ollama) pour configurer et exécuter une instance locale d'Ollama :

* [Télécharger](https://ollama.ai/download)
* Récupérer un modèle via `ollama pull llama2`

Ensuite, assurez-vous que le serveur Ollama est en cours d'exécution. Après cela, vous pouvez faire :

```python
<!--IMPORTS:[{"imported": "Ollama", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html", "title": "Quickstart"}, {"imported": "ChatOllama", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html", "title": "Quickstart"}]-->
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

llm = Ollama(model="llama2")
chat_model = ChatOllama()
```

  </TabItem>
  <TabItem value="anthropic" label="Anthropic (modèle de chat uniquement)">

Tout d'abord, nous devrons importer le package LangChain x Anthropic.

```shell
pip install langchain-anthropic
```

L'accès à l'API nécessite une clé API, que vous pouvez obtenir en créant un compte [ici](https://claude.ai/login). Une fois que nous avons une clé, nous voudrons la définir en tant que variable d'environnement en exécutant :

```shell
export ANTHROPIC_API_KEY="..."
```

Nous pouvons ensuite initialiser le modèle :

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Quickstart"}]-->
from langchain_anthropic import ChatAnthropic

chat_model = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0.2, max_tokens=1024)
```

Si vous préférez ne pas définir de variable d'environnement, vous pouvez passer la clé directement via le paramètre nommé `api_key` lors de l'initialisation de la classe Anthropic Chat Model :

```python
chat_model = ChatAnthropic(api_key="...")
```

  </TabItem>
  <TabItem value="cohere" label="Cohere">

Tout d'abord, nous devrons installer leur package partenaire :

```shell
pip install langchain-cohere
```

L'accès à l'API nécessite une clé API, que vous pouvez obtenir en créant un compte et en vous rendant [ici](https://dashboard.cohere.com/api-keys). Une fois que nous avons une clé, nous voudrons la définir en tant que variable d'environnement en exécutant :

```shell
export COHERE_API_KEY="..."
```

Nous pouvons ensuite initialiser le modèle :

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Quickstart"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere()
```

Si vous préférez ne pas définir de variable d'environnement, vous pouvez passer la clé directement via le paramètre nommé `cohere_api_key` lors de l'initialisation de la classe Cohere LLM :

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Quickstart"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere(cohere_api_key="...")
```

  </TabItem>
</Tabs>

Les objets `llm` et `chat_model` sont des objets qui représentent la configuration d'un modèle particulier.
Vous pouvez les initialiser avec des paramètres comme `temperature` et d'autres, et les transmettre.
La principale différence entre eux est leur schéma d'entrée et de sortie.
Les objets LLM prennent une chaîne de caractères en entrée et renvoient une chaîne de caractères.
Les objets ChatModel prennent une liste de messages en entrée et renvoient un message.

Nous pouvons voir la différence entre un LLM et un ChatModel lorsque nous l'invoquons.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Quickstart"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

Le LLM renvoie une chaîne de caractères, tandis que le ChatModel renvoie un message.

## Modèles d'invite

La plupart des applications d'apprentissage automatique de langage (LLM) ne transmettent pas directement l'entrée de l'utilisateur à un LLM. Généralement, ils ajoutent l'entrée de l'utilisateur à un plus grand morceau de texte, appelé un modèle d'invite, qui fournit un contexte supplémentaire sur la tâche spécifique à accomplir.

Dans l'exemple précédent, le texte que nous avons transmis au modèle contenait des instructions pour générer un nom d'entreprise. Pour notre application, il serait idéal si l'utilisateur ne devait fournir que la description d'une entreprise/d'un produit sans se soucier de donner des instructions au modèle.

Les PromptTemplates aident exactement à cela !
Ils regroupent toute la logique pour passer de l'entrée de l'utilisateur à une invite entièrement formatée.
Cela peut commencer très simplement - par exemple, une invite pour produire la chaîne ci-dessus serait juste :

```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Quickstart"}]-->
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```python
What is a good name for a company that makes colorful socks?
```

Cependant, les avantages de l'utilisation de ces derniers par rapport au formatage brut des chaînes sont nombreux.
Vous pouvez "partiellement" sortir des variables - par exemple, vous pouvez formater seulement certaines des variables à la fois.
Vous pouvez les composer ensemble, en combinant facilement différents modèles dans une seule invite.
Pour des explications de ces fonctionnalités, consultez la [section sur les invites](/docs/modules/model_io/prompts) pour plus de détails.

Les `PromptTemplate`s peuvent également être utilisés pour produire une liste de messages.
Dans ce cas, l'invite contient non seulement des informations sur le contenu, mais aussi sur chaque message (son rôle, sa position dans la liste, etc.).
Ici, ce qui se passe le plus souvent est qu'un `ChatPromptTemplate` est une liste de `ChatMessageTemplates`.
Chaque `ChatMessageTemplate` contient des instructions sur la façon de formater ce `ChatMessage` - son rôle, et aussi son contenu.
Examinons cela ci-dessous :

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts.chat", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Quickstart"}]-->
from langchain_core.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```pycon
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

Les ChatPromptTemplates peuvent également être construits d'autres manières - consultez la [section sur les invites](/docs/modules/model_io/prompts) pour plus de détails.

## Analyseurs de sortie

Les `OutputParser`s convertissent la sortie brute d'un modèle de langage dans un format utilisable en aval.
Il existe quelques types principaux d'`OutputParser`s, notamment :

- Convertir le texte d'un `LLM` en informations structurées (par exemple, du JSON)
- Convertir un `ChatMessage` en une simple chaîne
- Convertir les informations supplémentaires renvoyées par un appel, en plus du message (comme l'invocation de fonction d'OpenAI) en une chaîne.

Pour plus d'informations à ce sujet, consultez la [section sur les analyseurs de sortie](/docs/modules/model_io/output_parsers).

Dans ce guide de démarrage, nous utilisons un simple analyseur qui analyse une liste de valeurs séparées par des virgules.

```python
<!--IMPORTS:[{"imported": "CommaSeparatedListOutputParser", "source": "langchain.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html", "title": "Quickstart"}]-->
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
output_parser.parse("hi, bye")
# >> ['hi', 'bye']
```

## Composition avec LCEL

Nous pouvons maintenant combiner tout cela en une seule chaîne.
Cette chaîne prendra des variables d'entrée, les transmettra à un modèle d'invite pour créer une invite, transmettra l'invite à un modèle de langage, puis transmettra la sortie à un (éventuel) analyseur de sortie.
C'est un moyen pratique de regrouper une pièce de logique modulaire.
Voyons-le en action !

```python
template = "Generate a list of 5 {text}.\n\n{format_instructions}"

chat_prompt = ChatPromptTemplate.from_template(template)
chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())
chain = chat_prompt | chat_model | output_parser
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

Notez que nous utilisons la syntaxe `|` pour joindre ces composants.
Cette syntaxe `|` est alimentée par le langage d'expression LangChain (LCEL) et s'appuie sur l'interface `Runnable` universelle que tous ces objets implémentent.
Pour en savoir plus sur LCEL, lisez la documentation [ici](/docs/expression_language/).

## Conclusion

Voilà pour le démarrage avec les invites, les modèles et les analyseurs de sortie ! Cela n'a couvert que la surface de ce qu'il y a à apprendre. Pour plus d'informations, consultez :

- La [section sur les invites](/docs/modules/model_io/prompts/) pour plus d'informations sur la façon de travailler avec les modèles d'invite
- La [section LLM](/docs/modules/model_io/llms/) pour plus d'informations sur l'interface LLM
- La [section ChatModel](/docs/modules/model_io/chat/) pour plus d'informations sur l'interface ChatModel
- La [section sur les analyseurs de sortie](/docs/modules/model_io/output_parsers/) pour des informations sur les différents types d'analyseurs de sortie.
