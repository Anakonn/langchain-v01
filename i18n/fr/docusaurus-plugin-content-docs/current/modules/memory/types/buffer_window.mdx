---
translated: true
---

# Fenêtre tampon de conversation

`ConversationBufferWindowMemory` conserve une liste des interactions de la conversation au fil du temps. Il n'utilise que les K dernières interactions. Cela peut être utile pour conserver une fenêtre glissante des interactions les plus récentes, afin que le tampon ne devienne pas trop volumineux.

Explorons d'abord les fonctionnalités de base de ce type de mémoire.

```python
<!--IMPORTS:[{"imported": "ConversationBufferWindowMemory", "source": "langchain.memory", "docs": "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html", "title": "Conversation Buffer Window"}]-->
from langchain.memory import ConversationBufferWindowMemory
```

```python
memory = ConversationBufferWindowMemory( k=1)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.save_context({"input": "not much you"}, {"output": "not much"})
```

```python
memory.load_memory_variables({})
```

```output
    {'history': 'Human: not much you\nAI: not much'}
```

Nous pouvons également obtenir l'historique sous forme de liste de messages (cela est utile si vous l'utilisez avec un modèle de chat).

```python
memory = ConversationBufferWindowMemory( k=1, return_messages=True)
memory.save_context({"input": "hi"}, {"output": "whats up"})
memory.save_context({"input": "not much you"}, {"output": "not much"})
```

```python
memory.load_memory_variables({})
```

```output
    {'history': [HumanMessage(content='not much you', additional_kwargs={}),
      AIMessage(content='not much', additional_kwargs={})]}
```

## Utilisation dans une chaîne

Parcourons un exemple, en réglant à nouveau `verbose=True` afin de pouvoir voir l'invite.

```python
<!--IMPORTS:[{"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Conversation Buffer Window"}, {"imported": "ConversationChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html", "title": "Conversation Buffer Window"}]-->
from langchain_openai import OpenAI
from langchain.chains import ConversationChain
conversation_with_summary = ConversationChain(
    llm=OpenAI(temperature=0),
    # We set a low k=2, to only keep the last 2 interactions in memory
    memory=ConversationBufferWindowMemory(k=2),
    verbose=True
)
conversation_with_summary.predict(input="Hi, what's up?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:

    Human: Hi, what's up?
    AI:

    > Finished chain.





    " Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?"
```

```python
conversation_with_summary.predict(input="What's their issues?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:
    Human: Hi, what's up?
    AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?
    Human: What's their issues?
    AI:

    > Finished chain.





    " The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected."
```

```python
conversation_with_summary.predict(input="Is it going well?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:
    Human: Hi, what's up?
    AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?
    Human: What's their issues?
    AI:  The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected.
    Human: Is it going well?
    AI:

    > Finished chain.





    " Yes, it's going well so far. We've already identified the problem and are now working on a solution."
```

```python
# Notice here that the first interaction does not appear.
conversation_with_summary.predict(input="What's the solution?")
```

```output


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:
    Human: What's their issues?
    AI:  The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected.
    Human: Is it going well?
    AI:  Yes, it's going well so far. We've already identified the problem and are now working on a solution.
    Human: What's the solution?
    AI:

    > Finished chain.





    " The solution is to reset the router and reconfigure the settings. We're currently in the process of doing that."
```
