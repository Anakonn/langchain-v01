---
hide_table_of_contents: true
sidebar_class_name: hidden
sidebar_position: 3
title: Chaînes
translated: true
---

Les chaînes se réfèrent à des séquences d'appels - que ce soit à un LLM, un outil, ou une étape de prétraitement des données. La principale façon de le faire est avec [LCEL](/docs/expression_language).

LCEL est idéal pour construire vos chaînes, mais il est également agréable d'avoir des chaînes prêtes à l'emploi. LangChain prend en charge deux types de chaînes prêtes à l'emploi :

- Chaînes construites avec LCEL. Dans ce cas, LangChain propose une méthode de constructeur de plus haut niveau. Cependant, tout ce qui est fait en coulisse est la construction d'une chaîne avec LCEL.
- Chaînes [Legacy] construites en sous-classant à partir d'une classe `Chain` héritée. Ces chaînes n'utilisent pas LCEL en coulisse mais sont des classes autonomes.

Nous travaillons à la création de méthodes qui créent des versions LCEL de toutes les chaînes. Nous faisons cela pour plusieurs raisons.

1. Les chaînes construites de cette manière sont intéressantes car si vous souhaitez modifier les internes d'une chaîne, vous pouvez simplement modifier le LCEL.
2. Ces chaînes prennent en charge nativement le streaming, l'async et le batch dès le départ.
3. Ces chaînes obtiennent automatiquement de l'observabilité à chaque étape.

Cette page contient deux listes. Premièrement, une liste de tous les constructeurs de chaînes LCEL. Deuxièmement, une liste de toutes les chaînes Legacy.

## Chaînes LCEL

Voici un tableau de tous les `constructeurs de chaînes LCEL`.

Colonnes du tableau :

- **Constructeur de Chaîne :** La fonction de construction pour cette chaîne. Ce sont toutes des méthodes qui renvoient des Runnables LCEL. Nous fournissons également un lien vers la documentation API.
- **Appel de Fonction :** Si cela nécessite un appel de fonction OpenAI.
- **Autres Outils :** Autres outils (le cas échéant) utilisés dans cette chaîne.
- **Quand Utiliser :** Notre commentaire sur le moment d'utiliser cette chaîne.

| Constructeur de Chaîne                | Appel de Fonction      | Autres Outils  | Quand Utiliser                                                                    |
|---------------------------------------|------------------------|---------------|-----------------------------------------------------------------------------------|
| [create_stuff_documents_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html#langchain.chains.combine_documents.stuff.create_stuff_documents_chain)     |                        |               | Cette chaîne prend une liste de documents et les formate tous dans une invite, puis passe cette invite à un LLM. Elle passe TOUS les documents, donc vous devez vous assurer qu'ils rentrent dans la fenêtre de contexte du LLM que vous utilisez. |
| [create_openai_fn_runnable](https://api.python.langchain.com/en/latest/chains/langchain.chains.structured_output.base.create_openai_fn_runnable.html#langchain.chains.structured_output.base.create_openai_fn_runnable)        | ✅                     |               | Si vous souhaitez utiliser l'appel de fonction OpenAI pour structurer OPTIONNELLEMENT une réponse de sortie. Vous pouvez passer plusieurs fonctions pour son appel, mais il n'est pas obligé de l'appeler.                                     |
| [create_structured_output_runnable](https://api.python.langchain.com/en/latest/chains/langchain.chains.structured_output.base.create_structured_output_runnable.html#langchain.chains.structured_output.base.create_structured_output_runnable) | ✅                     |               | Si vous souhaitez utiliser l'appel de fonction OpenAI pour FORCER le LLM à répondre avec une certaine fonction. Vous ne pouvez passer qu'une seule fonction, et la chaîne renverra TOUJOURS cette réponse.                                      |
| [load_query_constructor_runnable](https://api.python.langchain.com/en/latest/chains/langchain.chains.query_constructor.base.load_query_constructor_runnable.html#langchain.chains.query_constructor.base.load_query_constructor_runnable)  |                        |               | Peut être utilisé pour générer des requêtes. Vous devez spécifier une liste d'opérations autorisées puis renvoyer un runnable qui convertit une requête en langage naturel en ces opérations autorisées.                                                                               |
| [create_sql_query_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sql_database.query.create_sql_query_chain.html#langchain.chains.sql_database.query.create_sql_query_chain)           |                        | Base de données SQL | Si vous souhaitez construire une requête pour une base de données SQL à partir du langage naturel.                            |
| [create_history_aware_retriever](https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html#langchain.chains.history_aware_retriever.create_history_aware_retriever)   |                        | Retrievers    | Cette chaîne prend l'historique des conversations puis l'utilise pour générer une requête de recherche qui est passée au retriever sous-jacent.                       |
| [create_retrieval_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html#langchain.chains.retrieval.create_retrieval_chain)           |                        | Retrievers    | Cette chaîne prend une demande de l'utilisateur, qui est ensuite passée au retriever pour récupérer les documents pertinents. Ces documents (et les entrées originales) sont ensuite passés à un LLM pour générer une réponse.      |

## Chaînes Héritées

Ci-dessous sont les `legacy chains`. Nous maintiendrons le support pour ceux-ci jusqu'à ce que nous créions une alternative LCEL.

Colonnes du tableau :

- **Chaîne:** Nom de la chaîne ou nom de la méthode du constructeur. Si méthode du constructeur, cela renverra une sous-classe de `Chain`.
- **Appel de Fonction:** Si la chaîne nécessite OpenAI Function Calling.
- **Autres Outils:** Autres outils utilisés dans la chaîne.
- **Quand Utiliser:** Notre commentaire sur quand utiliser.

| Chaîne                        |  Appel de Fonction | Autres Outils            | Quand Utiliser |
|------------------------------|--------------------|------------------------|-------------|
| [APIChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.api.base.APIChain.html#langchain.chains.api.base.APIChain)                    |                                            | Requests Wrapper               | Cette chaîne utilise un LLM pour convertir une requête en une demande d'API, puis exécute cette demande, obtient une réponse, et ensuite passe cette requête à un LLM pour répondre            |
| [OpenAPIEndpointChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.api.openapi.chain.OpenAPIEndpointChain.html#langchain.chains.api.openapi.chain.OpenAPIEndpointChain)         |                                            | OpenAPI Spec           | Similaire à APIChain, cette chaîne est conçue pour interagir avec les API. La principale différence est qu'elle est optimisée pour une utilisation facile avec les points de terminaison OpenAPI            |
| [ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain) |                                            | Retriever              |Cette chaîne peut être utilisée pour avoir des **conversations** avec un document. Elle prend une question et (optionnel) l'historique de conversation précédent. S'il y a un historique de conversation précédent, elle utilise un LLM pour réécrire la conversation en une requête à envoyer à un retriever (autrement, elle utilise simplement la nouvelle entrée utilisateur). Elle récupère ensuite ces documents et les passe (avec la conversation) à un LLM pour répondre.             |
| [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain)           |                    |                        |Cette chaîne prend une liste de documents et les formatte tous en une invite, puis passe cette invite à un LLM. Elle passe TOUS les documents, donc vous devez vous assurer qu'ils tiennent dans la fenêtre de contexte du LLM que vous utilisez.             |
| [ReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain)         |                                            |                        |Cette chaîne combine des documents en les réduisant de manière itérative. Elle regroupe les documents en morceaux (moins que la longueur de contexte) et les passe ensuite à un LLM. Elle prend ensuite les réponses et continue de cette manière jusqu'à ce qu'elle puisse tout intégrer en un seul appel LLM final. Elle est utile lorsque vous avez beaucoup de documents, que vous voulez que le LLM les traite tous, et que vous pouvez le faire en parallèle.              |
| [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)      |                        |                                            |Cette chaîne passe d'abord chaque document à travers un LLM, puis les réduit en utilisant le `ReduceDocumentsChain`. Elle est utile dans les mêmes situations que `ReduceDocumentsChain`, mais effectue un appel initial LLM avant d'essayer de réduire les documents.          |
| [RefineDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html#langchain.chains.combine_documents.refine.RefineDocumentsChain)         |    |                                        |Cette chaîne réduit les documents en générant une réponse initiale basée sur le premier document, puis en passant en revue les documents restants pour *affiner* sa réponse. Cela fonctionne de manière séquentielle, donc ne peut pas être parallélisé. Elle est utile dans des situations similaires à MapReduceDocuments Chain, mais pour les cas où vous souhaitez construire une réponse en affinant la réponse précédente (plutôt que de paralléliser les appels).                        |             |
| [MapRerankDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html#langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain)      |                        |                    |                                      Cela appelle un LLM sur chaque document, lui demandant non seulement de répondre mais aussi de produire un score de confiance. La réponse avec la plus haute confiance est ensuite retournée. Cela est utile lorsque vous avez beaucoup de documents, mais que vous souhaitez répondre sur la base d'un seul document, plutôt que d'essayer de combiner les réponses (comme le font les méthodes Refine et Reduce).|
| [ConstitutionalChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html#langchain.chains.constitutional_ai.base.ConstitutionalChain)          |                                            |                        |Cette chaîne répond, puis tente d'affiner sa réponse en fonction des principes constitutionnels fournis. Utilisez cela pour garantir que la réponse d'une chaîne suit certains principes.             |
| [LLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain)                                 |  |                  |                        |Cette chaîne combine simplement une invite avec un LLM et un analyseur de sortie. La manière recommandée de le faire est d'utiliser LCEL.             |
| [ElasticsearchDatabaseChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.elasticsearch_database.base.ElasticsearchDatabaseChain.html#langchain.chains.elasticsearch_database.base.ElasticsearchDatabaseChain)                           |                    | Elasticsearch Instance |Cette chaîne convertit une question en langage naturel en une requête `Elasticsearch`, puis l'exécute, et résume ensuite la réponse. Cela est utile lorsque vous souhaitez poser des questions en langage naturel à une base de données `Elasticsearch`.             |
| [FlareChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.flare.base.FlareChain.html#langchain.chains.flare.base.FlareChain)                   |                                            |                        |Cela implémente [FLARE](https://arxiv.org/abs/2305.06983), une technique avancée de récupération. Elle est principalement destinée à une méthode de récupération avancée exploratoire.             |
| [ArangoGraphQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.arangodb.ArangoGraphQAChain.html#langchain.chains.graph_qa.arangodb.ArangoGraphQAChain)                                   |                    |Arango Graph                        |Cette chaîne construit une requête Arango à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.             |
|[GraphCypherQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.cypher.GraphCypherQAChain.html#langchain.chains.graph_qa.cypher.GraphCypherQAChain)                                                      |                    |Un graphe qui fonctionne avec le langage de requête Cypher                        |Cette chaîne construit une requête Cypher à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.             |
|[FalkorDBGraphQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.falkordb.FalkorDBQAChain.html#langchain.chains.graph_qa.falkordb.FalkorDBQAChain)                                                      |                    |Falkor Database                        | Cette chaîne construit une requête FalkorDB à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.             |
|[HugeGraphQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.hugegraph.HugeGraphQAChain.html#langchain.chains.graph_qa.hugegraph.HugeGraphQAChain)                                                     |                    |HugeGraph                        |Cette chaîne construit une requête HugeGraph à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.              |
|[KuzuQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.kuzu.KuzuQAChain.html#langchain.chains.graph_qa.kuzu.KuzuQAChain)                                                      |                    |Kuzu Graph                        |Cette chaîne construit une requête Kuzu Graph à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.              |
|[NebulaGraphQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain.html#langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain)                                                      |                    |Nebula Graph                        |Cette chaîne construit une requête Nebula Graph à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.              |
|[NeptuneOpenCypherQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain.html#langchain.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain)                                                     |                    |Neptune Graph                        |Cette chaîne construit une requête Neptune Graph à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.              |
|[GraphSparqlChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.graph_qa.sparql.GraphSparqlQAChain.html#langchain.chains.graph_qa.sparql.GraphSparqlQAChain)                                                      |                    |Graphe qui fonctionne avec SparQL                        |Cette chaîne construit une requête SparQL à partir du langage naturel, exécute cette requête contre le graphe, et passe ensuite les résultats à un LLM pour répondre.              |
|[LLMMath](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_math.base.LLMMathChain.html#langchain.chains.llm_math.base.LLMMathChain)                                                      |                    |                        |Cette chaîne convertit une question utilisateur en un problème mathématique et l'exécute ensuite (en utilisant [numexpr](https://github.com/pydata/numexpr))             |
|[LLMCheckerChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_checker.base.LLMCheckerChain.html#langchain.chains.llm_checker.base.LLMCheckerChain)                                                      |                    |                        |Cette chaîne utilise un second appel LLM pour vérifier sa réponse initiale. Utilisez cela lorsque vous avez une couche supplémentaire de validation sur l'appel initial du LLM.             |
|[LLMSummarizationChecker](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain.html#langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain)                              |                        |                                            |Cette chaîne crée un résumé en utilisant une séquence d'appels LLM pour s'assurer qu'il est extra correct. Utilisez cela plutôt que la chaîne de résumé normale lorsque vous êtes d'accord avec plusieurs appels LLM (par exemple, vous vous souciez plus de l'exactitude que de la vitesse/coût).             |
|[create_citation_fuzzy_match_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain.html#langchain.chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain)                                                      |✅                    |                        |Utilise OpenAI function calling pour répondre aux questions et citer ses sources.             |
|[create_extraction_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.extraction.create_extraction_chain.html#langchain.chains.openai_functions.extraction.create_extraction_chain)                              |                        ✅                    |                        |Utilise OpenAI Function calling pour extraire des informations d'un texte.             |
|[create_extraction_chain_pydantic](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.extraction.create_extraction_chain_pydantic.html#langchain.chains.openai_functions.extraction.create_extraction_chain_pydantic)                              |                        ✅                    |                        |Utilise OpenAI function calling pour extraire des informations d'un texte dans un modèle Pydantic. Comparé à `create_extraction_chain` cela a une intégration plus étroite avec Pydantic.             |
|[get_openapi_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.openapi.get_openapi_chain.html#langchain.chains.openai_functions.openapi.get_openapi_chain)                              |                        ✅                    |OpenAPI Spec                        |Utilise OpenAI function calling pour interroger un OpenAPI.             |
|[create_qa_with_structure_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_structure_chain.html#langchain.chains.openai_functions.qa_with_structure.create_qa_with_structure_chain)                              |                        ✅                    |                        |Utilise OpenAI function calling pour faire des questions-réponses sur un texte et répondre dans un format spécifique.             |
|[create_qa_with_sources_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_sources_chain.html#langchain.chains.openai_functions.qa_with_structure.create_qa_with_sources_chain)                              |                        ✅                    |                        |Utilise OpenAI function calling pour répondre aux questions avec des citations.             |
|[QAGenerationChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_generation.base.QAGenerationChain.html#langchain.chains.qa_generation.base.QAGenerationChain)                                          |            |                    |Crée à la fois des questions et des réponses à partir de documents. Utilisé pour générer des paires question/réponse pour évaluation des projets de récupération.                        |
|[RetrievalQAWithSourcesChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html#langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain)                              |                        |                          Retriever                    |Fait des questions-réponses sur des documents récupérés, et cite ses sources. Utilisez cela lorsque vous voulez que la réponse contienne des sources dans le texte. Utilisez cela plutôt que `load_qa_with_sources_chain` lorsque vous voulez utiliser un retriever pour récupérer le document pertinent dans la chaîne (plutôt que de les passer directement).|
|[load_qa_with_sources_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain.html#langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain)                                                     | |Retriever                    |Fait des questions-réponses sur des documents que vous passez, et cite ses sources. Utilisez cela lorsque vous voulez que la réponse contienne des sources dans le texte. Utilisez cela plutôt que RetrievalQAWithSources lorsque vous voulez passer directement les documents (plutôt que de compter sur un retriever pour les obtenir).|
|[RetrievalQA](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html#langchain.chains.retrieval_qa.base.RetrievalQA)   |                                            |Retriever                        |Cette chaîne effectue d'abord une étape de récupération pour obtenir des documents pertinents, puis passe ces documents à un LLM pour générer une réponse.|
|[MultiPromptChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html#langchain.chains.router.multi_prompt.MultiPromptChain)                                                      |                    |                        |Cette chaîne oriente l'entrée entre plusieurs invites. Utilisez cela lorsque vous avez plusieurs invites potentielles que vous pourriez utiliser pour répondre et que vous souhaitez orienter vers une seule. |
|[MultiRetrievalQAChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain.html#langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain)|                                            |Retriever                        |Cette chaîne oriente l'entrée entre plusieurs retrievers. Utilisez cela lorsque vous avez plusieurs retrievers potentiels à partir desquels vous pourriez récupérer des documents pertinents et que vous souhaitez orienter vers un seul. |
|[EmbeddingRouterChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.router.embedding_router.EmbeddingRouterChain.html#langchain.chains.router.embedding_router.EmbeddingRouterChain)|                                            |                        |Cette chaîne utilise la similarité d'embeddings pour orienter les requêtes entrantes.|
|[LLMRouterChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.router.llm_router.LLMRouterChain.html#langchain.chains.router.llm_router.LLMRouterChain)|                                            |                        |Cette chaîne utilise un LLM pour orienter entre les options potentielles.|
|load_summarize_chain|                        |                    |                        |Cette chaîne résume un texte|
|[LLMRequestsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_requests.LLMRequestsChain.html#langchain.chains.llm_requests.LLMRequestsChain)|                        |                                            |Cette chaîne construit une URL à partir de l'entrée utilisateur, obtient les données à cette URL, puis résume la réponse. Comparé à APIChain, cette chaîne n'est pas axée sur une seule spécification d'API mais est plus générale.       |
