---
translated: true
---

# Ontotext GraphDB

>[Ontotext GraphDB](https://graphdb.ontotext.com/) est une base de donn√©es de graphes et un outil de d√©couverte de connaissances conforme √† [RDF](https://www.w3.org/RDF/) et [SPARQL](https://www.w3.org/TR/sparql11-query/).

>Ce notebook montre comment utiliser les LLM pour fournir une interrogation en langage naturel (NLQ vers SPARQL, √©galement appel√©e `text2sparql`) pour `Ontotext GraphDB`.

## Fonctionnalit√©s LLM de GraphDB

`GraphDB` prend en charge certaines fonctionnalit√©s d'int√©gration LLM comme d√©crit [ici](https://github.com/w3c/sparql-dev/issues/193) :

[gpt-queries](https://graphdb.ontotext.com/documentation/10.5/gpt-queries.html)

* pr√©dicats magiques pour demander √† un LLM du texte, une liste ou un tableau en utilisant les donn√©es de votre graphe de connaissances (KG)
* explication de la requ√™te
* explication, r√©sum√©, reformulation, traduction des r√©sultats

[retrieval-graphdb-connector](https://graphdb.ontotext.com/documentation/10.5/retrieval-graphdb-connector.html)

* Indexation des entit√©s KG dans une base de donn√©es vectorielle
* Prend en charge tout algorithme d'int√©gration de texte et base de donn√©es vectorielle
* Utilise le m√™me langage de connecteur puissant (indexation) que GraphDB pour Elastic, Solr, Lucene
* Synchronisation automatique des modifications des donn√©es RDF avec l'index des entit√©s KG
* Prend en charge les objets imbriqu√©s (pas de support UI dans la version 10.5 de GraphDB)
* S√©rialise les entit√©s KG en texte comme ceci (par exemple pour un jeu de donn√©es Wines) :

```text
Franvino:
- is a RedWine.
- made from grape Merlo.
- made from grape Cabernet Franc.
- has sugar dry.
- has year 2012.
```

[talk-to-graph](https://graphdb.ontotext.com/documentation/10.5/talk-to-graph.html)

* Un chatbot simple utilisant un index d'entit√©s KG d√©fini

Pour ce tutoriel, nous n'utiliserons pas l'int√©gration LLM de GraphDB, mais la g√©n√©ration de `SPARQL` √† partir de NLQ. Nous utiliserons l'ontologie et le jeu de donn√©es `Star Wars API` (`SWAPI`) que vous pouvez examiner [ici](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo/blob/main/starwars-data.trig).

## Configuration

Vous avez besoin d'une instance GraphDB en cours d'ex√©cution. Ce tutoriel montre comment ex√©cuter la base de donn√©es localement √† l'aide de l'[image Docker GraphDB](https://hub.docker.com/r/ontotext/graphdb). Il fournit un ensemble de docker compose, qui peuple GraphDB avec le jeu de donn√©es Star Wars. Tous les fichiers n√©cessaires, y compris ce notebook, peuvent √™tre t√©l√©charg√©s √† partir du [d√©p√¥t GitHub langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo).

* Installez [Docker](https://docs.docker.com/get-docker/). Ce tutoriel a √©t√© cr√©√© √† l'aide de la version Docker `24.0.7` qui inclut [Docker Compose](https://docs.docker.com/compose/). Pour les versions ant√©rieures de Docker, vous devrez peut-√™tre installer Docker Compose s√©par√©ment.
* Clonez le [d√©p√¥t GitHub langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo) dans un dossier local sur votre machine.
* D√©marrez GraphDB avec le script suivant ex√©cut√© √† partir du m√™me dossier :

```bash
docker build --tag graphdb .
docker compose up -d graphdb
```

  Vous devez attendre quelques secondes pour que la base de donn√©es d√©marre sur `http://localhost:7200/`. Le jeu de donn√©es Star Wars `starwars-data.trig` est automatiquement charg√© dans le r√©f√©rentiel `langchain`. Le point de terminaison SPARQL local `http://localhost:7200/repositories/langchain` peut √™tre utilis√© pour ex√©cuter des requ√™tes. Vous pouvez √©galement ouvrir le GraphDB Workbench √† partir de votre navigateur Web pr√©f√©r√© `http://localhost:7200/sparql` o√π vous pouvez faire des requ√™tes de mani√®re interactive.
* Configurez l'environnement de travail

Si vous utilisez `conda`, cr√©ez et activez un nouvel environnement conda (par exemple `conda create -n graph_ontotext_graphdb_qa python=3.9.18`).

Installez les biblioth√®ques suivantes :

```bash
pip install jupyter==1.0.0
pip install openai==1.6.1
pip install rdflib==7.0.0
pip install langchain-openai==0.0.2
pip install langchain>=0.1.5
```

Ex√©cutez Jupyter avec

```bash
jupyter notebook
```

## Sp√©cification de l'ontologie

Afin que le LLM puisse g√©n√©rer du SPARQL, il doit conna√Ætre le sch√©ma du graphe de connaissances (l'ontologie). Il peut √™tre fourni √† l'aide de l'un des deux param√®tres de la classe `OntotextGraphDBGraph` :

* `query_ontology` : une requ√™te `CONSTRUCT` qui est ex√©cut√©e sur le point de terminaison SPARQL et renvoie les d√©clarations du sch√©ma du graphe de connaissances. Nous vous recommandons de stocker l'ontologie dans son propre graphe nomm√©, ce qui facilitera l'obtention des seules d√©clarations pertinentes (comme dans l'exemple ci-dessous). Les requ√™tes `DESCRIBE` ne sont pas prises en charge, car `DESCRIBE` renvoie la description born√©e concise sym√©trique (SCBD), c'est-√†-dire √©galement les liens de classe entrants. Dans le cas de grands graphes avec un million d'instances, cela n'est pas efficace. V√©rifiez https://github.com/eclipse-rdf4j/rdf4j/issues/4857
* `local_file` : un fichier d'ontologie RDF local. Les formats RDF pris en charge sont `Turtle`, `RDF/XML`, `JSON-LD`, `N-Triples`, `Notation-3`, `Trig`, `Trix`, `N-Quads`.

Dans les deux cas, le dump d'ontologie doit :

* Inclure suffisamment d'informations sur les classes, les propri√©t√©s, l'attachement des propri√©t√©s aux classes (√† l'aide de rdfs:domain, schema:domainIncludes ou de restrictions OWL) et les taxonomies (individus importants).
* Ne pas inclure de d√©finitions et d'exemples trop verbeux et non pertinents qui ne contribuent pas √† la construction de SPARQL.

```python
from langchain_community.graphs import OntotextGraphDBGraph

# feeding the schema using a user construct query

graph = OntotextGraphDBGraph(
    query_endpoint="http://localhost:7200/repositories/langchain",
    query_ontology="CONSTRUCT {?s ?p ?o} FROM <https://swapi.co/ontology/> WHERE {?s ?p ?o}",
)
```

```python
# feeding the schema using a local RDF file

graph = OntotextGraphDBGraph(
    query_endpoint="http://localhost:7200/repositories/langchain",
    local_file="/path/to/langchain_graphdb_tutorial/starwars-ontology.nt",  # change the path here
)
```

De toute fa√ßon, l'ontologie (sch√©ma) est transmise au LLM sous forme de `Turtle` car `Turtle` avec les pr√©fixes appropri√©s est le plus compact et le plus facile pour le LLM √† m√©moriser.

L'ontologie de Star Wars est un peu inhabituelle dans la mesure o√π elle inclut de nombreux triplets sp√©cifiques sur les classes, par exemple que l'esp√®ce `:Aleena` vit sur `<planet/38>`, qu'elle est une sous-classe de `:Reptile`, qu'elle a certaines caract√©ristiques typiques (taille moyenne, dur√©e de vie moyenne, couleur de la peau) et que des individus sp√©cifiques (personnages) en sont des repr√©sentants :

```output
@prefix : <https://swapi.co/vocabulary/> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

:Aleena a owl:Class, :Species ;
    rdfs:label "Aleena" ;
    rdfs:isDefinedBy <https://swapi.co/ontology/> ;
    rdfs:subClassOf :Reptile, :Sentient ;
    :averageHeight 80.0 ;
    :averageLifespan "79" ;
    :character <https://swapi.co/resource/aleena/47> ;
    :film <https://swapi.co/resource/film/4> ;
    :language "Aleena" ;
    :planet <https://swapi.co/resource/planet/38> ;
    :skinColor "blue", "gray" .

    ...

```

Afin de garder ce tutoriel simple, nous utilisons un GraphDB non s√©curis√©. Si GraphDB est s√©curis√©, vous devez d√©finir les variables d'environnement 'GRAPHDB_USERNAME' et 'GRAPHDB_PASSWORD' avant l'initialisation de `OntotextGraphDBGraph`.

```python
os.environ["GRAPHDB_USERNAME"] = "graphdb-user"
os.environ["GRAPHDB_PASSWORD"] = "graphdb-password"

graph = OntotextGraphDBGraph(
    query_endpoint=...,
    query_ontology=...
)
```

## R√©ponse aux questions sur le jeu de donn√©es Star Wars

Nous pouvons maintenant utiliser `OntotextGraphDBQAChain` pour poser quelques questions.

```python
import os

from langchain.chains import OntotextGraphDBQAChain
from langchain_openai import ChatOpenAI

# We'll be using an OpenAI model which requires an OpenAI API Key.
# However, other models are available as well:
# https://python.langchain.com/docs/integrations/chat/

# Set the environment variable `OPENAI_API_KEY` to your OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-***"

# Any available OpenAI model can be used here.
# We use 'gpt-4-1106-preview' because of the bigger context window.
# The 'gpt-4-1106-preview' model_name will deprecate in the future and will change to 'gpt-4-turbo' or similar,
# so be sure to consult with the OpenAI API https://platform.openai.com/docs/models for the correct naming.

chain = OntotextGraphDBQAChain.from_llm(
    ChatOpenAI(temperature=0, model_name="gpt-4-1106-preview"),
    graph=graph,
    verbose=True,
)
```

Posons une question simple.

```python
chain.invoke({chain.input_key: "What is the climate on Tatooine?"})[chain.output_key]
```

```output


[1m> Entering new OntotextGraphDBQAChain chain...[0m
Generated SPARQL:
[32;1m[1;3mPREFIX : <https://swapi.co/vocabulary/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?climate
WHERE {
  ?planet rdfs:label "Tatooine" ;
          :climate ?climate .
}[0m

[1m> Finished chain.[0m
```

```output
'The climate on Tatooine is arid.'
```

Et une un peu plus compliqu√©e.

```python
chain.invoke({chain.input_key: "What is the climate on Luke Skywalker's home planet?"})[
    chain.output_key
]
```

```output


[1m> Entering new OntotextGraphDBQAChain chain...[0m
Generated SPARQL:
[32;1m[1;3mPREFIX : <https://swapi.co/vocabulary/>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?climate
WHERE {
  ?character rdfs:label "Luke Skywalker" .
  ?character :homeworld ?planet .
  ?planet :climate ?climate .
}[0m

[1m> Finished chain.[0m
```

```output
"The climate on Luke Skywalker's home planet is arid."
```

Nous pouvons √©galement poser des questions plus complexes comme

```python
chain.invoke(
    {
        chain.input_key: "What is the average box office revenue for all the Star Wars movies?"
    }
)[chain.output_key]
```

```output


[1m> Entering new OntotextGraphDBQAChain chain...[0m
Generated SPARQL:
[32;1m[1;3mPREFIX : <https://swapi.co/vocabulary/>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

SELECT (AVG(?boxOffice) AS ?averageBoxOffice)
WHERE {
  ?film a :Film .
  ?film :boxOffice ?boxOfficeValue .
  BIND(xsd:decimal(?boxOfficeValue) AS ?boxOffice)
}
[0m

[1m> Finished chain.[0m
```

```output
'The average box office revenue for all the Star Wars movies is approximately 754.1 million dollars.'
```

## Modificateurs de cha√Æne

La cha√Æne de questions-r√©ponses GraphDB d'Ontotext permet d'affiner les invites pour am√©liorer davantage votre cha√Æne de questions-r√©ponses et d'am√©liorer l'exp√©rience utilisateur globale de votre application.

### Invite de "G√©n√©ration SPARQL"

L'invite est utilis√©e pour la g√©n√©ration de requ√™tes SPARQL en fonction de la question de l'utilisateur et du sch√©ma du graphe de connaissances.

- `sparql_generation_prompt`

    Valeur par d√©faut :
  ````python
    GRAPHDB_SPARQL_GENERATION_TEMPLATE = """
    √âcrivez une requ√™te SPARQL SELECT pour interroger une base de donn√©es de graphes.
    L'ontologie du sch√©ma d√©limit√©e par des triples backticks au format Turtle est :
    ```
    {schema}
    ```
    Utilisez uniquement les classes et les propri√©t√©s fournies dans le sch√©ma pour construire la requ√™te SPARQL.
    N'utilisez aucune classe ou propri√©t√© qui n'est pas explicitement fournie dans la requ√™te SPARQL.
    Incluez tous les pr√©fixes n√©cessaires.
    N'incluez aucune explication ou excuse dans vos r√©ponses.
    Ne mettez pas la requ√™te entre backticks.
    N'incluez aucun autre texte que la requ√™te SPARQL g√©n√©r√©e.
    La question d√©limit√©e par des triples backticks est :
    ```
    {prompt}
    ```
    """
    GRAPHDB_SPARQL_GENERATION_PROMPT = PromptTemplate(
        input_variables=["schema", "prompt"],
        template=GRAPHDB_SPARQL_GENERATION_TEMPLATE,
    )
  ````

### Invite de "Correction SPARQL"

Parfois, le LLM peut g√©n√©rer une requ√™te SPARQL avec des erreurs syntaxiques ou des pr√©fixes manquants, etc. La cha√Æne essaiera de la corriger en invitant le LLM √† la corriger un certain nombre de fois.

- `sparql_fix_prompt`

    Valeur par d√©faut :
  ````python
    GRAPHDB_SPARQL_FIX_TEMPLATE = """
    Cette requ√™te SPARQL d√©limit√©e par des triples backticks
    ```
    {generated_sparql}
    ```
    n'est pas valide.
    L'erreur d√©limit√©e par des triples backticks est
    ```
    {error_message}
    ```
    Donnez-moi une version correcte de la requ√™te SPARQL.
    Ne modifiez pas la logique de la requ√™te.
    N'incluez aucune explication ou excuse dans vos r√©ponses.
    Ne mettez pas la requ√™te entre backticks.
    N'incluez aucun autre texte que la requ√™te SPARQL g√©n√©r√©e.
    L'ontologie du sch√©ma d√©limit√©e par des triples backticks au format Turtle est :
    ```
    {schema}
    ```
    """

    GRAPHDB_SPARQL_FIX_PROMPT = PromptTemplate(
        input_variables=["error_message", "generated_sparql", "schema"],
        template=GRAPHDB_SPARQL_FIX_TEMPLATE,
    )
  ````

- `max_fix_retries`

    Valeur par d√©faut : `5`

### "R√©pondre" √† l'invite

L'invite est utilis√©e pour r√©pondre √† la question en fonction des r√©sultats renvoy√©s par la base de donn√©es et de la question initiale de l'utilisateur. Par d√©faut, le LLM re√ßoit l'instruction de n'utiliser que les informations provenant du(des) r√©sultat(s) renvoy√©(s). Si l'ensemble des r√©sultats est vide, le LLM doit informer qu'il ne peut pas r√©pondre √† la question.

- `qa_prompt`

  Valeur par d√©faut :
  ````python
    GRAPHDB_QA_TEMPLATE = """T√¢che : G√©n√©rer une r√©ponse en langage naturel √† partir des r√©sultats d'une requ√™te SPARQL.
    Vous √™tes un assistant qui cr√©e des r√©ponses bien √©crites et compr√©hensibles par l'humain.
    La partie information contient les informations fournies, que vous pouvez utiliser pour construire une r√©ponse.
    Les informations fournies sont fiables, vous ne devez jamais les remettre en question ou essayer d'utiliser vos connaissances internes pour les corriger.
    Faites en sorte que votre r√©ponse sonne comme si les informations provenaient d'un assistant IA, mais n'ajoutez aucune information.
    N'utilisez pas de connaissances internes pour r√©pondre √† la question, dites simplement que vous ne savez pas s'il n'y a pas d'informations disponibles.
    Informations :
    {context}

    Question : {prompt}
    R√©ponse utile :"""
    GRAPHDB_QA_PROMPT = PromptTemplate(
        input_variables=["context", "prompt"], template=GRAPHDB_QA_TEMPLATE
    )
  ````

Une fois que vous aurez fini de jouer avec QA avec GraphDB, vous pourrez arr√™ter l'environnement Docker en ex√©cutant
``
docker compose down -v --remove-orphans
``
depuis le r√©pertoire contenant le fichier Docker compose.
