---
translated: true
---

# Déploiements MLflow pour les LLM

>[Les déploiements MLflow pour les LLM](https://www.mlflow.org/docs/latest/llms/deployments/index.html) est un outil puissant conçu pour simplifier l'utilisation et la gestion de divers fournisseurs de modèles de langage de grande taille (LLM), tels qu'OpenAI et Anthropic, au sein d'une organisation. Il offre une interface de haut niveau qui simplifie l'interaction avec ces services en fournissant un point de terminaison unifié pour gérer les requêtes spécifiques aux LLM.

## Installation et configuration

Installez `mlflow` avec les dépendances des déploiements MLflow :

```sh
pip install 'mlflow[genai]'
```

Définissez la clé API OpenAI en tant que variable d'environnement :

```sh
export OPENAI_API_KEY=...
```

Créez un fichier de configuration :

```yaml
endpoints:
  - name: completions
    endpoint_type: llm/v1/completions
    model:
      provider: openai
      name: text-davinci-003
      config:
        openai_api_key: $OPENAI_API_KEY

  - name: embeddings
    endpoint_type: llm/v1/embeddings
    model:
      provider: openai
      name: text-embedding-ada-002
      config:
        openai_api_key: $OPENAI_API_KEY
```

Démarrez le serveur de déploiements :

```sh
mlflow deployments start-server --config-path /path/to/config.yaml
```

## Exemple fourni par `MLflow`

>Le module `mlflow.langchain` fournit une API pour journaliser et charger les modèles `LangChain`.
> Ce module exporte des modèles LangChain multivariés dans le style langchain et des modèles LangChain univariés
> dans le style pyfunc.

Consultez la [documentation de l'API et les exemples](https://www.mlflow.org/docs/latest/llms/langchain/index.html) pour plus d'informations.

## Exemple de complétion

```python
<!--IMPORTS:[{"imported": "LLMChain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html", "title": "MLflow Deployments for LLMs"}, {"imported": "Mlflow", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.mlflow.Mlflow.html", "title": "MLflow Deployments for LLMs"}]-->
import mlflow
from langchain.chains import LLMChain, PromptTemplate
from langchain_community.llms import Mlflow

llm = Mlflow(
    target_uri="http://127.0.0.1:5000",
    endpoint="completions",
)

llm_chain = LLMChain(
    llm=Mlflow,
    prompt=PromptTemplate(
        input_variables=["adjective"],
        template="Tell me a {adjective} joke",
    ),
)
result = llm_chain.run(adjective="funny")
print(result)

with mlflow.start_run():
    model_info = mlflow.langchain.log_model(chain, "model")

model = mlflow.pyfunc.load_model(model_info.model_uri)
print(model.predict([{"adjective": "funny"}]))
```

## Exemple d'embeddings

```python
<!--IMPORTS:[{"imported": "MlflowEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.mlflow.MlflowEmbeddings.html", "title": "MLflow Deployments for LLMs"}]-->
from langchain_community.embeddings import MlflowEmbeddings

embeddings = MlflowEmbeddings(
    target_uri="http://127.0.0.1:5000",
    endpoint="embeddings",
)

print(embeddings.embed_query("hello"))
print(embeddings.embed_documents(["hello"]))
```

## Exemple de chat

```python
<!--IMPORTS:[{"imported": "ChatMlflow", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.mlflow.ChatMlflow.html", "title": "MLflow Deployments for LLMs"}, {"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "MLflow Deployments for LLMs"}, {"imported": "SystemMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html", "title": "MLflow Deployments for LLMs"}]-->
from langchain_community.chat_models import ChatMlflow
from langchain_core.messages import HumanMessage, SystemMessage

chat = ChatMlflow(
    target_uri="http://127.0.0.1:5000",
    endpoint="chat",
)

messages = [
    SystemMessage(
        content="You are a helpful assistant that translates English to French."
    ),
    HumanMessage(
        content="Translate this sentence from English to French: I love programming."
    ),
]
print(chat(messages))
```
