---
translated: true
---

# GPT4All

Cette page couvre comment utiliser le wrapper `GPT4All` dans LangChain. Le tutoriel est divisé en deux parties : l'installation et la configuration, suivies d'une utilisation avec un exemple.

## Installation et configuration

- Installez le package Python avec `pip install gpt4all`
- Téléchargez un [modèle GPT4All](https://gpt4all.io/index.html) et placez-le dans le répertoire de votre choix

Dans cet exemple, nous utilisons `mistral-7b-openorca.Q4_0.gguf` (Meilleur modèle de chat rapide global) :

```bash
mkdir models
wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf
```

## Utilisation

### GPT4All

Pour utiliser le wrapper GPT4All, vous devez fournir le chemin du fichier du modèle pré-entraîné et la configuration du modèle.

```python
<!--IMPORTS:[{"imported": "GPT4All", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.gpt4all.GPT4All.html", "title": "GPT4All"}]-->
from langchain_community.llms import GPT4All

# Instantiate the model. Callbacks support token-wise streaming
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text
response = model.invoke("Once upon a time, ")
```

Vous pouvez également personnaliser les paramètres de génération, tels que n_predict, temp, top_p, top_k et autres.

Pour diffuser les prédictions du modèle, ajoutez un CallbackManager.

```python
<!--IMPORTS:[{"imported": "GPT4All", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.gpt4all.GPT4All.html", "title": "GPT4All"}, {"imported": "StreamingStdOutCallbackHandler", "source": "langchain.callbacks.streaming_stdout", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html", "title": "GPT4All"}, {"imported": "StreamlitCallbackHandler", "source": "langchain.callbacks.streamlit", "docs": "https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streamlit.StreamlitCallbackHandler.html", "title": "GPT4All"}]-->
from langchain_community.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# There are many CallbackHandlers supported, such as
# from langchain.callbacks.streamlit import StreamlitCallbackHandler

callbacks = [StreamingStdOutCallbackHandler()]
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text. Tokens are streamed through the callback manager.
model("Once upon a time, ", callbacks=callbacks)
```

## Fichier du modèle

Vous pouvez trouver des liens de téléchargement de fichiers de modèles sur [https://gpt4all.io/](https://gpt4all.io/index.html).

Pour un parcours plus détaillé de cela, consultez [ce notebook](/docs/integrations/llms/gpt4all)
