---
translated: true
---

# PremAI

>[PremAI](https://app.premai.io) est une plateforme unifi√©e qui vous permet de construire des applications pr√™tes pour la production et aliment√©es par GenAI avec le moins d'efforts possible, afin que vous puissiez vous concentrer davantage sur l'exp√©rience utilisateur et la croissance globale.

## ChatPremAI

Cet exemple explique comment utiliser LangChain pour interagir avec diff√©rents mod√®les de chat avec `ChatPremAI`.

### Installation et configuration

Nous commen√ßons par installer langchain et premai-sdk. Vous pouvez taper la commande suivante pour installer :

```bash
pip install premai langchain
```

Avant de continuer, assurez-vous d'avoir cr√©√© un compte sur PremAI et d√©marr√© un projet. Sinon, voici comment vous pouvez commencer gratuitement :

1. Connectez-vous √† [PremAI](https://app.premai.io/accounts/login/), si c'est la premi√®re fois, et cr√©ez votre cl√© API [ici](https://app.premai.io/api_keys/).

2. Allez sur [app.premai.io](https://app.premai.io) et cela vous am√®nera au tableau de bord du projet.

3. Cr√©ez un projet et cela g√©n√©rera un ID de projet (√©crit sous forme d'ID). Cet ID vous aidera √† interagir avec votre application d√©ploy√©e.

4. Rendez-vous sur LaunchPad (celui avec l'ic√¥ne üöÄ). Et l√†, d√©ployez le mod√®le de votre choix. Votre mod√®le par d√©faut sera `gpt-4`. Vous pouvez √©galement d√©finir et fixer diff√©rents param√®tres de g√©n√©ration (comme max-tokens, temp√©rature, etc.) et pr√©-d√©finir votre invite syst√®me.

F√©licitations pour avoir cr√©√© votre premi√®re application d√©ploy√©e sur PremAI üéâ Maintenant, nous pouvons utiliser langchain pour interagir avec notre application.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "PremAI"}, {"imported": "SystemMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html", "title": "PremAI"}, {"imported": "ChatPremAI", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.premai.ChatPremAI.html", "title": "PremAI"}]-->
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.chat_models import ChatPremAI
```

### Configuration de l'instance ChatPrem dans LangChain

Une fois que nous avons import√© nos modules requis, configurons notre client. Pour l'instant, supposons que notre `project_id` est 8. Mais assurez-vous d'utiliser votre ID de projet, sinon, cela g√©n√©rera une erreur.

Pour utiliser langchain avec prem, vous n'avez pas besoin de passer de nom de mod√®le ou de d√©finir des param√®tres avec notre client de chat. Tous ces param√®tres utiliseront le nom de mod√®le par d√©faut et les param√®tres du mod√®le LaunchPad.

`REMARQUE :` Si vous modifiez le `model_name` ou tout autre param√®tre comme `temperature` lors de la configuration du client, cela remplacera les configurations par d√©faut existantes.

```python
import os
import getpass

if "PREMAI_API_KEY" not in os.environ:
    os.environ["PREMAI_API_KEY"] = getpass.getpass("PremAI API Key:")

chat = ChatPremAI(project_id=8)
```

### Appel du mod√®le

Maintenant, vous √™tes pr√™t. Nous pouvons maintenant commencer √† interagir avec notre application. `ChatPremAI` prend en charge deux m√©thodes `invoke` (qui est la m√™me que `generate`) et `stream`.

La premi√®re nous donnera un r√©sultat statique. Tandis que la seconde diffusera les jetons un par un. Voici comment vous pouvez g√©n√©rer des compl√©ments de type chat.

### G√©n√©ration

```python
human_message = HumanMessage(content="Who are you?")

chat.invoke([human_message])
```

Cela a l'air int√©ressant, n'est-ce pas ? J'ai d√©fini mon invite syst√®me LaunchPad par d√©faut comme : `Parle toujours comme un pirate` Vous pouvez √©galement remplacer l'invite syst√®me par d√©faut si n√©cessaire. Voici comment vous pouvez le faire.

```python
system_message = SystemMessage(content="You are a friendly assistant.")
human_message = HumanMessage(content="Who are you?")

chat.invoke([system_message, human_message])
```

Vous pouvez √©galement modifier les param√®tres de g√©n√©ration lors de l'appel du mod√®le. Voici comment vous pouvez proc√©der :

```python
chat.invoke(
    [system_message, human_message],
    temperature = 0.7, max_tokens = 20, top_p = 0.95
)
```

### Remarques importantes :

Avant de continuer, veuillez noter que la version actuelle de ChatPrem ne prend pas en charge les param√®tres : [n](https://platform.openai.com/docs/api-reference/chat/create#chat-create-n) et [stop](https://platform.openai.com/docs/api-reference/chat/create#chat-create-stop) ne sont pas pris en charge.

Nous fournirons un support pour ces deux param√®tres ci-dessus dans les versions ult√©rieures.

### Diffusion en continu

Et enfin, voici comment vous pouvez diffuser les jetons pour des applications de chat dynamiques.

```python
import sys

for chunk in chat.stream("hello how are you"):
    sys.stdout.write(chunk.content)
    sys.stdout.flush()
```

Comme ci-dessus, si vous voulez remplacer l'invite syst√®me et les param√®tres de g√©n√©ration, voici comment vous pouvez le faire.

```python
import sys

for chunk in chat.stream(
    "hello how are you",
    system_prompt = "You are an helpful assistant", temperature = 0.7, max_tokens = 20
):
    sys.stdout.write(chunk.content)
    sys.stdout.flush()
```

## Int√©gration

Dans cette section, nous allons discuter de la fa√ßon dont nous pouvons acc√©der √† diff√©rents mod√®les d'int√©gration √† l'aide de `PremEmbeddings`. Commen√ßons par faire quelques importations et d√©finir notre objet d'int√©gration.

```python
from langchain_community.embeddings import PremEmbeddings
```

Une fois que nous avons import√© nos modules requis, configurons notre client. Pour l'instant, supposons que notre `project_id` est 8. Mais assurez-vous d'utiliser votre ID de projet, sinon, cela g√©n√©rera une erreur.

```python

import os
import getpass

if os.environ.get("PREMAI_API_KEY") is None:
    os.environ["PREMAI_API_KEY"] = getpass.getpass("PremAI API Key:")

# Define a model as a required parameter here since there is no default embedding model

model = "text-embedding-3-large"
embedder = PremEmbeddings(project_id=8, model=model)
```

Nous avons d√©fini notre mod√®le d'int√©gration. Nous prenons en charge de nombreux mod√®les d'int√©gration. Voici un tableau qui montre le nombre de mod√®les d'int√©gration que nous prenons en charge.

| Fournisseur | Slug                                     | Jetons de contexte |
|-------------|------------------------------------------|-------------------|
| cohere      | embed-english-v3.0                       | N/A               |
| openai      | text-embedding-3-small                   | 8191              |
| openai      | text-embedding-3-large                   | 8191              |
| openai      | text-embedding-ada-002                   | 8191              |
| replicate   | replicate/all-mpnet-base-v2              | N/A               |
| together    | togethercomputer/Llama-2-7B-32K-Instruct | N/A               |
| mistralai   | mistral-embed                            | 4096              |

Pour changer de mod√®le, vous n'avez qu'√† copier le `slug` et acc√©der √† votre mod√®le d'int√©gration. Maintenant, commen√ßons √† utiliser notre mod√®le d'int√©gration avec une seule requ√™te suivie de plusieurs requ√™tes (√©galement appel√©es document).

```python
query = "Hello, this is a test query"
query_result = embedder.embed_query(query)

# Let's print the first five elements of the query embedding vector

print(query_result[:5])
```

Enfin, int√©grons un document.

```python
documents = [
    "This is document1",
    "This is document2",
    "This is document3"
]

doc_result = embedder.embed_documents(documents)

# Similar to the previous result, let's print the first five element
# of the first document vector

print(doc_result[0][:5])
```
