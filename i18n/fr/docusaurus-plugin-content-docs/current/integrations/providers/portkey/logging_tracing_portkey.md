---
translated: true
---

# Journalisation, tra√ßage et surveillance

Lors de la construction d'applications ou d'agents √† l'aide de Langchain, vous finissez par effectuer plusieurs appels d'API pour r√©pondre √† une seule demande d'utilisateur. Cependant, ces requ√™tes ne sont pas encha√Æn√©es lorsque vous voulez les analyser. Avec [**Portkey**](/docs/integrations/providers/portkey/), tous les embeddings, les compl√©ments et les autres requ√™tes d'une seule demande d'utilisateur seront journalis√©s et trac√©s avec un ID commun, vous permettant d'avoir une visibilit√© compl√®te des interactions des utilisateurs.

Ce notebook sert de guide √©tape par √©tape sur la fa√ßon de journaliser, de tracer et de surveiller les appels LLM Langchain √† l'aide de `Portkey` dans votre application Langchain.

Commen√ßons par importer Portkey, OpenAI et les outils Agent.

```python
import os

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_openai import ChatOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders
```

Collez votre cl√© d'API OpenAI ci-dessous. [(Vous pouvez la trouver ici)](https://platform.openai.com/account/api-keys)

```python
os.environ["OPENAI_API_KEY"] = "..."
```

## Obtenir la cl√© d'API Portkey

1. Inscrivez-vous √† [Portkey ici](https://app.portkey.ai/signup)
2. Sur votre [tableau de bord](https://app.portkey.ai/), cliquez sur l'ic√¥ne de profil en bas √† gauche, puis sur "Copier la cl√© API"
3. Collez-la ci-dessous

```python
PORTKEY_API_KEY = "..."  # Paste your Portkey API Key here
```

## D√©finir l'ID de trace

1. D√©finissez l'ID de trace pour votre requ√™te ci-dessous
2. L'ID de trace peut √™tre commun √† tous les appels d'API provenant d'une seule demande

```python
TRACE_ID = "uuid-trace-id"  # Set trace id here
```

## G√©n√©rer les en-t√™tes Portkey

```python
portkey_headers = createHeaders(
    api_key=PORTKEY_API_KEY, provider="openai", trace_id=TRACE_ID
)
```

D√©finissez les invites et les outils √† utiliser

```python
from langchain import hub
from langchain_core.tools import tool

prompt = hub.pull("hwchase17/openai-tools-agent")


@tool
def multiply(first_int: int, second_int: int) -> int:
    """Multiply two integers together."""
    return first_int * second_int


@tool
def exponentiate(base: int, exponent: int) -> int:
    "Exponentiate the base to the exponent power."
    return base**exponent


tools = [multiply, exponentiate]
```

Ex√©cutez votre agent comme d'habitude. Le **seul** changement est que nous **inclurons les en-t√™tes ci-dessus** dans la requ√™te maintenant.

```python
model = ChatOpenAI(
    base_url=PORTKEY_GATEWAY_URL, default_headers=portkey_headers, temperature=0
)

# Construct the OpenAI Tools agent
agent = create_openai_tools_agent(model, tools, prompt)

# Create an agent executor by passing in the agent and tools
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

agent_executor.invoke(
    {
        "input": "Take 3 to the fifth power and multiply that by thirty six, then square the result"
    }
)
```

```output


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `exponentiate` with `{'base': 3, 'exponent': 5}`


[0m[33;1m[1;3m243[0m[32;1m[1;3m
Invoking: `multiply` with `{'first_int': 243, 'second_int': 36}`


[0m[36;1m[1;3m8748[0m[32;1m[1;3m
Invoking: `exponentiate` with `{'base': 8748, 'exponent': 2}`


[0m[33;1m[1;3m76527504[0m[32;1m[1;3mThe result of taking 3 to the fifth power, multiplying it by 36, and then squaring the result is 76,527,504.[0m

[1m> Finished chain.[0m
```

```output
{'input': 'Take 3 to the fifth power and multiply that by thirty six, then square the result',
 'output': 'The result of taking 3 to the fifth power, multiplying it by 36, and then squaring the result is 76,527,504.'}
```

## Comment fonctionne la journalisation et le tra√ßage sur Portkey

**Journalisation**
- L'envoi de votre requ√™te via Portkey garantit que toutes les requ√™tes sont journalis√©es par d√©faut
- Chaque journal de requ√™te contient `timestamp`, `nom du mod√®le`, `co√ªt total`, `temps de requ√™te`, `json de la requ√™te`, `json de la r√©ponse` et d'autres fonctionnalit√©s Portkey

**[Tra√ßage](https://portkey.ai/docs/product/observability-modern-monitoring-for-llms/traces)**
- L'ID de trace est transmis avec chaque requ√™te et est visible dans les journaux du tableau de bord Portkey
- Vous pouvez √©galement d√©finir un **ID de trace distinct** pour chaque requ√™te si vous le souhaitez
- Vous pouvez √©galement ajouter les commentaires de l'utilisateur √† un ID de trace. [Plus d'informations ici](https://portkey.ai/docs/product/observability-modern-monitoring-for-llms/feedback)

Pour la requ√™te ci-dessus, vous pourrez voir la trace du journal complet comme ceci
![Afficher les traces Langchain sur Portkey](https://assets.portkey.ai/docs/agent_tracing.gif)

## Fonctionnalit√©s avanc√©es de LLMOps - Mise en cache, √©tiquetage, nouvelles tentatives

En plus de la journalisation et du tra√ßage, Portkey fournit d'autres fonctionnalit√©s qui ajoutent des capacit√©s de production √† vos workflows existants :

**Mise en cache**

R√©pondez aux requ√™tes des clients pr√©c√©demment servis √† partir du cache au lieu de les envoyer √† nouveau √† OpenAI. Correspondance de cha√Ænes exactes OU s√©mantiquement similaires. Le cache peut √©conomiser les co√ªts et r√©duire les latences jusqu'√† 20 fois. [Documentation](https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/cache-simple-and-semantic)

**Nouvelles tentatives**

Retraitez automatiquement les requ√™tes d'API non r√©ussies **jusqu'√† 5** fois. Utilise une strat√©gie de **retour exponentiel**, qui espacer les tentatives de nouvelle pour √©viter la surcharge du r√©seau. [Documentation](https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations)

**√âtiquetage**

Suivez et auditez chaque interaction utilisateur de mani√®re d√©taill√©e avec des √©tiquettes pr√©d√©finies. [Documentation](https://portkey.ai/docs/product/observability-modern-monitoring-for-llms/metadata)
