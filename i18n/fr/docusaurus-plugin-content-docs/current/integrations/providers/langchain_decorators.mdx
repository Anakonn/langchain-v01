---
translated: true
---

# LangChain Decorators ‚ú®

~~~
Avertissement : `LangChain decorators` n'est pas cr√©√© par l'√©quipe LangChain et n'est pas pris en charge par elle.
~~~

>`LangChain decorators` est une couche au-dessus de LangChain qui fournit du sucre syntaxique üç≠ pour √©crire des invites et des cha√Ænes langchain personnalis√©es
>
>Pour les commentaires, les probl√®mes, les contributions - veuillez soulever un probl√®me ici :
>[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)

Principes et avantages principaux :

- une mani√®re plus `pythonique` d'√©crire du code
- √©crire des invites sur plusieurs lignes qui ne briseront pas le flux de votre code avec l'indentation
- utiliser le support int√©gr√© de l'IDE pour le **hinting**, le **type checking** et les **popups avec la documentation** pour jeter un coup d'≈ìil rapide √† la fonction pour voir l'invite, les param√®tres qu'elle consomme, etc.
- tirer parti de toute la puissance de l'√©cosyst√®me ü¶úüîó LangChain
- ajouter un support pour les **param√®tres optionnels**
- partager facilement les param√®tres entre les invites en les liant √† une seule classe

Voici un exemple simple de code √©crit avec **LangChain Decorators ‚ú®**

```python

@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:
    """
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    """
    return

# run it naturally
write_me_short_post(topic="starwars")
# or
write_me_short_post(topic="starwars", platform="redit")
```

# D√©marrage rapide

## Installation

```bash
pip install langchain_decorators
```

## Exemples

Une bonne id√©e pour commencer est de revoir les exemples ici :
 - [notebook jupyter](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)
 - [notebook colab](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)

# D√©finition d'autres param√®tres

Ici, nous marquons simplement une fonction comme une invite avec le d√©corateur `llm_prompt`, la transformant effectivement en une LLMChain. Au lieu de l'ex√©cuter

LLMchain standard prend beaucoup plus de param√®tres d'initialisation que juste les variables d'entr√©e et l'invite... voici ce d√©tail d'impl√©mentation cach√© dans le d√©corateur.
Voici comment cela fonctionne :

1. Utilisation des **param√®tres globaux** :

```python
# define global settings for all prompty (if not set - chatGPT is the current default)
from langchain_decorators import GlobalSettings

GlobalSettings.define_settings(
    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally
    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming
)
```

2. Utilisation de **types d'invite** pr√©d√©finis

```python
#You can change the default prompt types
from langchain_decorators import PromptTypes, PromptTypeSettings

PromptTypes.AGENT_REASONING.llm = ChatOpenAI()

# Or you can just define your own ones:
class MyCustomPromptTypes(PromptTypes):
    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))

@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4)
def write_a_complicated_code(app_idea:str)->str:
    ...

```

3. D√©finir les param√®tres **directement dans le d√©corateur**

```python
<!--IMPORTS:[{"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "LangChain Decorators \u2728"}]-->
from langchain_openai import OpenAI

@llm_prompt(
    llm=OpenAI(temperature=0.7),
    stop_tokens=["\nObservation"],
    ...
    )
def creative_writer(book_title:str)->str:
    ...
```

## Passer une m√©moire et/ou des rappels :

Pour passer l'un de ces √©l√©ments, il suffit de les d√©clarer dans la fonction (ou d'utiliser kwargs pour passer n'importe quoi)

```python

@llm_prompt()
async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):
    """
    {history_key}
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass

await write_me_short_post(topic="old movies")

```

# Streaming simplifi√©

Si nous voulons tirer parti du streaming :
 - nous devons d√©finir l'invite comme une fonction asynchrone
 - activer le streaming sur le d√©corateur, ou nous pouvons d√©finir PromptType avec le streaming activ√©
 - capturer le flux √† l'aide de StreamingContext

De cette fa√ßon, nous marquons simplement quelle invite doit √™tre diffus√©e en continu, sans avoir besoin de bidouiller avec le LLM √† utiliser, en passant le gestionnaire de diffusion en continu cr√©√© et distribu√© dans une partie particuli√®re de notre cha√Æne... il suffit d'activer/d√©sactiver le streaming sur l'invite/le type d'invite...

Le streaming ne se produira que si nous l'appelons dans le contexte de diffusion en continu... l√†, nous pouvons d√©finir une fonction simple pour g√©rer le flux

```python
# this code example is complete and should run as it is

from langchain_decorators import StreamingContext, llm_prompt

# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)
# note that only async functions can be streamed (will get an error if it's not)
@llm_prompt(capture_stream=True)
async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass



# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real world
tokens=[]
def capture_stream_func(new_token:str):
    tokens.append(new_token)

# if we want to capture the stream, we need to wrap the execution into StreamingContext...
# this will allow us to capture the stream even if the prompt call is hidden inside higher level method
# only the prompts marked with capture_stream will be captured here
with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):
    result = await run_prompt()
    print("Stream finished ... we can distinguish tokens thanks to alternating colors")


print("\nWe've captured",len(tokens),"tokensüéâ\n")
print("Here is the result:")
print(result)
```

# D√©clarations d'invite

Par d√©faut, l'invite est la totalit√© de la documentation de la fonction, sauf si vous marquez votre invite

## Documentation de votre invite

Nous pouvons sp√©cifier quelle partie de notre documentation est la d√©finition de l'invite, en sp√©cifiant un bloc de code avec l'√©tiquette de langue `<prompt>`

```python
@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.

    It needs to be a code block, marked as a `<prompt>` language
    ```<prompt>
    Write me a short header for my post about {topic} for {platform} platform.
    It should be for {audience} audience.
    (Max 15 words)
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    return
```

## Invite de messages de discussion

Pour les mod√®les de discussion, il est tr√®s utile de d√©finir l'invite comme un ensemble de mod√®les de messages... voici comment faire :

```python
@llm_prompt
def simulate_conversation(human_input:str, agent_role:str="a pirate"):
    """
    ## System message
     - note the `:system` sufix inside the <prompt:_role_> tag


    ```<prompt:system>
    You are a {agent_role} hacker. You mus act like one.
    You reply always in code, using python or javascript code block...
    for example:

    ... do not reply with anything else.. just with code - respecting your role.
    ```

    # human message
    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)
    ``` <prompt:user>
    Helo, who are you
    ```
    a reply:


    ``` <prompt:assistant>
    \``` python <<- escaping inner code block with \ that should be part of the prompt
    def hello():
        print("Argh... hello you pesky pirate")
    \```
    ```

    we can also add some history using placeholder
    ```<prompt:placeholder>
    {history}
    ```
    ```<prompt:user>
    {human_input}
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    pass

```

les r√¥les ici sont les r√¥les natifs du mod√®le (assistant, utilisateur, syst√®me pour chatGPT)

# Sections facultatives

- vous pouvez d√©finir des sections enti√®res de votre invite qui doivent √™tre facultatives
- si un √©l√©ment d'entr√©e dans la section est manquant, toute la section ne sera pas rendue

la syntaxe pour cela est la suivante :

```python
@llm_prompt
def prompt_with_optional_partials():
    """
    this text will be rendered always, but

    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}

    you can also place it in between the words
    this too will be rendered{? , but
        this  block will be rendered only if {this_value} and {this_value}
        is not empty?} !
    """
```

# Analyseurs de sortie

- le d√©corateur llm_prompt essaie nativement de d√©tecter le meilleur analyseur de sortie en fonction du type de sortie. (s'il n'est pas d√©fini, il renvoie la cha√Æne brute)
- les sorties de liste, de dictionnaire et de pydantic sont √©galement prises en charge nativement (automatiquement)

```python
# this code example is complete and should run as it is

from langchain_decorators import llm_prompt

@llm_prompt
def write_name_suggestions(company_business:str, count:int)->list:
    """ Write me {count} good name suggestions for company that {company_business}
    """
    pass

write_name_suggestions(company_business="sells cookies", count=5)
```

## Structures plus complexes

pour les dictionnaires / pydantic, vous devez sp√©cifier les instructions de formatage...
cela peut √™tre fastidieux, c'est pourquoi vous pouvez laisser l'analyseur de sortie vous g√©n√©rer les instructions en fonction du mod√®le (pydantic)

```python
from langchain_decorators import llm_prompt
from pydantic import BaseModel, Field


class TheOutputStructureWeExpect(BaseModel):
    name:str = Field (description="The name of the company")
    headline:str = Field( description="The description of the company (for landing page)")
    employees:list[str] = Field(description="5-8 fake employee names with their positions")

@llm_prompt()
def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:
    """ Generate a fake company that {company_business}
    {FORMAT_INSTRUCTIONS}
    """
    return

company = fake_company_generator(company_business="sells cookies")

# print the result nicely formatted
print("Company name: ",company.name)
print("company headline: ",company.headline)
print("company employees: ",company.employees)

```

# Lier l'invite √† un objet

```python
from pydantic import BaseModel
from langchain_decorators import llm_prompt

class AssistantPersonality(BaseModel):
    assistant_name:str
    assistant_role:str
    field:str

    @property
    def a_property(self):
        return "whatever"

    def hello_world(self, function_kwarg:str=None):
        """
        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method
        """


    @llm_prompt
    def introduce_your_self(self)->str:
        """
        ```¬†<prompt:system>
        You are an assistant named {assistant_name}.
        Your role is to act as {assistant_role}
        ```
        ```<prompt:user>
        Introduce your self (in less than 20 words)
        ```
        """



personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")

print(personality.introduce_your_self(personality))
```

# Autres exemples :

- ces exemples et quelques autres sont √©galement disponibles dans le [notebook colab ici](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)
- y compris la [r√©impl√©mentation de l'agent ReAct](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp) en utilisant purement les d√©corateurs langchain
