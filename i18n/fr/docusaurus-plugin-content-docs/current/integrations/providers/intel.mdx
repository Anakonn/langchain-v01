---
translated: true
---

# Intel

>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel) est l'interface entre les biblioth√®ques ü§ó Transformers et Diffusers et les diff√©rents outils et biblioth√®ques fournis par Intel pour acc√©l√©rer les pipelines de bout en bout sur les architectures Intel.

>[Intel¬Æ Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX) est une bo√Æte √† outils innovante con√ßue pour acc√©l√©rer GenAI/LLM partout avec les performances optimales des mod√®les bas√©s sur Transformer sur diverses plateformes Intel, notamment Intel Gaudi2, Intel CPU et Intel GPU.

Cette page explique comment utiliser optimum-intel et ITREX avec LangChain.

## Optimum-intel

Toutes les fonctionnalit√©s li√©es √† [optimum-intel](https://github.com/huggingface/optimum-intel.git) et [IPEX](https://github.com/intel/intel-extension-for-pytorch).

### Installation

Installez optimum-intel et ipex en utilisant :

```bash
pip install optimum[neural-compressor]
pip install intel_extension_for_pytorch
```

Veuillez suivre les instructions d'installation comme indiqu√© ci-dessous :

* Installez optimum-intel comme indiqu√© [ici](https://github.com/huggingface/optimum-intel).
* Installez IPEX comme indiqu√© [ici](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu).

### Mod√®les d'int√©gration

Voir un [exemple d'utilisation](/docs/integrations/text_embedding/optimum_intel).
Nous proposons √©galement un notebook tutoriel complet "rag_with_quantized_embeddings.ipynb" pour utiliser l'int√©grateur dans un pipeline RAG dans le r√©pertoire cookbook.

```python
<!--IMPORTS:[{"imported": "QuantizedBiEncoderEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.optimum_intel.QuantizedBiEncoderEmbeddings.html", "title": "Intel"}]-->
from langchain_community.embeddings import QuantizedBiEncoderEmbeddings
```

## Intel¬Æ Extension for Transformers (ITREX)

(ITREX) est une bo√Æte √† outils innovante pour acc√©l√©rer les mod√®les bas√©s sur Transformer sur les plateformes Intel, en particulier, efficace sur le processeur Intel Xeon Scalable de 4e g√©n√©ration Sapphire Rapids (code-nomm√© Sapphire Rapids).

La quantification est un processus qui consiste √† r√©duire la pr√©cision de ces poids en les repr√©sentant √† l'aide d'un nombre plus petit de bits. La quantification uniquement des poids se concentre sp√©cifiquement sur la quantification des poids du r√©seau neuronal tout en conservant les autres composants, tels que les activations, dans leur pr√©cision d'origine.

Alors que les mod√®les de langage √† grande √©chelle (LLM) deviennent plus r√©pandus, il y a un besoin croissant de nouvelles m√©thodes de quantification am√©lior√©es qui peuvent r√©pondre aux demandes de calcul de ces architectures modernes tout en maintenant la pr√©cision. Par rapport √† la [quantification normale](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md) comme W8A8, la quantification uniquement des poids est probablement un meilleur compromis pour √©quilibrer les performances et la pr√©cision, car nous verrons ci-dessous que le goulot d'√©tranglement du d√©ploiement des LLM est la bande passante m√©moire et normalement la quantification uniquement des poids pourrait conduire √† une meilleure pr√©cision.

Ici, nous pr√©senterons les mod√®les d'int√©gration et la quantification uniquement des poids pour les mod√®les de langage √† grande √©chelle avec ITREX. La quantification uniquement des poids est une technique utilis√©e dans l'apprentissage en profondeur pour r√©duire les exigences de m√©moire et de calcul des r√©seaux neuronaux. Dans le contexte des r√©seaux de neurones profonds, les param√®tres du mod√®le, √©galement connus sous le nom de poids, sont g√©n√©ralement repr√©sent√©s √† l'aide de nombres √† virgule flottante, ce qui peut consommer une quantit√© importante de m√©moire et n√©cessiter des ressources de calcul intensives.

Toutes les fonctionnalit√©s li√©es √† [intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers).

### Installation

Installez intel-extension-for-transformers. Pour les exigences syst√®me et d'autres conseils d'installation, veuillez vous r√©f√©rer au [Guide d'installation](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)

```bash
pip install intel-extension-for-transformers
```

Installez les autres packages requis.

```bash
pip install -U torch onnx accelerate datasets
```

### Mod√®les d'int√©gration

Voir un [exemple d'utilisation](/docs/integrations/text_embedding/itrex).

```python
<!--IMPORTS:[{"imported": "QuantizedBgeEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.itrex.QuantizedBgeEmbeddings.html", "title": "Intel"}]-->
from langchain_community.embeddings import QuantizedBgeEmbeddings
```

### Quantification uniquement des poids avec ITREX

Voir un [exemple d'utilisation](/docs/integrations/llms/weight_only_quantization).

## D√©tail des param√®tres de configuration

Voici le d√©tail de la classe `WeightOnlyQuantConfig`.

#### weight_dtype (cha√Æne) : Type de donn√©es des poids, la valeur par d√©faut est "nf4".

Nous prenons en charge la quantification des poids dans les types de donn√©es suivants pour le stockage (weight_dtype dans WeightOnlyQuantConfig) :
* **int8** : Utilise le type de donn√©es 8 bits.
* **int4_fullrange** : Utilise la valeur -8 de la plage int4 par rapport √† la plage int4 normale [-7,7].
* **int4_clip** : Rogne et conserve les valeurs dans la plage int4, en mettant les autres √† z√©ro.
* **nf4** : Utilise le type de donn√©es float 4 bits normalis√©.
* **fp4_e2m1** : Utilise le type de donn√©es float 4 bits r√©gulier. "e2" signifie que 2 bits sont utilis√©s pour l'exposant et "m1" signifie que 1 bit est utilis√© pour la mantisse.

#### compute_dtype (cha√Æne) : Type de donn√©es de calcul, la valeur par d√©faut est "fp32".

Bien que ces techniques stockent les poids en 4 ou 8 bits, le calcul se fait toujours en float32, bfloat16 ou int8 (compute_dtype dans WeightOnlyQuantConfig) :
* **fp32** : Utilise le type de donn√©es float32 pour le calcul.
* **bf16** : Utilise le type de donn√©es bfloat16 pour le calcul.
* **int8** : Utilise le type de donn√©es 8 bits pour le calcul.

#### llm_int8_skip_modules (liste des noms de modules) : Modules √† ignorer la quantification, la valeur par d√©faut est None.

Il s'agit d'une liste de modules √† ignorer la quantification.

#### scale_dtype (cha√Æne) : Le type de donn√©es d'√©chelle, la valeur par d√©faut est "fp32".

Prend en charge uniquement "fp32" (float32).

#### mse_range (bool√©en) : Rechercher la meilleure plage de rognage dans la plage [0,805, 1,0, 0,005], la valeur par d√©faut est False.

#### use_double_quant (bool√©en) : Quantifier l'√©chelle, la valeur par d√©faut est False.

Pas encore pris en charge.

#### double_quant_dtype (cha√Æne) : R√©serv√© pour la double quantification.

#### double_quant_scale_dtype (cha√Æne) : R√©serv√© pour la double quantification.

#### group_size (int) : Taille du groupe lors de la quantification.

#### scheme (cha√Æne) : Format dans lequel les poids seront quantifi√©s. La valeur par d√©faut est "sym".

* **sym** : Sym√©trique.
* **asym** : Asym√©trique.

#### algorithm (string) : Quel algorithme pour am√©liorer la pr√©cision. La valeur par d√©faut est "RTN"

* **RTN** : Round-to-nearest (RTN) est une m√©thode de quantification que nous pouvons imaginer de mani√®re tr√®s intuitive.
* **AWQ** : Prot√©ger seulement 1% des poids saillants peut grandement r√©duire l'erreur de quantification. Les canaux de poids saillants sont s√©lectionn√©s en observant la distribution de l'activation et du poids par canal. Les poids saillants sont √©galement quantifi√©s apr√®s avoir multipli√© un grand facteur d'√©chelle avant la quantification pour pr√©server.
* **TEQ** : Une transformation √©quivalente entra√Ænable qui pr√©serve la pr√©cision FP32 dans la quantification des poids uniquement.
