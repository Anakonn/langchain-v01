---
translated: true
---

# Google Memorystore pour Redis

> [Google Memorystore pour Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) est un service enti√®rement g√©r√© qui s'appuie sur le magasin de donn√©es en m√©moire Redis pour construire des caches d'applications offrant un acc√®s aux donn√©es en sous-milliseconde. √âtendez votre application de base de donn√©es pour construire des exp√©riences aliment√©es par l'IA en tirant parti des int√©grations Langchain de Memorystore pour Redis.

Ce notebook explique comment utiliser [Memorystore pour Redis](https://cloud.google.com/memorystore/docs/redis/memorystore-for-redis-overview) pour stocker des embeddings vectoriels avec la classe `MemorystoreVectorStore`.

En savoir plus sur le package sur [GitHub](https://github.com/googleapis/langchain-google-memorystore-redis-python/).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googleapis/langchain-google-memorystore-redis-python/blob/main/docs/vector_store.ipynb)

## Pr√©requis

## Avant de commencer

Pour ex√©cuter ce notebook, vous devrez :

* [Cr√©er un projet Google Cloud](https://developers.google.com/workspace/guides/create-project)
* [Activer l'API Memorystore pour Redis](https://console.cloud.google.com/flows/enableapi?apiid=redis.googleapis.com)
* [Cr√©er une instance Memorystore pour Redis](https://cloud.google.com/memorystore/docs/redis/create-instance-console). Assurez-vous que la version soit sup√©rieure ou √©gale √† 7.2.

### ü¶úüîó Installation de la biblioth√®que

L'int√©gration se trouve dans le package `langchain-google-memorystore-redis`, nous devons donc l'installer.

```python
%pip install -upgrade --quiet langchain-google-memorystore-redis langchain
```

**Colab uniquement :** D√©commentez la cellule suivante pour red√©marrer le noyau ou utilisez le bouton pour le faire. Pour Vertex AI Workbench, vous pouvez red√©marrer le terminal √† l'aide du bouton en haut.

```python
# # Automatically restart kernel after installs so that your environment can access the new packages
# import IPython

# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)
```

### ‚òÅ D√©finissez votre projet Google Cloud

D√©finissez votre projet Google Cloud afin de pouvoir utiliser les ressources Google Cloud dans ce notebook.

Si vous ne connaissez pas votre ID de projet, essayez ce qui suit :

* Ex√©cutez `gcloud config list`.
* Ex√©cutez `gcloud projects list`.
* Consultez la page d'assistance : [Localiser l'ID du projet](https://support.google.com/googleapi/answer/7014113).

```python
# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.

PROJECT_ID = "my-project-id"  # @param {type:"string"}

# Set the project id
!gcloud config set project {PROJECT_ID}
```

### üîê Authentification

Authentifiez-vous sur Google Cloud en tant qu'utilisateur IAM connect√© √† ce notebook afin d'acc√©der √† votre projet Google Cloud.

* Si vous utilisez Colab pour ex√©cuter ce notebook, utilisez la cellule ci-dessous et continuez.
* Si vous utilisez Vertex AI Workbench, consultez les instructions de configuration [ici](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env).

```python
from google.colab import auth

auth.authenticate_user()
```

## Utilisation de base

### Initialiser un index vectoriel

```python
import redis
from langchain_google_memorystore_redis import (
    DistanceStrategy,
    HNSWConfig,
    RedisVectorStore,
)

# Connect to a Memorystore for Redis instance
redis_client = redis.from_url("redis://127.0.0.1:6379")

# Configure HNSW index with descriptive parameters
index_config = HNSWConfig(
    name="my_vector_index", distance_strategy=DistanceStrategy.COSINE, vector_size=128
)

# Initialize/create the vector store index
RedisVectorStore.init_index(client=redis_client, index_config=index_config)
```

### Pr√©parer les documents

Le texte doit √™tre trait√© et repr√©sent√© num√©riquement avant d'interagir avec un magasin vectoriel. Cela implique :

* Chargement du texte : Le TextLoader obtient les donn√©es textuelles √† partir d'un fichier (par exemple, "state_of_the_union.txt").
* Fractionnement du texte : Le CharacterTextSplitter divise le texte en plus petits morceaux pour les mod√®les d'embedding.

```python
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

loader = TextLoader("./state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
```

### Ajouter des documents au magasin vectoriel

Apr√®s la pr√©paration du texte et la g√©n√©ration des embeddings, les m√©thodes suivantes les ins√®rent dans le magasin vectoriel Redis.

#### M√©thode 1 : M√©thode de classe pour l'insertion directe

Cette approche combine la cr√©ation d'embedding et l'insertion en une seule √©tape √† l'aide de la m√©thode from_documents :

```python
from langchain_community.embeddings.fake import FakeEmbeddings

embeddings = FakeEmbeddings(size=128)
redis_client = redis.from_url("redis://127.0.0.1:6379")
rvs = RedisVectorStore.from_documents(
    docs, embedding=embeddings, client=redis_client, index_name="my_vector_index"
)
```

#### M√©thode 2 : Insertion bas√©e sur l'instance

Cette approche offre de la flexibilit√© lorsque vous travaillez avec une nouvelle instance RedisVectorStore ou une instance existante :

* [Facultatif] Cr√©er une instance RedisVectorStore : Instancier un objet RedisVectorStore pour la personnalisation. Si vous avez d√©j√† une instance, passez √† l'√©tape suivante.
* Ajouter du texte avec des m√©tadonn√©es : Fournir le texte brut et les m√©tadonn√©es √† l'instance. La g√©n√©ration d'embedding et l'insertion dans le magasin vectoriel sont g√©r√©es automatiquement.

```python
rvs = RedisVectorStore(
    client=redis_client, index_name="my_vector_index", embeddings=embeddings
)
ids = rvs.add_texts(
    texts=[d.page_content for d in docs], metadatas=[d.metadata for d in docs]
)
```

### Effectuer une recherche de similarit√© (KNN)

Avec le magasin vectoriel peupl√©, il est possible de rechercher du texte s√©mantiquement similaire √† une requ√™te. Voici comment utiliser KNN (K-Nearest Neighbors) avec les param√®tres par d√©faut :

* Formuler la requ√™te : Une question en langage naturel exprime l'intention de recherche (par exemple, "Qu'a dit le pr√©sident √† propos de Ketanji Brown Jackson").
* R√©cup√©rer les r√©sultats similaires : La m√©thode `similarity_search` trouve les √©l√©ments du magasin vectoriel les plus proches de la requ√™te en termes de sens.

```python
import pprint

query = "What did the president say about Ketanji Brown Jackson"
knn_results = rvs.similarity_search(query=query)
pprint.pprint(knn_results)
```

### Effectuer une recherche de similarit√© bas√©e sur une plage

Les requ√™tes de plage offrent plus de contr√¥le en sp√©cifiant un seuil de similarit√© souhait√© ainsi que le texte de la requ√™te :

* Formuler la requ√™te : Une question en langage naturel d√©finit l'intention de recherche.
* D√©finir le seuil de similarit√© : Le param√®tre `distance_threshold` d√©termine √† quel point une correspondance doit √™tre consid√©r√©e comme pertinente.
* R√©cup√©rer les r√©sultats : La m√©thode `similarity_search_with_score` trouve les √©l√©ments du magasin vectoriel qui se situent dans le seuil de similarit√© sp√©cifi√©.

```python
rq_results = rvs.similarity_search_with_score(query=query, distance_threshold=0.8)
pprint.pprint(rq_results)
```

### Effectuer une recherche de Pertinence Marginale Maximale (MMR)

Les requ√™tes MMR visent √† trouver des r√©sultats qui sont √† la fois pertinents pour la requ√™te et diversifi√©s les uns par rapport aux autres, r√©duisant ainsi la redondance dans les r√©sultats de recherche.

* Formuler la requ√™te : Une question en langage naturel d√©finit l'intention de recherche.
* √âquilibrer la pertinence et la diversit√© : Le param√®tre `lambda_mult` contr√¥le le compromis entre la pertinence stricte et la promotion de la vari√©t√© dans les r√©sultats.
* R√©cup√©rer les r√©sultats MMR : La m√©thode `max_marginal_relevance_search` renvoie les √©l√©ments qui optimisent la combinaison de pertinence et de diversit√© en fonction du param√®tre lambda.

```python
mmr_results = rvs.max_marginal_relevance_search(query=query, lambda_mult=0.90)
pprint.pprint(mmr_results)
```

## Utiliser le magasin vectoriel comme un Retriever

Pour une int√©gration transparente avec d'autres composants LangChain, un magasin vectoriel peut √™tre converti en Retriever. Cela offre plusieurs avantages :

* Compatibilit√© LangChain : De nombreux outils et m√©thodes LangChain sont con√ßus pour interagir directement avec les retrievers.
* Facilit√© d'utilisation : La m√©thode `as_retriever()` convertit le magasin vectoriel dans un format qui simplifie l'interrogation.

```python
retriever = rvs.as_retriever()
results = retriever.invoke(query)
pprint.pprint(results)
```

## Nettoyage

### Supprimer des documents du magasin vectoriel

Il est parfois n√©cessaire de supprimer des documents (et leurs vecteurs associ√©s) du magasin vectoriel. La m√©thode `delete` offre cette fonctionnalit√©.

```python
rvs.delete(ids)
```

### Supprimer un index vectoriel

Il peut y avoir des circonstances o√π la suppression d'un index vectoriel existant est n√©cessaire. Les raisons courantes incluent :

* Modifications de la configuration de l'index : si les param√®tres de l'index doivent √™tre modifi√©s, il est souvent n√©cessaire de supprimer et de recr√©er l'index.
* Gestion du stockage : la suppression des index inutilis√©s peut aider √† lib√©rer de l'espace au sein de l'instance Redis.

Attention : la suppression d'un index vectoriel est une op√©ration irr√©versible. Assurez-vous que les vecteurs stock√©s et la fonctionnalit√© de recherche ne sont plus n√©cessaires avant de proc√©der.

```python
# Delete the vector index
RedisVectorStore.drop_index(client=redis_client, index_name="my_vector_index")
```
