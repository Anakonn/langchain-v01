---
sidebar_position: 2
translated: true
---

# Étape de retour de l'invite

Parfois, la qualité de la recherche et les générations de modèles peuvent être perturbées par les spécificités d'une question. Une façon de gérer cela est de générer d'abord une question plus abstraite, "étape de retour", et d'interroger en fonction de la question d'origine et de la question d'étape de retour.

Par exemple, si nous posons une question du type "Pourquoi mon agent LangGraph astream_events renvoie-t-il {LONG_TRACE} au lieu de {DESIRED_OUTPUT}", nous récupérerons probablement des documents plus pertinents si nous recherchons avec la question plus générique "Comment fonctionne astream_events avec un agent LangGraph" plutôt que si nous recherchons avec la question spécifique de l'utilisateur.

Examinons comment nous pourrions utiliser l'invite d'étape de retour dans le contexte de notre bot de questions-réponses sur les vidéos YouTube de LangChain.

## Configuration

#### Installer les dépendances

```python
# %pip install -qU langchain-core langchain-openai
```

#### Définir les variables d'environnement

Nous utiliserons OpenAI dans cet exemple :

```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

## Génération de la question d'étape de retour

La génération de bonnes questions d'étape de retour se résume à écrire une bonne invite :

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

system = """You are an expert at taking a specific question and extracting a more generic question that gets at \
the underlying principles needed to answer the specific question.

You will be asked about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.

LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.
LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.
LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.
LangSmith is a platform that makes it easy to trace and test LLM applications.

Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question. \

If you don't recognize a word or acronym to not try to rewrite it.

Write concise questions."""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
step_back = prompt | llm | StrOutputParser()
```

```python
question = (
    "I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search. "
    "How do I get just the LLM calls from the event stream"
)
result = step_back.invoke({"question": question})
print(result)
```

```output
What are the specific methods or functions provided by LangGraph for extracting LLM calls from an event stream that includes various types of interactions and data sources?
```

## Renvoyer la question d'étape de retour et la question d'origine

Pour augmenter notre rappel, nous voudrons probablement récupérer des documents en fonction de la question d'étape de retour et de la question d'origine. Nous pouvons facilement renvoyer les deux comme suit :

```python
from langchain_core.runnables import RunnablePassthrough

step_back_and_original = RunnablePassthrough.assign(step_back=step_back)

step_back_and_original.invoke({"question": question})
```

```output
{'question': 'I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search. How do I get just the LLM calls from the event stream',
 'step_back': 'What are the specific methods or functions provided by LangGraph for extracting LLM calls from an event stream generated by an agent built using external tools like Gemini Pro, vectorstores, and DuckDuckGo search?'}
```

## Utilisation de l'appel de fonction pour obtenir une sortie structurée

Si nous composions cette technique avec d'autres techniques d'analyse de requête, nous utiliserions probablement l'appel de fonction pour obtenir des objets de requête structurés. Nous pouvons utiliser l'appel de fonction pour l'invite d'étape de retour comme suit :

```python
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.pydantic_v1 import BaseModel, Field


class StepBackQuery(BaseModel):
    step_back_question: str = Field(
        ...,
        description="Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question.",
    )


llm_with_tools = llm.bind_tools([StepBackQuery])
hyde_chain = prompt | llm_with_tools | PydanticToolsParser(tools=[StepBackQuery])
hyde_chain.invoke({"question": question})
```

```output
[StepBackQuery(step_back_question='What are the steps to filter and extract specific types of calls from an event stream in a Python framework like LangGraph?')]
```
