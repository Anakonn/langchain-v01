---
sidebar_position: 1
translated: true
---

# Gestion de la m√©moire

Une caract√©ristique cl√© des chatbots est leur capacit√© √† utiliser le contenu des tours de conversation pr√©c√©dents comme contexte. Cette gestion de l'√©tat peut prendre plusieurs formes, notamment :

- Simplement ins√©rer les messages pr√©c√©dents dans l'invite du mod√®le de chat.
- Ce qui pr√©c√®de, mais en r√©duisant les anciens messages pour r√©duire la quantit√© d'informations distractives que le mod√®le doit traiter.
- Des modifications plus complexes comme la synth√®se de r√©sum√©s pour les conversations de longue dur√©e.

Nous entrerons plus en d√©tail sur quelques techniques ci-dessous !

## Configuration

Vous devrez installer quelques packages et d√©finir votre cl√© d'API OpenAI en tant que variable d'environnement nomm√©e `OPENAI_API_KEY` :

```python
%pip install --upgrade --quiet langchain langchain-openai

# Set env var OPENAI_API_KEY or load from a .env file:
import dotenv

dotenv.load_dotenv()
```

```output
[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.
```

```output
True
```

Configurons √©galement un mod√®le de chat que nous utiliserons pour les exemples ci-dessous.

```python
from langchain_openai import ChatOpenAI

chat = ChatOpenAI(model="gpt-3.5-turbo-1106")
```

## Passage de messages

La forme la plus simple de m√©moire consiste simplement √† transmettre les messages de l'historique du chat dans une cha√Æne. Voici un exemple :

```python
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

chain = prompt | chat

chain.invoke(
    {
        "messages": [
            HumanMessage(
                content="Translate this sentence from English to French: I love programming."
            ),
            AIMessage(content="J'adore la programmation."),
            HumanMessage(content="What did you just say?"),
        ],
    }
)
```

```output
AIMessage(content='I said "J\'adore la programmation," which means "I love programming" in French.')
```

Nous pouvons voir qu'en transmettant la conversation pr√©c√©dente √† une cha√Æne, elle peut l'utiliser comme contexte pour r√©pondre aux questions. C'est le concept de base de la m√©moire des chatbots - le reste du guide d√©montrera des techniques pratiques pour transmettre ou reformater les messages.

## Historique du chat

Il est tout √† fait acceptable de stocker et de transmettre les messages directement sous forme de tableau, mais nous pouvons √©galement utiliser la classe `message history` int√©gr√©e √† LangChain pour stocker et charger les messages. Les instances de cette classe sont responsables du stockage et du chargement des messages de chat √† partir d'un stockage persistant. LangChain s'int√®gre √† de nombreux fournisseurs - vous pouvez voir une [liste des int√©grations ici](/docs/integrations/memory) - mais pour cette d√©monstration, nous utiliserons une classe de d√©monstration √©ph√©m√®re.

Voici un exemple de l'API :

```python
from langchain.memory import ChatMessageHistory

demo_ephemeral_chat_history = ChatMessageHistory()

demo_ephemeral_chat_history.add_user_message(
    "Translate this sentence from English to French: I love programming."
)

demo_ephemeral_chat_history.add_ai_message("J'adore la programmation.")

demo_ephemeral_chat_history.messages
```

```output
[HumanMessage(content='Translate this sentence from English to French: I love programming.'),
 AIMessage(content="J'adore la programmation.")]
```

Nous pouvons l'utiliser directement pour stocker les tours de conversation pour notre cha√Æne :

```python
demo_ephemeral_chat_history = ChatMessageHistory()

input1 = "Translate this sentence from English to French: I love programming."

demo_ephemeral_chat_history.add_user_message(input1)

response = chain.invoke(
    {
        "messages": demo_ephemeral_chat_history.messages,
    }
)

demo_ephemeral_chat_history.add_ai_message(response)

input2 = "What did I just ask you?"

demo_ephemeral_chat_history.add_user_message(input2)

chain.invoke(
    {
        "messages": demo_ephemeral_chat_history.messages,
    }
)
```

```output
AIMessage(content='You asked me to translate the sentence "I love programming" from English to French.')
```

## Gestion automatique de l'historique

Les exemples pr√©c√©dents transmettent explicitement les messages √† la cha√Æne. Il s'agit d'une approche tout √† fait acceptable, mais elle n√©cessite une gestion externe des nouveaux messages. LangChain inclut √©galement un wrapper pour les cha√Ænes LCEL qui peuvent g√©rer ce processus automatiquement, appel√© `RunnableWithMessageHistory`.

Pour montrer comment cela fonctionne, modifions l√©g√®rement l'invite ci-dessus pour qu'elle prenne une variable `input` finale qui remplisse un mod√®le `HumanMessage` apr√®s l'historique du chat. Cela signifie que nous attendrons un param√®tre `chat_history` qui contient tous les messages AVANT le message actuel, au lieu de tous les messages.

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability.",
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

chain = prompt | chat
```

Nous transmettrons la derni√®re entr√©e √† la conversation ici et laisserons la classe `RunnableWithMessageHistory` envelopper notre cha√Æne et faire le travail d'ajouter cette variable `input` √† l'historique du chat.

Ensuite, d√©clarons notre cha√Æne envelopp√©e :

```python
from langchain_core.runnables.history import RunnableWithMessageHistory

demo_ephemeral_chat_history_for_chain = ChatMessageHistory()

chain_with_message_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: demo_ephemeral_chat_history_for_chain,
    input_messages_key="input",
    history_messages_key="chat_history",
)
```

Cette classe prend quelques param√®tres suppl√©mentaires en plus de la cha√Æne que nous voulons envelopper :

- Une fonction d'usine qui renvoie un historique de messages pour un identifiant de session donn√©. Cela permet √† votre cha√Æne de g√©rer plusieurs utilisateurs √† la fois en chargeant diff√©rents messages pour diff√©rentes conversations.
- Une `input_messages_key` qui sp√©cifie quelle partie de l'entr√©e doit √™tre suivie et stock√©e dans l'historique du chat. Dans cet exemple, nous voulons suivre la cha√Æne pass√©e en tant que `input`.
- Une `history_messages_key` qui sp√©cifie dans quelle partie de l'invite les messages pr√©c√©dents doivent √™tre ins√©r√©s. Notre invite a un `MessagesPlaceholder` nomm√© `chat_history`, nous sp√©cifions donc cette propri√©t√© pour qu'elle corresponde.
- (Pour les cha√Ænes avec plusieurs sorties) une `output_messages_key` qui sp√©cifie quelle sortie stocker dans l'historique. C'est l'inverse de `input_messages_key`.

Nous pouvons invoquer cette nouvelle cha√Æne normalement, avec un champ `configurable` suppl√©mentaire qui sp√©cifie l'`identifiant de session` particulier √† transmettre √† la fonction d'usine. Cela est inutilis√© pour la d√©monstration, mais dans les cha√Ænes du monde r√©el, vous voudrez renvoyer un historique de chat correspondant √† la session pass√©e :

```python
chain_with_message_history.invoke(
    {"input": "Translate this sentence from English to French: I love programming."},
    {"configurable": {"session_id": "unused"}},
)
```

```output
AIMessage(content='The translation of "I love programming" in French is "J\'adore la programmation."')
```

```python
chain_with_message_history.invoke(
    {"input": "What did I just ask you?"}, {"configurable": {"session_id": "unused"}}
)
```

```output
AIMessage(content='You just asked me to translate the sentence "I love programming" from English to French.')
```

## Modification de l'historique du chat

La modification des messages de chat stock√©s peut aider votre chatbot √† g√©rer une vari√©t√© de situations. Voici quelques exemples :

### R√©duction des messages

Les LLM et les mod√®les de chat ont des fen√™tres de contexte limit√©es, et m√™me si vous n'atteignez pas directement les limites, vous voudrez peut-√™tre limiter la quantit√© de distraction que le mod√®le doit g√©rer. Une solution consiste √† ne charger et √† ne stocker que les `n` messages les plus r√©cents. Utilisons un exemple d'historique avec quelques messages pr√©charg√©s :

```python
demo_ephemeral_chat_history = ChatMessageHistory()

demo_ephemeral_chat_history.add_user_message("Hey there! I'm Nemo.")
demo_ephemeral_chat_history.add_ai_message("Hello!")
demo_ephemeral_chat_history.add_user_message("How are you today?")
demo_ephemeral_chat_history.add_ai_message("Fine thanks!")

demo_ephemeral_chat_history.messages
```

```output
[HumanMessage(content="Hey there! I'm Nemo."),
 AIMessage(content='Hello!'),
 HumanMessage(content='How are you today?'),
 AIMessage(content='Fine thanks!')]
```

Utilisons cet historique de messages avec la cha√Æne `RunnableWithMessageHistory` que nous avons d√©clar√©e ci-dessus :

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability.",
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

chain = prompt | chat

chain_with_message_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: demo_ephemeral_chat_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

chain_with_message_history.invoke(
    {"input": "What's my name?"},
    {"configurable": {"session_id": "unused"}},
)
```

```output
AIMessage(content='Your name is Nemo.')
```

Nous pouvons voir que la cha√Æne se souvient du nom pr√©charg√©.

Mais supposons que nous ayons une tr√®s petite fen√™tre de contexte et que nous voulions r√©duire le nombre de messages transmis √† la cha√Æne aux 2 plus r√©cents. Nous pouvons utiliser la m√©thode `clear` pour supprimer les messages et les r√©ajouter √† l'historique. Nous n'avons pas besoin de le faire, mais pla√ßons cette m√©thode au d√©but de notre cha√Æne pour nous assurer qu'elle est toujours appel√©e :

```python
from langchain_core.runnables import RunnablePassthrough


def trim_messages(chain_input):
    stored_messages = demo_ephemeral_chat_history.messages
    if len(stored_messages) <= 2:
        return False

    demo_ephemeral_chat_history.clear()

    for message in stored_messages[-2:]:
        demo_ephemeral_chat_history.add_message(message)

    return True


chain_with_trimming = (
    RunnablePassthrough.assign(messages_trimmed=trim_messages)
    | chain_with_message_history
)
```

Appelons cette nouvelle cha√Æne et v√©rifions les messages par la suite :

```python
chain_with_trimming.invoke(
    {"input": "Where does P. Sherman live?"},
    {"configurable": {"session_id": "unused"}},
)
```

```output
AIMessage(content="P. Sherman's address is 42 Wallaby Way, Sydney.")
```

```python
demo_ephemeral_chat_history.messages
```

```output
[HumanMessage(content="What's my name?"),
 AIMessage(content='Your name is Nemo.'),
 HumanMessage(content='Where does P. Sherman live?'),
 AIMessage(content="P. Sherman's address is 42 Wallaby Way, Sydney.")]
```

Et nous pouvons voir que notre historique a supprim√© les deux messages les plus anciens tout en ajoutant la conversation la plus r√©cente √† la fin. La prochaine fois que la cha√Æne sera appel√©e, `trim_messages` sera appel√©e √† nouveau, et seuls les deux messages les plus r√©cents seront transmis au mod√®le. Dans ce cas, cela signifie que le mod√®le oubliera le nom que nous lui avons donn√© la prochaine fois que nous l'invoquerons :

```python
chain_with_trimming.invoke(
    {"input": "What is my name?"},
    {"configurable": {"session_id": "unused"}},
)
```

```output
AIMessage(content="I'm sorry, I don't have access to your personal information.")
```

```python
demo_ephemeral_chat_history.messages
```

```output
[HumanMessage(content='Where does P. Sherman live?'),
 AIMessage(content="P. Sherman's address is 42 Wallaby Way, Sydney."),
 HumanMessage(content='What is my name?'),
 AIMessage(content="I'm sorry, I don't have access to your personal information.")]
```

### R√©sum√© de la m√©moire

Nous pouvons utiliser ce m√™me mod√®le de mani√®re diff√©rente. Par exemple, nous pourrions utiliser un appel LLM suppl√©mentaire pour g√©n√©rer un r√©sum√© de la conversation avant d'appeler notre cha√Æne. Recr√©ons notre historique de chat et notre cha√Æne de chatbot :

```python
demo_ephemeral_chat_history = ChatMessageHistory()

demo_ephemeral_chat_history.add_user_message("Hey there! I'm Nemo.")
demo_ephemeral_chat_history.add_ai_message("Hello!")
demo_ephemeral_chat_history.add_user_message("How are you today?")
demo_ephemeral_chat_history.add_ai_message("Fine thanks!")

demo_ephemeral_chat_history.messages
```

```output
[HumanMessage(content="Hey there! I'm Nemo."),
 AIMessage(content='Hello!'),
 HumanMessage(content='How are you today?'),
 AIMessage(content='Fine thanks!')]
```

Nous modifierons l√©g√®rement l'invite pour que le LLM sache qu'il recevra un r√©sum√© condens√© au lieu d'un historique de chat :

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.",
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
    ]
)

chain = prompt | chat

chain_with_message_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: demo_ephemeral_chat_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)
```

Et maintenant, cr√©ons une fonction qui distillera les interactions pr√©c√©dentes en un r√©sum√©. Nous pouvons ajouter celle-ci au d√©but de la cha√Æne √©galement :

```python
def summarize_messages(chain_input):
    stored_messages = demo_ephemeral_chat_history.messages
    if len(stored_messages) == 0:
        return False
    summarization_prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            (
                "user",
                "Distill the above chat messages into a single summary message. Include as many specific details as you can.",
            ),
        ]
    )
    summarization_chain = summarization_prompt | chat

    summary_message = summarization_chain.invoke({"chat_history": stored_messages})

    demo_ephemeral_chat_history.clear()

    demo_ephemeral_chat_history.add_message(summary_message)

    return True


chain_with_summarization = (
    RunnablePassthrough.assign(messages_summarized=summarize_messages)
    | chain_with_message_history
)
```

Voyons s'il se souvient du nom que nous lui avons donn√© :

```python
chain_with_summarization.invoke(
    {"input": "What did I say my name was?"},
    {"configurable": {"session_id": "unused"}},
)
```

```output
AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')
```

```python
demo_ephemeral_chat_history.messages
```

```output
[AIMessage(content='The conversation is between Nemo and an AI. Nemo introduces himself and the AI responds with a greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.'),
 HumanMessage(content='What did I say my name was?'),
 AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')]
```

Notez que l'invocation de la cha√Æne √† nouveau g√©n√©rera un autre r√©sum√© g√©n√©r√© √† partir du r√©sum√© initial plus les nouveaux messages, et ainsi de suite. Vous pourriez √©galement concevoir une approche hybride o√π un certain nombre de messages sont conserv√©s dans l'historique du chat tandis que d'autres sont r√©sum√©s.
