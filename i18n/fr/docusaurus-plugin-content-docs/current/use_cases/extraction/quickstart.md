---
sidebar_position: 0
title: D√©marrage rapide
translated: true
---

Dans ce d√©marrage rapide, nous utiliserons des [mod√®les de conversation](/docs/modules/model_io/chat/) capables d'**appeler des fonctions/outils** pour extraire des informations √† partir de texte.

:::important
L'extraction √† l'aide d'**appels de fonctions/outils** ne fonctionne qu'avec les [mod√®les qui prennent en charge **l'appel de fonctions/outils**](/docs/modules/model_io/chat/function_calling).
:::

## Configuration

Nous utiliserons la m√©thode de [sortie structur√©e](/docs/modules/model_io/chat/structured_output) disponible sur les LLM capables d'**appeler des fonctions/outils**.

S√©lectionnez un mod√®le, installez les d√©pendances pour celui-ci et configurez les cl√©s API !

```python
!pip install langchain

# Install a model capable of tool calling
# pip install langchain-openai
# pip install langchain-mistralai
# pip install langchain-fireworks

# Set env vars for the relevant model or load from a .env file:
# import dotenv
# dotenv.load_dotenv()
```

## Le sch√©ma

Tout d'abord, nous devons d√©crire les informations que nous voulons extraire du texte.

Nous utiliserons Pydantic pour d√©finir un sch√©ma d'exemple pour extraire des informations personnelles.

```python
from typing import Optional

from langchain_core.pydantic_v1 import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    # ^ Doc-string for the entity Person.
    # This doc-string is sent to the LLM as the description of the schema Person,
    # and it can help to improve extraction results.

    # Note that:
    # 1. Each field is an `optional` -- this allows the model to decline to extract it!
    # 2. Each field has a `description` -- this description is used by the LLM.
    # Having a good description can help improve extraction results.
    name: Optional[str] = Field(default=None, description="The name of the person")
    hair_color: Optional[str] = Field(
        default=None, description="The color of the peron's hair if known"
    )
    height_in_meters: Optional[str] = Field(
        default=None, description="Height measured in meters"
    )
```

Il existe deux meilleures pratiques lors de la d√©finition du sch√©ma :

1. Documenter les **attributs** et le **sch√©ma** lui-m√™me : ces informations sont envoy√©es au LLM et sont utilis√©es pour am√©liorer la qualit√© de l'extraction d'informations.
2. Ne pas forcer le LLM √† inventer des informations ! Ci-dessus, nous avons utilis√© `Optional` pour les attributs, permettant au LLM de renvoyer `None` s'il ne conna√Æt pas la r√©ponse.

:::important
Pour de meilleures performances, documentez bien le sch√©ma et assurez-vous que le mod√®le n'est pas forc√© de renvoyer des r√©sultats s'il n'y a pas d'informations √† extraire dans le texte.
:::

## L'extracteur

Cr√©ons un extracteur d'informations √† l'aide du sch√©ma que nous avons d√©fini ci-dessus.

```python
from typing import Optional

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI

# Define a custom prompt to provide instructions and any additional context.
# 1) You can add examples into the prompt template to improve extraction quality
# 2) Introduce additional parameters to take context into account (e.g., include metadata
#    about the document from which the text was extracted.)
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert extraction algorithm. "
            "Only extract relevant information from the text. "
            "If you do not know the value of an attribute asked to extract, "
            "return null for the attribute's value.",
        ),
        # Please see the how-to about improving performance with
        # reference examples.
        # MessagesPlaceholder('examples'),
        ("human", "{text}"),
    ]
)
```

Nous devons utiliser un mod√®le qui prend en charge l'appel de fonctions/outils.

Veuillez consulter [la sortie structur√©e](/docs/modules/model_io/chat/structured_output) pour obtenir la liste de certains mod√®les qui peuvent √™tre utilis√©s avec cette API.

```python
from langchain_mistralai import ChatMistralAI

llm = ChatMistralAI(model="mistral-large-latest", temperature=0)

runnable = prompt | llm.with_structured_output(schema=Person)
```

Essayons-le

```python
text = "Alan Smith is 6 feet tall and has blond hair."
runnable.invoke({"text": text})
```

```output
Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288')
```

:::important

L'extraction est g√©n√©rative ü§Ø

Les LLM sont des mod√®les g√©n√©ratifs, donc ils peuvent faire des choses assez cool comme extraire correctement la taille de la personne en m√®tres
m√™me si elle √©tait fournie en pieds !
:::

## Entit√©s multiples

Dans **la plupart des cas**, vous devriez extraire une liste d'entit√©s plut√¥t qu'une seule entit√©.

Cela peut √™tre facilement r√©alis√© en utilisant Pydantic en imbriquant des mod√®les les uns dans les autres.

```python
from typing import List, Optional

from langchain_core.pydantic_v1 import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    # ^ Doc-string for the entity Person.
    # This doc-string is sent to the LLM as the description of the schema Person,
    # and it can help to improve extraction results.

    # Note that:
    # 1. Each field is an `optional` -- this allows the model to decline to extract it!
    # 2. Each field has a `description` -- this description is used by the LLM.
    # Having a good description can help improve extraction results.
    name: Optional[str] = Field(default=None, description="The name of the person")
    hair_color: Optional[str] = Field(
        default=None, description="The color of the peron's hair if known"
    )
    height_in_meters: Optional[str] = Field(
        default=None, description="Height measured in meters"
    )


class Data(BaseModel):
    """Extracted data about people."""

    # Creates a model so that we can extract multiple entities.
    people: List[Person]
```

:::important
L'extraction peut ne pas √™tre parfaite ici. Continuez √† voir comment utiliser les **exemples de r√©f√©rence** pour am√©liorer la qualit√© de l'extraction et consultez la section **directives** !
:::

```python
runnable = prompt | llm.with_structured_output(schema=Data)
text = "My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me."
runnable.invoke({"text": text})
```

```output
Data(people=[Person(name='Jeff', hair_color=None, height_in_meters=None), Person(name='Anna', hair_color=None, height_in_meters=None)])
```

:::tip
Lorsque le sch√©ma permet l'extraction de **plusieurs entit√©s**, il permet √©galement au mod√®le d'extraire **aucune entit√©** si aucune information pertinente
n'est pr√©sente dans le texte en fournissant une liste vide.

C'est g√©n√©ralement une **bonne** chose ! Cela permet de sp√©cifier des attributs **obligatoires** sur une entit√© sans n√©cessairement forcer le mod√®le √† d√©tecter cette entit√©.
:::

## Prochaines √©tapes

Maintenant que vous comprenez les bases de l'extraction avec LangChain, vous √™tes pr√™t √† passer au reste du guide pratique :

- [Ajouter des exemples](/docs/use_cases/extraction/how_to/examples) : Apprenez √† utiliser des **exemples de r√©f√©rence** pour am√©liorer les performances.
- [G√©rer les longs textes](/docs/use_cases/extraction/how_to/handle_long_text) : Que devez-vous faire si le texte ne rentre pas dans la fen√™tre de contexte du LLM ?
- [G√©rer les fichiers](/docs/use_cases/extraction/how_to/handle_files) : Exemples d'utilisation des chargeurs et analyseurs de documents LangChain pour extraire √† partir de fichiers comme les PDF.
- [Utiliser une approche d'analyse](/docs/use_cases/extraction/how_to/parse) : Utilisez une approche bas√©e sur les invites pour extraire avec des mod√®les qui ne prennent pas en charge **l'appel d'outils/de fonctions**.
- [Directives](/docs/use_cases/extraction/guidelines) : Directives pour obtenir de bonnes performances sur les t√¢ches d'extraction.
