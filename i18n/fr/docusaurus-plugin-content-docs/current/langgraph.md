---
fixed: true
translated: true
---

# ü¶úüï∏Ô∏èLangGraph

[![T√©l√©chargements](https://static.pepy.tech/badge/langgraph/month)](https://pepy.tech/project/langgraph)
[![Probl√®mes ouverts](https://img.shields.io/github/issues-raw/langchain-ai/langgraph)](https://github.com/langchain-ai/langgraph/issues)
[![](https://dcbadge.vercel.app/api/server/6adMQxSpJS?compact=true&style=flat)](https://discord.com/channels/1038097195422978059/1170024642245832774)
[![Documentation](https://img.shields.io/badge/docs-latest-blue)](https://langchain-ai.github.io/langgraph/)

‚ö° Cr√©ation d'agents linguistiques sous forme de graphes ‚ö°

## Aper√ßu

[LangGraph](https://langchain-ai.github.io/langgraph/) est une biblioth√®que permettant de cr√©er des applications multiacteurs avec √©tats √† l'aide de LLM.
Inspir√© par [Pregel](https://research.google/pubs/pub37252/) et [Apache Beam](https://beam.apache.org/), LangGraph vous permet de coordonner et de sauvegarder plusieurs cha√Ænes (ou acteurs) √† travers des √©tapes de calcul cycliques √† l'aide de fonctions Python r√©guli√®res (ou [JS](https://github.com/langchain-ai/langgraphjs))). L'interface publique s'inspire de [NetworkX](https://networkx.org/documentation/latest/).

L'utilisation principale est d'ajouter des **cycles** et de la **persistance** √† votre application LLM. Si vous n'avez besoin que de Directed Acyclic Graphs (DAGs) rapides, vous pouvez d√©j√† y arriver en utilisant le [LangChain Expression Language](https://python.langchain.com/docs/expression_language/).

Les cycles sont importants pour les comportements agentiques, o√π vous appelez un LLM en boucle, lui demandant quelle action prendre ensuite.

## Installation

```shell
pip install -U langgraph
```

## D√©marrage rapide

L'un des concepts centraux de LangGraph est l'√©tat. Chaque ex√©cution de graphe cr√©e un √©tat qui est transmis entre les n≈ìuds du graphe lors de leur ex√©cution, et chaque n≈ìud met √† jour cet √©tat interne avec sa valeur de retour apr√®s son ex√©cution. La fa√ßon dont le graphe met √† jour son √©tat interne est d√©finie soit par le type de graphe choisi, soit par une fonction personnalis√©e.

L'√©tat dans LangGraph peut √™tre assez g√©n√©ral, mais pour simplifier les choses au d√©but, nous allons montrer un exemple o√π l'√©tat du graphe se limite √† une liste de messages de discussion √† l'aide de la classe `MessageGraph` int√©gr√©e. Cela est pratique lors de l'utilisation de LangGraph avec les mod√®les de discussion LangChain, car nous pouvons directement renvoyer la sortie du mod√®le de discussion.

Commen√ßons par installer le package d'int√©gration OpenAI de LangChain :

```python
pip install langchain_openai
```

Nous devons √©galement exporter quelques variables d'environnement :

```shell
export OPENAI_API_KEY=sk-...
```

Et maintenant, nous sommes pr√™ts ! Le graphe ci-dessous contient un seul n≈ìud appel√© `"oracle"` qui ex√©cute un mod√®le de discussion, puis renvoie le r√©sultat :

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}, {"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}]-->
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import END, MessageGraph

model = ChatOpenAI(temperature=0)

graph = MessageGraph()

graph.add_node("oracle", model)
graph.add_edge("oracle", END)

graph.set_entry_point("oracle")

runnable = graph.compile()
```

Ex√©cutons-le !

```python
runnable.invoke(HumanMessage("What is 1 + 1?"))
```

```python
[HumanMessage(content='What is 1 + 1?'), AIMessage(content='1 + 1 equals 2.')]
```

Que venons-nous de faire ici ? D√©composons-le √©tape par √©tape :

1. Tout d'abord, nous initialisons notre mod√®le et un `MessageGraph`.
2. Ensuite, nous ajoutons un seul n≈ìud au graphe, appel√© `"oracle"`, qui appelle simplement le mod√®le avec l'entr√©e donn√©e.
3. Nous ajoutons une ar√™te de ce n≈ìud `"oracle"` √† la cha√Æne sp√©ciale `END` (`"__end__"`). Cela signifie que l'ex√©cution se terminera apr√®s le n≈ìud actuel.
4. Nous d√©finissons `"oracle"` comme point d'entr√©e du graphe.
5. Nous compilons le graphe, le traduisant en op√©rations [pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) de bas niveau, assurant ainsi qu'il peut √™tre ex√©cut√©.

Ensuite, lors de l'ex√©cution du graphe :

1. LangGraph ajoute le message d'entr√©e √† l'√©tat interne, puis transmet l'√©tat au n≈ìud d'entr√©e, `"oracle"`.
2. Le n≈ìud `"oracle"` s'ex√©cute, invoquant le mod√®le de discussion.
3. Le mod√®le de discussion renvoie un `AIMessage`. LangGraph l'ajoute √† l'√©tat.
4. L'ex√©cution progresse jusqu'√† la valeur sp√©ciale `END` et renvoie l'√©tat final.

Et en r√©sultat, nous obtenons une liste de deux messages de discussion en sortie.

### Interaction avec LCEL

Pour ceux qui connaissent d√©j√† LangChain - `add_node` accepte en fait n'importe quelle fonction ou [runnable](https://python.langchain.com/docs/expression_language/interface/) en entr√©e. Dans l'exemple ci-dessus, le mod√®le est utilis√© "tel quel", mais nous aurions √©galement pu passer une fonction :

```python
def call_oracle(messages: list):
    return model.invoke(messages)

graph.add_node("oracle", call_oracle)
```

Assurez-vous simplement d'√™tre attentif au fait que l'entr√©e du [runnable](https://python.langchain.com/docs/expression_language/interface/) est l'**√©tat actuel complet**. Donc cela √©chouera :

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}, {"imported": "MessagesPlaceholder", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}]-->
# This will not work with MessageGraph!
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant named {name} who always speaks in pirate dialect"),
    MessagesPlaceholder(variable_name="messages"),
])

chain = prompt | model

# State is a list of messages, but our chain expects a dict input:
#
# { "name": some_string, "messages": [] }
#
# Therefore, the graph will throw an exception when it executes here.
graph.add_node("oracle", chain)
```

## Ar√™tes conditionnelles

Passons maintenant √† quelque chose de l√©g√®rement moins trivial. Les LLM ont du mal avec les math√©matiques, alors permettons √† l'LLM d'appeler conditionnellement un n≈ìud `"multiply"` en utilisant l'[appel d'outils](https://python.langchain.com/docs/modules/model_io/chat/function_calling/).

Nous allons recr√©er notre graphe avec un `"multiply"` suppl√©mentaire qui prendra le r√©sultat du message le plus r√©cent, s'il s'agit d'un appel d'outil, et calculera le r√©sultat.
Nous [lierons](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind_tools) √©galement le sch√©ma de la calculatrice au mod√®le OpenAI en tant qu'outil pour permettre au mod√®le d'utiliser l'outil n√©cessaire pour r√©pondre √† l'√©tat actuel :

```python
<!--IMPORTS:[{"imported": "tool", "source": "langchain_core.tools", "docs": "https://api.python.langchain.com/en/latest/tools/langchain_core.tools.tool.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}]-->
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

@tool
def multiply(first_number: int, second_number: int):
    """Multiplies two numbers together."""
    return first_number * second_number

model = ChatOpenAI(temperature=0)
model_with_tools = model.bind_tools([multiply])

builder = MessageGraph()

builder.add_node("oracle", model_with_tools)

tool_node = ToolNode([multiply])
builder.add_node("multiply", tool_node)

builder.add_edge("multiply", END)

builder.set_entry_point("oracle")
```

R√©fl√©chissons maintenant - que voulons-nous qu'il se passe ?

- Si le n≈ìud `"oracle"` renvoie un message attendant un appel d'outil, nous voulons ex√©cuter le n≈ìud `"multiply"`
- Sinon, nous pouvons simplement terminer l'ex√©cution

Nous pouvons y parvenir √† l'aide d'**ar√™tes conditionnelles**, qui appellent une fonction sur l'√©tat actuel et acheminent l'ex√©cution vers un n≈ìud en fonction de la sortie de la fonction.

Voici √† quoi cela ressemble :

```python
from typing import Literal

def router(state: List[BaseMessage]) -> Literal["multiply", "__end__"]:
    tool_calls = state[-1].additional_kwargs.get("tool_calls", [])
    if len(tool_calls):
        return "multiply"
    else:
        return "__end__"

builder.add_conditional_edges("oracle", router)
```

Si la sortie du mod√®le contient un appel d'outil, nous passons au n≈ìud `"multiply"`. Sinon, nous terminons l'ex√©cution.

Parfait ! Il ne reste plus qu'√† compiler le graphe et √† l'essayer. Les questions li√©es aux math√©matiques sont achemin√©es vers l'outil de calculatrice :

```python
runnable = builder.compile()

runnable.invoke(HumanMessage("What is 123 * 456?"))
```

```output

[HumanMessage(content='What is 123 * 456?'),
 AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_OPbdlm8Ih1mNOObGf3tMcNgb', 'function': {'arguments': '{"first_number":123,"second_number":456}', 'name': 'multiply'}, 'type': 'function'}]}),
 ToolMessage(content='56088', tool_call_id='call_OPbdlm8Ih1mNOObGf3tMcNgb')]
```

Tandis que les r√©ponses conversationnelles sont directement renvoy√©es :

```python
runnable.invoke(HumanMessage("What is your name?"))
```

```output
[HumanMessage(content='What is your name?'),
 AIMessage(content='My name is Assistant. How can I assist you today?')]
```

## Cycles

Maintenant, examinons un exemple cyclique plus g√©n√©ral. Nous allons recr√©er la classe `AgentExecutor` de LangChain. L'agent lui-m√™me utilisera des mod√®les de chat et l'appel d'outils.
Cet agent repr√©sentera tout son √©tat sous forme de liste de messages.

Nous devrons installer quelques packages communautaires LangChain, ainsi que [Tavily](https://app.tavily.com/sign-in) pour l'utiliser comme outil d'exemple.

```shell
pip install -U langgraph langchain_openai tavily-python
```

Nous devons √©galement exporter quelques variables d'environnement suppl√©mentaires pour l'acc√®s √† l'API OpenAI et Tavily.

```shell
export OPENAI_API_KEY=sk-...
export TAVILY_API_KEY=tvly-...
```

Facultativement, nous pouvons configurer [LangSmith](https://docs.smith.langchain.com/) pour une observabilit√© de classe mondiale.

```shell
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY=ls__...
```

### Configurer les outils

Comme ci-dessus, nous allons d'abord d√©finir les outils que nous voulons utiliser.
Pour cet exemple simple, nous utiliserons un outil de recherche web.
Cependant, il est tr√®s facile de cr√©er vos propres outils - consultez la documentation [ici](https://python.langchain.com/docs/modules/agents/tools/custom_tools) pour savoir comment proc√©der.

```python
<!--IMPORTS:[{"imported": "TavilySearchResults", "source": "langchain_community.tools.tavily_search", "docs": "https://api.python.langchain.com/en/latest/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}]-->
from langchain_community.tools.tavily_search import TavilySearchResults

tools = [TavilySearchResults(max_results=1)]
```

Nous pouvons maintenant envelopper ces outils dans un simple [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode) LangGraph.
Cette classe re√ßoit la liste des messages (contenant [tool_calls](/html-tag/20]), appelle le(s) outil(s) que le LLM a demand√© d'ex√©cuter et renvoie la sortie sous forme de nouveau [ToolMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.tool.ToolMessage.html#langchain_core.messages.tool.ToolMessage)(s).

```python
from langgraph.prebuilt import ToolNode

tool_node = ToolNode(tools)
```

### Configurer le mod√®le

Maintenant, nous devons charger le mod√®le de chat √† utiliser.

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}]-->
from langchain_openai import ChatOpenAI

# We will set streaming=True so that we can stream tokens
# See the streaming section for more information on this.
model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, streaming=True)
```

Apr√®s avoir fait cela, nous devrions nous assurer que le mod√®le sait qu'il a ces outils disponibles √† appeler.
Nous pouvons le faire en convertissant les outils LangChain dans le format pour l'appel d'outils OpenAI √† l'aide de la m√©thode [bind_tools()](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind_tools).

```python
model = model.bind_tools(tools)
```

### D√©finir l'√©tat de l'agent

Cette fois, nous utiliserons le `StateGraph` plus g√©n√©ral.
Ce graphe est param√©tr√© par un objet d'√©tat qu'il fait circuler dans chaque n≈ìud.
Rappelez-vous que chaque n≈ìud renvoie ensuite des op√©rations pour mettre √† jour cet √©tat.
Ces op√©rations peuvent soit D√âFINIR des attributs sp√©cifiques sur l'√©tat (par exemple, √©craser les valeurs existantes), soit AJOUTER √† l'attribut existant.
S'il faut d√©finir ou ajouter est indiqu√© en annotant l'objet d'√©tat avec lequel vous construisez le graphe.

Pour cet exemple, l'√©tat que nous suivrons sera simplement une liste de messages.
Nous voulons que chaque n≈ìud ajoute simplement des messages √† cette liste.
Par cons√©quent, nous utiliserons un `TypedDict` avec une seule cl√© (`messages`) et l'annoter afin que nous ajoutions toujours √† la cl√© `messages` lors de la mise √† jour √† l'aide du second param√®tre (`operator.add`).
(Remarque : l'√©tat peut √™tre de n'importe quel [type](https://docs.python.org/3/library/stdtypes.html#type-objects), y compris les [pydantic BaseModel](https://docs.pydantic.dev/latest/api/base_model/)).

```python
from typing import TypedDict, Annotated

def add_messages(left: list, right: list):
    """Add-don't-overwrite."""
    return left + right

class AgentState(TypedDict):
    # The `add_messages` function within the annotation defines
    # *how* updates should be merged into the state.
    messages: Annotated[list, add_messages]
```

Vous pouvez consid√©rer le `MessageGraph` utilis√© dans l'exemple initial comme une version pr√©configur√©e de ce graphe, o√π l'√©tat est directement un tableau de messages,
et l'√©tape de mise √† jour ajoute toujours les valeurs renvoy√©es d'un n≈ìud √† l'√©tat interne.

### D√©finir les n≈ìuds

Nous devons maintenant d√©finir quelques n≈ìuds diff√©rents dans notre graphe.
Dans `langgraph`, un n≈ìud peut √™tre soit une fonction Python r√©guli√®re, soit un [runnable](https://python.langchain.com/docs/expression_language/).

Il y a deux n≈ìuds principaux dont nous avons besoin pour cela :

1. L'agent : responsable de d√©cider quelles actions (le cas √©ch√©ant) √† entreprendre.
2. Une fonction pour invoquer les outils : si l'agent d√©cide de prendre une action, ce n≈ìud ex√©cutera alors cette action. Nous l'avons d√©j√† d√©fini ci-dessus.

Nous devrons √©galement d√©finir quelques ar√™tes.
Certaines de ces ar√™tes peuvent √™tre conditionnelles.
La raison pour laquelle elles sont conditionnelles est que la destination d√©pend du contenu de l'`√âtat` du graphe.

Le chemin emprunt√© n'est pas connu avant que ce n≈ìud ne soit ex√©cut√© (le LLM d√©cide). Pour notre cas d'utilisation, nous aurons besoin d'un de chaque type d'ar√™te :

1. Ar√™te conditionnelle : apr√®s avoir appel√© l'agent, nous devrions soit :

   a. Ex√©cuter les outils si l'agent a demand√© de prendre une action, OU

   b. Terminer (r√©pondre √† l'utilisateur) si l'agent n'a pas demand√© d'ex√©cuter des outils

2. Ar√™te normale : apr√®s avoir invoqu√© les outils, le graphe devrait toujours revenir √† l'agent pour d√©cider de ce qu'il faut faire ensuite

D√©finissons les n≈ìuds, ainsi qu'une fonction pour d√©finir l'ar√™te conditionnelle √† emprunter.

```python
from typing import Literal

# Define the function that determines whether to continue or not
def should_continue(state: AgentState) -> Literal["action", "__end__"]:
    messages = state['messages']
    last_message = messages[-1]
    # If the LLM makes a tool call, then we route to the "action" node
    if last_message.tool_calls:
        return "action"
    # Otherwise, we stop (reply to the user)
    return "__end__"


# Define the function that calls the model
def call_model(state: AgentState):
    messages = state['messages']
    response = model.invoke(messages)
    # We return a list, because this will get added to the existing list
    return {"messages": [response]}
```

### D√©finir le graphe

Nous pouvons maintenant tout rassembler et d√©finir le graphe !

```python
from langgraph.graph import StateGraph, END
# Define a new graph
workflow = StateGraph(AgentState)

# Define the two nodes we will cycle between
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)

# Set the entrypoint as `agent`
# This means that this node is the first one called
workflow.set_entry_point("agent")

# We now add a conditional edge
workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    should_continue,
)

# We now add a normal edge from `tools` to `agent`.
# This means that after `tools` is called, `agent` node is called next.
workflow.add_edge('action', 'agent')

# Finally, we compile it!
# This compiles it into a LangChain Runnable,
# meaning you can use it as you would any other runnable
app = workflow.compile()
```

### L'utiliser !

Nous pouvons maintenant l'utiliser !
Cela expose maintenant l'[interface](https://python.langchain.com/docs/expression_language/) identique √† tous les autres runnables LangChain.
Ce [runnable](https://python.langchain.com/docs/expression_language/interface/) accepte une liste de messages.

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph"}]-->
from langchain_core.messages import HumanMessage

inputs = {"messages": [HumanMessage(content="what is the weather in sf")]}
app.invoke(inputs)
```

Cela peut prendre un peu de temps - il effectue quelques appels en coulisses.
Afin de commencer √† voir quelques r√©sultats interm√©diaires au fur et √† mesure, nous pouvons utiliser le streaming - voir ci-dessous pour plus d'informations √† ce sujet.

## Streaming

LangGraph prend en charge plusieurs types de streaming diff√©rents.

### Streaming de la sortie des n≈ìuds

L'un des avantages de l'utilisation de LangGraph est qu'il est facile de diffuser la sortie au fur et √† mesure qu'elle est produite par chaque n≈ìud.

```python
inputs = {"messages": [HumanMessage(content="what is the weather in sf")]}
for output in app.stream(inputs, stream_mode="updates"):
    # stream() yields dictionaries with output keyed by node name
    for key, value in output.items():
        print(f"Output from node '{key}':")
        print("---")
        print(value)
    print("\n---\n")
```

```output
Output from node 'agent':
---
{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "query": "weather in San Francisco"\n}', 'name': 'tavily_search_results_json'}})]}

---

Output from node 'action':
---
{'messages': [FunctionMessage(content="[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States  Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco  San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco  Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.'}]", name='tavily_search_results_json')]}

---

Output from node 'agent':
---
{'messages': [AIMessage(content="I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.")]}

---

Output from node '__end__':
---
{'messages': [HumanMessage(content='what is the weather in sf'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "query": "weather in San Francisco"\n}', 'name': 'tavily_search_results_json'}}), FunctionMessage(content="[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States  Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco  San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco  Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.'}]", name='tavily_search_results_json'), AIMessage(content="I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.")]}

---
```

### Streaming des jetons LLM

Vous pouvez √©galement acc√©der aux jetons LLM au fur et √† mesure qu'ils sont produits par chaque n≈ìud.
Dans ce cas, seul le n≈ìud "agent" produit des jetons LLM.
Pour que cela fonctionne correctement, vous devez utiliser un LLM qui prend en charge le streaming et l'avoir d√©fini lors de la construction du LLM (par exemple, `ChatOpenAI(model="gpt-3.5-turbo-1106", streaming=True)`)

```python
inputs = {"messages": [HumanMessage(content="what is the weather in sf")]}
async for output in app.astream_log(inputs, include_types=["llm"]):
    # astream_log() yields the requested logs (here LLMs) in JSONPatch format
    for op in output.ops:
        if op["path"] == "/streamed_output/-":
            # this is the output from .stream()
            ...
        elif op["path"].startswith("/logs/") and op["path"].endswith(
            "/streamed_output/-"
        ):
            # because we chose to only include LLMs, these are LLM tokens
            print(op["value"])
```

```output
content='' additional_kwargs={'function_call': {'arguments': '', 'name': 'tavily_search_results_json'}}
content='' additional_kwargs={'function_call': {'arguments': '{\n', 'name': ''}}}
content='' additional_kwargs={'function_call': {'arguments': ' ', 'name': ''}}
content='' additional_kwargs={'function_call': {'arguments': ' "', 'name': ''}}
content='' additional_kwargs={'function_call': {'arguments': 'query', 'name': ''}}
...
```

## Quand utiliser

Quand devriez-vous utiliser ceci par rapport √† [LangChain Expression Language](https://python.langchain.com/docs/expression_language/) ?

Si vous avez besoin de cycles.

LangChain Expression Language vous permet de d√©finir facilement des cha√Ænes (DAG) mais n'a pas de bon m√©canisme pour ajouter des cycles.
`langgraph` ajoute cette syntaxe.

## Documentation

Nous esp√©rons que cela vous a donn√© un aper√ßu de ce que vous pouvez construire ! Consultez le reste de la documentation pour en savoir plus.

### Tutoriels

Apprenez √† construire avec LangGraph gr√¢ce √† des exemples guid√©s dans les [Tutoriels LangGraph](https://langchain-ai.github.io/langgraph/tutorials/).

Nous vous recommandons de commencer par l'[Introduction √† LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/) avant d'essayer les guides plus avanc√©s.

### Guides pratiques

Les [guides pratiques LangGraph](https://langchain-ai.github.io/langgraph/how-tos/) montrent comment accomplir des choses sp√©cifiques dans LangGraph, du streaming, √† l'ajout de m√©moire et de persistance, en passant par les mod√®les de conception courants (branchement, sous-graphes, etc.), c'est l√† que vous devez aller si vous voulez copier et ex√©cuter un extrait de code sp√©cifique.

### R√©f√©rence

L'API LangGraph a quelques classes et m√©thodes importantes qui sont toutes couvertes dans les [Documents de r√©f√©rence](https://langchain-ai.github.io/langgraph/reference/graphs/). Consultez-les pour voir les arguments de fonction sp√©cifiques et des exemples simples sur la fa√ßon d'utiliser l'API du graphe et du checkpointing, ou pour voir certains des composants pr√©d√©finis de plus haut niveau.
