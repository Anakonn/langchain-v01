---
fixed: true
translated: true
---

# ü¶úÔ∏èüèì LangServe

[![Release Notes](https://img.shields.io/github/release/langchain-ai/langserve)](https://github.com/langchain-ai/langserve/releases)
[![Downloads](https://static.pepy.tech/badge/langserve/month)](https://pepy.tech/project/langserve)
[![Open Issues](https://img.shields.io/github/issues-raw/langchain-ai/langserve)](https://github.com/langchain-ai/langserve/issues)
[![](https://dcbadge.vercel.app/api/server/6adMQxSpJS?compact=true&style=flat)](https://discord.com/channels/1038097195422978059/1170024642245832774)

üö© Nous allons lancer une version h√©berg√©e de LangServe pour des d√©ploiements en un clic des
applications LangChain. [Inscrivez-vous ici](https://airtable.com/apppQ9p5XuujRl3wJ/shrABpHWdxry8Bacm)
pour vous inscrire sur la liste d'attente.

## Vue d'ensemble

[LangServe](https://github.com/langchain-ai/langserve) aide les d√©veloppeurs
√† d√©ployer `LangChain` [runnables et cha√Ænes](https://python.langchain.com/docs/expression_language/)
en tant qu'API REST.

Cette biblioth√®que est int√©gr√©e avec [FastAPI](https://fastapi.tiangolo.com/) et
utilise [pydantic](https://docs.pydantic.dev/latest/) pour la validation des donn√©es.

De plus, elle fournit un client qui peut √™tre utilis√© pour appeler des runnables d√©ploy√©s sur un
serveur.
Un client JavaScript est disponible
dans [LangChain.js](https://js.langchain.com/docs/ecosystem/langserve).

## Fonctionnalit√©s

- Sch√©mas d'entr√©e et de sortie automatiquement inf√©r√©s de votre objet LangChain, et
  appliqu√©s √† chaque appel d'API, avec des messages d'erreur riches
- Page de documentation de l'API avec JSONSchema et Swagger (ins√©rer un lien d'exemple)
- Endpoints efficaces `/invoke`, `/batch` et `/stream` avec support pour de nombreuses
  requ√™tes concurrentes sur un seul serveur
- Endpoint `/stream_log` pour streamer toutes (ou certaines) √©tapes interm√©diaires de votre
  cha√Æne/agent
- **nouveau** depuis la version 0.0.40, supporte `/stream_events` pour faciliter le streaming sans besoin d'analyser la sortie de `/stream_log`.
- Page de playground √† `/playground/` avec sortie de streaming et √©tapes interm√©diaires
- Tra√ßage int√©gr√© (optionnel) vers [LangSmith](https://www.langchain.com/langsmith), il suffit
  d'ajouter votre cl√© API (voir [Instructions](https://docs.smith.langchain.com/)))
- Tout est construit avec des biblioth√®ques Python open-source √©prouv√©es comme FastAPI, Pydantic,
  uvloop et asyncio.
- Utilisez le SDK client pour appeler un serveur LangServe comme s'il s'agissait d'un Runnable fonctionnant
  localement (ou appeler directement l'API HTTP)
- [LangServe Hub](https://github.com/langchain-ai/langchain/blob/master/templates/README.md)

## Limitations

- Les callbacks clients ne sont pas encore support√©s pour les √©v√©nements qui proviennent du serveur
- Les documents OpenAPI ne seront pas g√©n√©r√©s lors de l'utilisation de Pydantic V2. Fast API ne supporte pas
  [le m√©lange des namespaces pydantic v1 et v2](https://github.com/tiangolo/fastapi/issues/10360).
  Voir la section ci-dessous pour plus de d√©tails.

## LangServe h√©berg√©

Nous allons lancer une version h√©berg√©e de LangServe pour des d√©ploiements en un clic des
applications LangChain. [Inscrivez-vous ici](https://airtable.com/apppQ9p5XuujRl3wJ/shrABpHWdxry8Bacm)
pour vous inscrire sur la liste d'attente.

## S√©curit√©

- Vuln√©rabilit√© dans les versions 0.0.13 - 0.0.15 -- le endpoint playground permet d'acc√©der
  √† des fichiers arbitraires sur
  le serveur. [R√©solu dans la version 0.0.16](https://github.com/langchain-ai/langserve/pull/98).

## Installation

Pour les deux, client et serveur :

```bash
pip install "langserve[all]"
```

ou `pip install "langserve[client]"` pour le code client,
et `pip install "langserve[server]"` pour le code serveur.

## LangChain CLI üõ†Ô∏è

Utilisez le CLI `LangChain` pour d√©marrer rapidement un projet `LangServe`.

Pour utiliser le CLI langchain assurez-vous d'avoir une version r√©cente de `langchain-cli`
install√©e. Vous pouvez l'installer avec `pip install -U langchain-cli`.

## Configuration

**Remarque**: Nous utilisons `poetry` pour la gestion des d√©pendances. Veuillez suivre la [documentation](https://python-poetry.org/docs/) de poetry pour en savoir plus.

### 1. Cr√©ez une nouvelle application en utilisant la commande CLI langchain

```sh
langchain app new my-app
```

### 2. D√©finissez le runnable dans add_routes. Allez dans server.py et √©ditez

```sh
add_routes(app. NotImplemented)
```

### 3. Utilisez `poetry` pour ajouter des packages tiers (par exemple, langchain-openai, langchain-anthropic, langchain-mistral etc).

```sh
poetry add [package-name] // e.g `poetry add langchain-openai`
```

### 4. Configurez les variables d'environnement pertinentes. Par exemple,

```sh
export OPENAI_API_KEY="sk-..."
```

### 5. Servez votre application

```sh
poetry run langchain serve --port=8100
```

## Exemples

D√©marrez rapidement votre instance LangServe avec
[LangChain Templates](https://github.com/langchain-ai/langchain/blob/master/templates/README.md).

Pour plus d'exemples, voir les templates
[index](https://github.com/langchain-ai/langchain/blob/master/templates/docs/INDEX.md)
ou le [r√©pertoire d'exemples](https://github.com/langchain-ai/langserve/tree/main/examples).

| Description                                                                                                                                                                                                                                                        | Liens                                                                                                                                                                                                                               |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **LLMs** Exemple minimal qui r√©serve les mod√®les de chat OpenAI et Anthropic. Utilise async, supporte le batching et le streaming.                                                                                                                                              | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/llm/server.py), [client](https://github.com/langchain-ai/langserve/blob/main/examples/llm/client.ipynb)                                                       |
| **Retriever** Serveur simple qui expose un retriever en tant que runnable.                                                                                                                                                                                                | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/retrieval/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/retrieval/client.ipynb)                                           |
| **Conversational Retriever** Un [Conversational Retriever](https://python.langchain.com/docs/expression_language/cookbook/retrieval#conversational-retrieval-chain) expos√© via LangServe                                                                           | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/conversational_retrieval_chain/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/conversational_retrieval_chain/client.ipynb) |
| **Agent** sans **historique de conversation** bas√© sur [OpenAI tools](https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent)                                                                                                            | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/agent/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/agent/client.ipynb)                                                   |
| **Agent** avec **historique de conversation** bas√© sur [OpenAI tools](https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent)                                                                                                               | [serveur](https://github.com/langchain-ai/langserve/blob/main/examples/agent_with_history/server.py), [client](https://github.com/langchain-ai/langserve/blob/main/examples/agent_with_history/client.ipynb)                         |
| [RunnableWithMessageHistory](https://python.langchain.com/docs/expression_language/how_to/message_history) pour impl√©menter un chat persist√© sur le backend, bas√© sur un `session_id` fourni par le client.                                                                    | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence/client.ipynb)                   |
| [RunnableWithMessageHistory](https://python.langchain.com/docs/expression_language/how_to/message_history) pour impl√©menter un chat persist√© sur le backend, bas√© sur un `conversation_id` fourni par le client, et `user_id` (voir Auth pour impl√©menter correctement `user_id`). | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence_and_user/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/chat_with_persistence_and_user/client.ipynb) |
| [Configurable Runnable](https://python.langchain.com/docs/expression_language/how_to/configure) pour cr√©er un retriever qui supporte la configuration du nom de l'index au moment de l'ex√©cution.                                                                                      | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_retrieval/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_retrieval/client.ipynb)                 |
| [Configurable Runnable](https://python.langchain.com/docs/expression_language/how_to/configure) qui montre les champs configurables et les alternatives configurables.                                                                                                      | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_chain/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/configurable_chain/client.ipynb)                         |
| **APIHandler** Montre comment utiliser `APIHandler` au lieu de `add_routes`. Cela offre plus de flexibilit√© aux d√©veloppeurs pour d√©finir des endpoints. Fonctionne bien avec tous les patterns FastAPI, mais n√©cessite un peu plus d'effort.                                                        | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/api_handler_examples/server.py)                                                                                                                               |
| **LCEL Example** Exemple utilisant LCEL pour manipuler une entr√©e de dictionnaire.                                                                                                                                                                                          | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/passthrough_dict/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/passthrough_dict/client.ipynb)                             |
| **Auth** avec `add_routes`: Authentification simple qui peut √™tre appliqu√©e √† tous les endpoints associ√©s √† l'application. (Pas utile en soi pour impl√©menter une logique par utilisateur.)                                                                                           | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/global_deps/server.py)                                                                                                                                   |
| **Auth** avec `add_routes`: M√©canisme d'authentification simple bas√© sur les d√©pendances de chemin. (Pas utile en soi pour impl√©menter une logique par utilisateur.)                                                                                                                    | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/path_dependencies/server.py)                                                                                                                             |
| **Auth** avec `add_routes`: Impl√©menter une logique par utilisateur et une authentification pour les endpoints utilisant un modificateur de configuration par requ√™te. (**Remarque**: Pour le moment, ne s'int√®gre pas avec les documents OpenAPI.)                                                                                 | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/client.ipynb)     |
| **Auth** avec `APIHandler`: Impl√©menter une logique par utilisateur et une authentification montrant comment rechercher uniquement dans les documents poss√©d√©s par l'utilisateur.                                                                                                                                           | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/client.ipynb)                             |
| **Widgets** Diff√©rents widgets pouvant √™tre utilis√©s avec le playground (upload de fichiers et chat)                                                                                                                                                                              | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/tuples/server.py)                                                                                                                                |
| **Widgets** Widget d'upload de fichiers utilis√© pour le playground LangServe.                                                                                                                                                                                                      | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/client.ipynb)                               |

## Application Exemple

### Serveur

Voici un serveur qui d√©ploie un mod√®le de chat OpenAI, un mod√®le de chat Anthropic, et une cha√Æne
qui utilise
le mod√®le Anthropic pour raconter une blague sur un sujet.

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}, {"imported": "ChatAnthropic", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.anthropic.ChatAnthropic.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}, {"imported": "ChatOpenAI", "source": "langchain.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}]-->
#!/usr/bin/env python
from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatAnthropic, ChatOpenAI
from langserve import add_routes

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

add_routes(
    app,
    ChatOpenAI(),
    path="/openai",
)

add_routes(
    app,
    ChatAnthropic(),
    path="/anthropic",
)

model = ChatAnthropic()
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
add_routes(
    app,
    prompt | model,
    path="/joke",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

Si vous avez l'intention d'appeler votre endpoint depuis le navigateur, vous devrez √©galement d√©finir les en-t√™tes CORS.
Vous pouvez utiliser le middleware int√©gr√© de FastAPI pour cela :

```python
from fastapi.middleware.cors import CORSMiddleware

# Set all CORS enabled origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)
```

### Documentation

Si vous avez d√©ploy√© le serveur ci-dessus, vous pouvez consulter les documents OpenAPI g√©n√©r√©s en utilisant :

> ‚ö†Ô∏è Si vous utilisez pydantic v2, les documents ne seront pas g√©n√©r√©s pour _invoke_, _batch_, _stream_,
> _stream_log_. Voir la section [Pydantic](#pydantic) ci-dessous pour plus de d√©tails.

```sh
curl localhost:8000/docs
```

assurez-vous d'**ajouter** le suffixe `/docs`.

> ‚ö†Ô∏è La page d'index `/` n'est pas d√©finie par **conception**, donc `curl localhost:8000` ou visiter
> l'URL
> retournera un 404. Si vous souhaitez du contenu √† `/` d√©finissez un endpoint `@app.get("/")`.

### Client

SDK Python

```python
<!--IMPORTS:[{"imported": "SystemMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}, {"imported": "HumanMessage", "source": "langchain.schema", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}, {"imported": "ChatPromptTemplate", "source": "langchain.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}, {"imported": "RunnableMap", "source": "langchain.schema.runnable", "docs": "https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableMap.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}]-->

from langchain.schema import SystemMessage, HumanMessage
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableMap
from langserve import RemoteRunnable

openai = RemoteRunnable("http://localhost:8000/openai/")
anthropic = RemoteRunnable("http://localhost:8000/anthropic/")
joke_chain = RemoteRunnable("http://localhost:8000/joke/")

joke_chain.invoke({"topic": "parrots"})

# or async
await joke_chain.ainvoke({"topic": "parrots"})

prompt = [
    SystemMessage(content='Act like either a cat or a parrot.'),
    HumanMessage(content='Hello!')
]

# Supports astream
async for msg in anthropic.astream(prompt):
    print(msg, end="", flush=True)

prompt = ChatPromptTemplate.from_messages(
    [("system", "Tell me a long story about {topic}")]
)

# Can define custom chains
chain = prompt | RunnableMap({
    "openai": openai,
    "anthropic": anthropic,
})

chain.batch([{"topic": "parrots"}, {"topic": "cats"}])
```

En TypeScript (n√©cessite LangChain.js version 0.0.166 ou ult√©rieure):

```typescript
import { RemoteRunnable } from "@langchain/core/runnables/remote";

const chain = new RemoteRunnable({
  url: `http://localhost:8000/joke/`,
});
const result = await chain.invoke({
  topic: "cats",
});
```

Python utilisant `requests`:

```python
import requests

response = requests.post(
    "http://localhost:8000/joke/invoke",
    json={'input': {'topic': 'cats'}}
)
response.json()
```

Vous pouvez √©galement utiliser `curl`:

```sh
curl --location --request POST 'http://localhost:8000/joke/invoke' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "input": {
            "topic": "cats"
        }
    }'
```

## Endpoints

Le code suivant:

```python
...
add_routes(
    app,
    runnable,
    path="/my_runnable",
)
```

ajoute ces endpoints au serveur :

- `POST /my_runnable/invoke` - invoquer le runnable sur une seule entr√©e
- `POST /my_runnable/batch` - invoquer le runnable sur un lot d'entr√©es
- `POST /my_runnable/stream` - invoquer sur une seule entr√©e et diffuser la sortie
- `POST /my_runnable/stream_log` - invoquer sur une seule entr√©e et diffuser la sortie, y compris la sortie des √©tapes interm√©diaires au fur et √† mesure qu'elles sont g√©n√©r√©es
- `POST /my_runnable/astream_events` - invoquer sur une seule entr√©e et diffuser des √©v√©nements au fur et √† mesure qu'ils sont g√©n√©r√©s, y compris √† partir d'√©tapes interm√©diaires.
- `GET /my_runnable/input_schema` - sch√©ma json pour l'entr√©e du runnable
- `GET /my_runnable/output_schema` - sch√©ma json pour la sortie du runnable
- `GET /my_runnable/config_schema` - sch√©ma json pour la configuration du runnable

Ces endpoints correspondent √†
l'[interface LangChain Expression Language](https://python.langchain.com/docs/expression_language/interface) -- veuillez vous r√©f√©rer √† cette documentation pour plus de d√©tails.

## Playground

Vous pouvez trouver une page de playground pour votre runnable √† `/my_runnable/playground/`. Cela
expose une interface utilisateur simple pour [configurer](https://python.langchain.com/docs/expression_language/how_to/configure) et invoquer votre runnable avec une sortie en flux et des √©tapes interm√©diaires.

<p align="center">
<img src="https://github.com/langchain-ai/langserve/assets/3205522/5ca56e29-f1bb-40f4-84b5-15916384a276" width="50%"/>
</p>

### Widgets

Le playground prend en charge les [widgets](#playground-widgets) et peut √™tre utilis√© pour tester votre runnable avec diff√©rentes entr√©es. Voir la section [widgets](#widgets) ci-dessous pour plus de d√©tails.

### Partage

De plus, pour les runnables configurables, le playground vous permettra de configurer le runnable et de partager un lien avec la configuration :

<p align="center">
<img src="https://github.com/langchain-ai/langserve/assets/3205522/86ce9c59-f8e4-4d08-9fa3-62030e0f521d" width="50%"/>
</p>

## Chat playground

LangServe prend √©galement en charge un playground ax√© sur le chat que vous pouvez activer et utiliser sous `/my_runnable/playground/`.
Contrairement au playground g√©n√©ral, seuls certains types de runnables sont pris en charge - le sch√©ma d'entr√©e du runnable doit
√™tre un `dict` avec soit :

- une seule cl√©, et la valeur de cette cl√© doit √™tre une liste de messages de chat.
- deux cl√©s, dont l'une des valeurs est une liste de messages, et l'autre repr√©sentant le message le plus r√©cent.

Nous vous recommandons d'utiliser le premier format.

Le runnable doit √©galement renvoyer soit un `AIMessage` soit une cha√Æne de caract√®res.

Pour l'activer, vous devez d√©finir `playground_type="chat",` lors de l'ajout de votre route. Voici un exemple :

```python
# Declare a chain
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful, professional assistant named Cob."),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

chain = prompt | ChatAnthropic(model="claude-2")


class InputChat(BaseModel):
    """Input for the chat endpoint."""

    messages: List[Union[HumanMessage, AIMessage, SystemMessage]] = Field(
        ...,
        description="The chat messages representing the current conversation.",
    )


add_routes(
    app,
    chain.with_types(input_type=InputChat),
    enable_feedback_endpoint=True,
    enable_public_trace_link_endpoint=True,
    playground_type="chat",
)
```

Si vous utilisez LangSmith, vous pouvez √©galement d√©finir `enable_feedback_endpoint=True` sur votre route pour activer les boutons de pouce vers le haut/pouce vers le bas apr√®s chaque message, et `enable_public_trace_link_endpoint=True` pour ajouter un bouton qui cr√©e des traces publiques pour les ex√©cutions.
Notez que vous devrez √©galement d√©finir les variables d'environnement suivantes :

```bash
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_PROJECT="YOUR_PROJECT_NAME"
export LANGCHAIN_API_KEY="YOUR_API_KEY"
```

Voici un exemple avec les deux options ci-dessus activ√©es :

<p align="center">
<img src="./.github/img/chat_playground.png" width="50%"/>
</p>

Remarque : Si vous activez les liens de traces publiques, les internes de votre cha√Æne seront expos√©s. Nous recommandons d'utiliser ce param√®tre uniquement pour des d√©mos ou des tests.

## Legacy Chains

LangServe fonctionne avec les Runnables (construits via [LangChain Expression Language](https://python.langchain.com/docs/expression_language/)) et les cha√Ænes h√©rit√©es (h√©ritant de `Chain`). Cependant, certains des sch√©mas d'entr√©e pour les cha√Ænes h√©rit√©es peuvent √™tre incomplets/incorrects, entra√Ænant des erreurs.
Cela peut √™tre corrig√© en mettant √† jour la propri√©t√© `input_schema` de ces cha√Ænes dans LangChain. Si vous rencontrez des erreurs, veuillez ouvrir un probl√®me sur CE d√©p√¥t, et nous travaillerons pour y rem√©dier.

## D√©ploiement

### D√©ployer sur AWS

Vous pouvez d√©ployer sur AWS en utilisant le [AWS Copilot CLI](https://aws.github.io/copilot-cli/)

```bash
copilot init --app [application-name] --name [service-name] --type 'Load Balanced Web Service' --dockerfile './Dockerfile' --deploy
```

Cliquez [ici](https://aws.amazon.com/containers/copilot/) pour en savoir plus.

### D√©ployer sur Azure

Vous pouvez d√©ployer sur Azure en utilisant Azure Container Apps (Serverless) :

```shell
az containerapp up --name [container-app-name] --source . --resource-group [resource-group-name] --environment  [environment-name] --ingress external --target-port 8001 --env-vars=OPENAI_API_KEY=your_key
```

Vous pouvez trouver plus d'informations [ici](https://learn.microsoft.com/en-us/azure/container-apps/containerapp-up)

### D√©ployer sur GCP

Vous pouvez d√©ployer sur GCP Cloud Run en utilisant la commande suivante :

```shell
gcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key
```

### Contribution de la communaut√©

#### D√©ployer sur Railway

[Exemple de d√©p√¥t Railway](https://github.com/PaulLockett/LangServe-Railway/tree/main)

[![D√©ployer sur Railway](https://railway.app/button.svg)](https://railway.app/template/pW9tXP?referralCode=c-aq4K)

## Pydantic

LangServe prend en charge Pydantic 2 avec certaines limitations.

1. Les documents OpenAPI ne seront pas g√©n√©r√©s pour invoke/batch/stream/stream_log lors de l'utilisation de Pydantic V2. Fast API ne prend pas en charge [le m√©lange des namespaces pydantic v1 et v2].
2. LangChain utilise le namespace v1 dans Pydantic v2. Veuillez lire les [directives suivantes pour assurer la compatibilit√© avec LangChain](https://github.com/langchain-ai/langchain/discussions/9337)

√Ä l'exception de ces limitations, nous nous attendons √† ce que les endpoints API, le playground et toutes les autres fonctionnalit√©s fonctionnent comme pr√©vu.

## Avanc√©

### Gestion de l'authentification

Si vous avez besoin d'ajouter de l'authentification √† votre serveur, veuillez lire la documentation de Fast API sur les [d√©pendances](https://fastapi.tiangolo.com/tutorial/dependencies/) et la [s√©curit√©](https://fastapi.tiangolo.com/tutorial/security/).

Les exemples ci-dessous montrent comment connecter la logique d'authentification aux endpoints LangServe en utilisant les primitives FastAPI.

Vous √™tes responsable de fournir la logique d'authentification r√©elle, la table des utilisateurs, etc.

Si vous n'√™tes pas s√ªr de ce que vous faites, vous pouvez essayer d'utiliser une solution existante [Auth0](https://auth0.com/).

#### Utilisation de add_routes

Si vous utilisez `add_routes`, consultez les exemples [ici](https://github.com/langchain-ai/langserve/tree/main/examples/auth).

| Description                                                                                                                                                                        | Liens                                                                                                                                                                                                                           |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Auth** avec `add_routes` : Authentification simple pouvant √™tre appliqu√©e √† tous les endpoints associ√©s √† l'application. (Pas utile en soi pour impl√©menter une logique par utilisateur.)           | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/global_deps/server.py)                                                                                                                               |
| **Auth** avec `add_routes` : M√©canisme d'authentification simple bas√© sur les d√©pendances de chemin. (Pas utile en soi pour impl√©menter une logique par utilisateur.)                                    | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/path_dependencies/server.py)                                                                                                                         |
| **Auth** avec `add_routes` : Impl√©menter une logique par utilisateur et une authentification pour les endpoints qui utilisent un modificateur de configuration par requ√™te. (**Remarque** : Pour le moment, ne s'int√®gre pas avec les documents OpenAPI.) | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/per_req_config_modifier/client.ipynb) |

Alternativement, vous pouvez utiliser le [middleware](https://fastapi.tiangolo.com/tutorial/middleware/) de FastAPI.

Utiliser des d√©pendances globales et des d√©pendances de chemin a l'avantage que l'auth sera correctement prise en charge dans la page de documentation OpenAPI, mais ces options ne sont pas suffisantes pour impl√©menter une logique par utilisateur (par exemple, cr√©er une application qui ne peut rechercher que dans les documents appartenant √† l'utilisateur).

Si vous devez impl√©menter une logique par utilisateur, vous pouvez utiliser le `per_req_config_modifier` ou `APIHandler` (ci-dessous) pour impl√©menter cette logique.

**Par utilisateur**

Si vous avez besoin d'une autorisation ou d'une logique d√©pendant de l'utilisateur, sp√©cifiez `per_req_config_modifier` lors de l'utilisation de `add_routes`. Utilisez un callable qui re√ßoit l'objet `Request` brut et peut en extraire les informations pertinentes pour l'authentification et les autorisations.

#### Utilisation de APIHandler

Si vous vous sentez √† l'aise avec FastAPI et python, vous pouvez utiliser le [APIHandler](https://github.com/langchain-ai/langserve/blob/main/examples/api_handler_examples/server.py) de LangServe.

| Description                                                                                                                                                                                                 | Liens                                                                                                                                                                                                           |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Auth** avec `APIHandler` : Impl√©menter une logique par utilisateur et une authentification montrant comment rechercher uniquement dans les documents appartenant √† l'utilisateur.                                                                                    | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/auth/api_handler/client.ipynb)         |
| **APIHandler** Montre comment utiliser `APIHandler` au lieu de `add_routes`. Cela fournit plus de flexibilit√© aux d√©veloppeurs pour d√©finir des endpoints. Fonctionne bien avec tous les mod√®les FastAPI, mais demande un peu plus d'effort. | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/api_handler_examples/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/api_handler_examples/client.ipynb) |

C'est un peu plus de travail, mais cela vous donne un contr√¥le total sur la d√©finition des endpoints, vous pouvez donc faire toute la logique personnalis√©e dont vous avez besoin pour l'authentification.

### Fichiers

Les applications LLM traitent souvent des fichiers. Il existe diff√©rentes architectures
qui peuvent √™tre mises en place pour impl√©menter le traitement des fichiers ; √† un niveau √©lev√© :

1. Le fichier peut √™tre t√©l√©charg√© sur le serveur via un point de terminaison d√©di√© et trait√© en utilisant un
   point de terminaison s√©par√©
2. Le fichier peut √™tre t√©l√©charg√© soit par valeur (octets du fichier) soit par r√©f√©rence (par exemple, URL s3
   pour le contenu du fichier)
3. Le point de terminaison de traitement peut √™tre bloquant ou non bloquant
4. Si un traitement significatif est requis, le traitement peut √™tre d√©l√©gu√© √† un pool de processus d√©di√©

Vous devez d√©terminer quelle est l'architecture appropri√©e pour votre application.

Actuellement, pour t√©l√©charger des fichiers par valeur vers un runnable, utilisez l'encodage base64 pour le
fichier (`multipart/form-data` n'est pas encore support√©).

Voici
un [exemple](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing)
qui montre
comment utiliser l'encodage base64 pour envoyer un fichier √† un runnable distant.

N'oubliez pas que vous pouvez toujours t√©l√©charger des fichiers par r√©f√©rence (par exemple, URL s3) ou les t√©l√©charger en tant que
multipart/form-data vers un point de terminaison d√©di√©.

### Types d'Entr√©e et de Sortie Personnalis√©s

Les types d'entr√©e et de sortie sont d√©finis sur tous les runnables.

Vous pouvez y acc√©der via les propri√©t√©s `input_schema` et `output_schema`.

`LangServe` utilise ces types pour la validation et la documentation.

Si vous souhaitez remplacer les types d√©duits par d√©faut, vous pouvez utiliser la m√©thode `with_types`.

Voici un exemple simple pour illustrer l'id√©e :

```python
<!--IMPORTS:[{"imported": "RunnableLambda", "source": "langchain.schema.runnable", "docs": "https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}]-->
from typing import Any

from fastapi import FastAPI
from langchain.schema.runnable import RunnableLambda

app = FastAPI()


def func(x: Any) -> int:
    """Mistyped function that should accept an int but accepts anything."""
    return x + 1


runnable = RunnableLambda(func).with_types(
    input_type=int,
)

add_routes(app, runnable)
```

### Types d'Utilisateurs Personnalis√©s

H√©ritez de `CustomUserType` si vous souhaitez que les donn√©es se d√©s√©rialisent dans un
mod√®le pydantic plut√¥t que l'√©quivalent de la repr√©sentation dict.

Pour le moment, ce type ne fonctionne que c√¥t√© _serveur_ et est utilis√©
pour sp√©cifier le comportement de _d√©codage_ souhait√©. Si vous h√©ritez de ce type,
le serveur conservera le type d√©cod√© en tant que mod√®le pydantic au lieu de le convertir en dict.

```python
<!--IMPORTS:[{"imported": "RunnableLambda", "source": "langchain.schema.runnable", "docs": "https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html", "title": "\ud83e\udd9c\ufe0f\ud83c\udfd3 LangServe"}]-->
from fastapi import FastAPI
from langchain.schema.runnable import RunnableLambda

from langserve import add_routes
from langserve.schema import CustomUserType

app = FastAPI()


class Foo(CustomUserType):
    bar: int


def func(foo: Foo) -> int:
    """Sample function that expects a Foo type which is a pydantic model"""
    assert isinstance(foo, Foo)
    return foo.bar


# Note that the input and output type are automatically inferred!
# You do not need to specify them.
# runnable = RunnableLambda(func).with_types( # <-- Not needed in this case
#     input_type=Foo,
#     output_type=int,
#
add_routes(app, RunnableLambda(func), path="/foo")
```

### Widgets du Terrain de Jeu

Le terrain de jeu vous permet de d√©finir des widgets personnalis√©s pour votre runnable depuis le backend.

Voici quelques exemples :

| Description                                                                           | Liens                                                                                                                                                                                                 |
| :------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Widgets** Diff√©rents widgets qui peuvent √™tre utilis√©s avec le terrain de jeu (t√©l√©chargement de fichiers et chat) | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/tuples/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/client.ipynb)     |
| **Widgets** Widget de t√©l√©chargement de fichiers utilis√© pour le terrain de jeu LangServe.                         | [serveur](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/server.py), [client](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing/client.ipynb) |

#### Sch√©ma

- Un widget est sp√©cifi√© au niveau du champ et exp√©di√© dans le cadre du sch√©ma JSON du
  type d'entr√©e
- Un widget doit contenir une cl√© appel√©e `type` avec la valeur √©tant l'un d'une liste bien connue
  de widgets
- Les autres cl√©s de widget seront associ√©es √† des valeurs d√©crivant des chemins dans un objet JSON

```typescript
type JsonPath = number | string | (number | string)[];
type NameSpacedPath = { title: string; path: JsonPath }; // Using title to mimick json schema, but can use namespace
type OneOfPath = { oneOf: JsonPath[] };

type Widget = {
  type: string; // Some well known type (e.g., base64file, chat etc.)
  [key: string]: JsonPath | NameSpacedPath | OneOfPath;
};
```

### Widgets Disponibles

Il n'y a que deux widgets que l'utilisateur peut sp√©cifier manuellement pour le moment :

1. Widget de T√©l√©chargement de Fichiers
2. Widget d'Historique de Chat

Voir ci-dessous plus d'informations sur ces widgets.

Tous les autres widgets sur l'interface utilisateur du terrain de jeu sont cr√©√©s et g√©r√©s automatiquement par l'interface utilisateur
bas√©e sur le sch√©ma de configuration du Runnable. Lorsque vous cr√©ez des Runnables Configurables,
le terrain de jeu devrait cr√©er des widgets appropri√©s pour vous permettre de contr√¥ler le comportement.

#### Widget de T√©l√©chargement de Fichiers

Permet la cr√©ation d'une entr√©e de t√©l√©chargement de fichiers dans l'interface utilisateur du terrain de jeu pour les fichiers
qui sont t√©l√©charg√©s sous forme de cha√Ænes encod√©es en base64. Voici le
[exemple complet](https://github.com/langchain-ai/langserve/tree/main/examples/file_processing).

Extrait :

```python
try:
    from pydantic.v1 import Field
except ImportError:
    from pydantic import Field

from langserve import CustomUserType


# ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise
#            the server will decode it into a dict instead of a pydantic model.
class FileProcessingRequest(CustomUserType):
    """Request including a base64 encoded file."""

    # The extra field is used to specify a widget for the playground UI.
    file: str = Field(..., extra={"widget": {"type": "base64file"}})
    num_chars: int = 100

```

Exemple de widget :

<p align="center">
<img src="https://github.com/langchain-ai/langserve/assets/3205522/52199e46-9464-4c2e-8be8-222250e08c3f" width="50%"/>
</p>

### Widget de Chat

Regardez
l'[exemple de widget](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/tuples/server.py).

Pour d√©finir un widget de chat, assurez-vous de passer "type": "chat".

- "input" est JSONPath vers le champ dans la _Requ√™te_ qui contient le nouveau message d'entr√©e.
- "output" est JSONPath vers le champ dans la _R√©ponse_ qui contient le(s) nouveau(x) message(s) de sortie.
- Ne sp√©cifiez pas ces champs si l'entr√©e ou la sortie enti√®re doit √™tre utilis√©e telle quelle (
  par exemple, si la sortie est une liste de messages de chat.)

Voici un extrait :

```python
class ChatHistory(CustomUserType):
    chat_history: List[Tuple[str, str]] = Field(
        ...,
        examples=[[("human input", "ai response")]],
        extra={"widget": {"type": "chat", "input": "question", "output": "answer"}},
    )
    question: str


def _format_to_messages(input: ChatHistory) -> List[BaseMessage]:
    """Format the input to a list of messages."""
    history = input.chat_history
    user_input = input.question

    messages = []

    for human, ai in history:
        messages.append(HumanMessage(content=human))
        messages.append(AIMessage(content=ai))
    messages.append(HumanMessage(content=user_input))
    return messages


model = ChatOpenAI()
chat_model = RunnableParallel({"answer": (RunnableLambda(_format_to_messages) | model)})
add_routes(
    app,
    chat_model.with_types(input_type=ChatHistory),
    config_keys=["configurable"],
    path="/chat",
)
```

Exemple de widget :

<p align="center">
<img src="https://github.com/langchain-ai/langserve/assets/3205522/a71ff37b-a6a9-4857-a376-cf27c41d3ca4" width="50%"/>
</p>

Vous pouvez √©galement sp√©cifier une liste de messages en tant que param√®tre directement, comme montr√© dans cet extrait :

```python
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assisstant named Cob."),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

chain = prompt | ChatAnthropic(model="claude-2")


class MessageListInput(BaseModel):
    """Input for the chat endpoint."""
    messages: List[Union[HumanMessage, AIMessage]] = Field(
        ...,
        description="The chat messages representing the current conversation.",
        extra={"widget": {"type": "chat", "input": "messages"}},
    )


add_routes(
    app,
    chain.with_types(input_type=MessageListInput),
    path="/chat",
)
```

Voir [ce fichier d'exemple](https://github.com/langchain-ai/langserve/tree/main/examples/widgets/chat/message_list/server.py) pour un exemple.

### Activation / D√©sactivation des Points de Terminaison (LangServe >=0.0.33)

Vous pouvez activer / d√©sactiver les points de terminaison qui sont expos√©s lors de l'ajout de routes pour une cha√Æne donn√©e.

Utilisez `enabled_endpoints` si vous voulez vous assurer de ne jamais obtenir un nouveau point de terminaison lors de la mise √† niveau de langserve √† une version plus r√©cente.

Activer : Le code ci-dessous n'activera que `invoke`, `batch` et les
variantes correspondantes du point de terminaison `config_hash`.

```python
add_routes(app, chain, enabled_endpoints=["invoke", "batch", "config_hashes"], path="/mychain")
```

D√©sactiver : Le code ci-dessous d√©sactivera le terrain de jeu pour la cha√Æne

```python
add_routes(app, chain, disabled_endpoints=["playground"], path="/mychain")
```
