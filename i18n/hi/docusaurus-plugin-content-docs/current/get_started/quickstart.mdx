---
sidebar_position: 1
translated: true
---

# त्वरित प्रारम्भ

इस त्वरित प्रारम्भ में हम आपको दिखाएंगे कि कैसे:
- LangChain, LangSmith और LangServe के साथ सेटअप करें
- LangChain के सबसे बुनियादी और सामान्य घटकों का उपयोग करें: प्रॉम्प्ट टेम्प्लेट्स, मॉडल्स, और आउटपुट पार्सर्स
- LangChain Expression Language का उपयोग करें, जो कि LangChain पर आधारित प्रोटोकॉल है और घटक चेनिंग को सुविधाजनक बनाता है
- LangChain के साथ एक सरल एप्लिकेशन बनाएं
- LangSmith के साथ अपने एप्लिकेशन को ट्रेस करें
- LangServe के साथ अपने एप्लिकेशन को सर्व करें

यह कवर करने के लिए एक उचित राशि है! चलिए इसमें गहराई से जाते हैं।

## सेटअप

### Jupyter Notebook

यह गाइड (और डाक्यूमेंटेशन में अधिकांश अन्य गाइड्स) [Jupyter notebooks](https://jupyter.org/) का उपयोग करता है और मानता है कि पाठक भी ऐसा ही कर रहे हैं। Jupyter notebooks LLM सिस्टम के साथ काम करना सीखने के लिए सही हैं क्योंकि अक्सर चीजें गलत हो सकती हैं (अप्रत्याशित आउटपुट, API डाउन, आदि) और एक इंटरैक्टिव वातावरण में गाइड्स के माध्यम से जाना उन्हें बेहतर समझने का एक शानदार तरीका है।

आपको Jupyter Notebook में गाइड से गुजरने की आवश्यकता नहीं है, लेकिन यह सिफारिश की जाती है। स्थापना के निर्देशों के लिए [यहां देखें](https://jupyter.org/install)।

### इंस्टालेशन

LangChain को इंस्टॉल करने के लिए रन करें:

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="pip" label="Pip" default>
    <CodeBlock language="bash">pip install langchain</CodeBlock>
  </TabItem>
  <TabItem value="conda" label="Conda">
    <CodeBlock language="bash">conda install langchain -c conda-forge</CodeBlock>
  </TabItem>
</Tabs>

अधिक विवरण के लिए, हमारे [इंस्टालेशन गाइड](/docs/get_started/installation) देखें।

### LangSmith

कई एप्लिकेशंस जो आप LangChain के साथ बनाते हैं उनमें कई चरण होते हैं जिनमें LLM कॉल्स की कई बार आवश्यकता होती है।
जैसे-जैसे ये एप्लिकेशंस अधिक से अधिक जटिल होते जाते हैं, यह जानने में सक्षम होना महत्वपूर्ण हो जाता है कि आपके चेन या एजेंट के अंदर वास्तव में क्या हो रहा है।
इसे करने का सबसे अच्छा तरीका [LangSmith](https://smith.langchain.com) का उपयोग करना है।

ध्यान दें कि LangSmith की आवश्यकता नहीं है, लेकिन यह सहायक है।
यदि आप LangSmith का उपयोग करना चाहते हैं, तो ऊपर दिए गए लिंक पर साइन अप करने के बाद, सुनिश्चित करें कि आप अपने परिवेश चर सेट करें ताकि ट्रेस लॉगिंग शुरू हो सके:

```shell
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

## LangChain के साथ निर्माण

LangChain डेटा और गणना के बाहरी स्रोतों को LLMs से जोड़ने वाले एप्लिकेशन बनाने में सक्षम बनाता है।
इस त्वरित प्रारम्भ में, हम कुछ विभिन्न तरीकों से ऐसा करने के बारे में बताएंगे।
हम एक सरल LLM चेन से शुरू करेंगे, जो केवल प्रॉम्प्ट टेम्प्लेट में जानकारी पर निर्भर करती है।
अगला, हम एक रिट्रीवल चेन बनाएंगे, जो एक अलग डेटाबेस से डेटा लाता है और उसे प्रॉम्प्ट टेम्प्लेट में पास करता है।
फिर हम चैट इतिहास जोड़ेंगे, एक वार्तालाप रिट्रीवल चेन बनाने के लिए। यह आपको इस LLM के साथ एक चैट तरीके से इंटरैक्ट करने की अनुमति देता है, ताकि यह पिछले प्रश्नों को याद रख सके।
अंत में, हम एक एजेंट बनाएंगे - जो यह निर्धारित करने के लिए एक LLM का उपयोग करता है कि क्या प्रश्नों का उत्तर देने के लिए डेटा लाना आवश्यक है।
हम इन्हें उच्च स्तर पर कवर करेंगे, लेकिन इनमें बहुत सारे विवरण हैं!
हम संबंधित डॉक्स के लिंक प्रदान करेंगे।

## LLM चेन

हम दिखाएंगे कि API द्वारा उपलब्ध मॉडल्स का उपयोग कैसे करें, जैसे OpenAI, और स्थानीय ओपन सोर्स मॉडल्स, Ollama जैसे इंटीग्रेशन का उपयोग करके।

<Tabs>
  <TabItem value="openai" label="OpenAI" default>

पहले हमें LangChain x OpenAI इंटीग्रेशन पैकेज को इम्पोर्ट करना होगा।

```shell
pip install langchain-openai
```

API को एक्सेस करने के लिए एक API कुंजी की आवश्यकता होती है, जिसे आप एक खाता बनाकर और [यहां](https://platform.openai.com/account/api-keys) जाकर प्राप्त कर सकते हैं। एक बार जब हमारे पास कुंजी हो जाती है, तो हम इसे एक परिवेश चर के रूप में सेट करना चाहेंगे:

```shell
export OPENAI_API_KEY="..."
```

फिर हम मॉडल को प्रारंभ कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}]-->
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
```

यदि आप परिवेश चर सेट करना पसंद नहीं करते हैं तो आप OpenAI LLM क्लास को प्रारंभ करते समय `api_key` नामक पैरामीटर के माध्यम से कुंजी सीधे पास कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}]-->
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(api_key="...")
```

  </TabItem>
  <TabItem value="local" label="Local (using Ollama)">

[Ollama](https://ollama.ai/) आपको ओपन-सोर्स बड़े भाषा मॉडल्स, जैसे Llama 2, स्थानीय रूप से चलाने की अनुमति देता है।

पहले, एक स्थानीय Ollama इंस्टेंस सेट अप और रन करने के लिए [इन निर्देशों](https://github.com/jmorganca/ollama) का पालन करें:

* [डाउनलोड करें](https://ollama.ai/download)
* `ollama pull llama2` के माध्यम से एक मॉडल प्राप्त करें

फिर, सुनिश्चित करें कि Ollama सर्वर चल रहा है। उसके बाद, आप कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "Ollama", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html", "title": "Quickstart"}]-->
from langchain_community.llms import Ollama
llm = Ollama(model="llama2")
```

  </TabItem>
  <TabItem value="anthropic" label="Anthropic">

पहले हमें LangChain x Anthropic पैकेज को इम्पोर्ट करना होगा।

```shell
pip install langchain-anthropic
```

API को एक्सेस करने के लिए एक API कुंजी की आवश्यकता होती है, जिसे आप एक खाता बनाकर [यहां](https://claude.ai/login) जाकर प्राप्त कर सकते हैं। एक बार जब हमारे पास कुंजी हो जाती है, तो हम इसे एक परिवेश चर के रूप में सेट करना चाहेंगे:

```shell
export ANTHROPIC_API_KEY="..."
```

फिर हम मॉडल को प्रारंभ कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Quickstart"}]-->
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0.2, max_tokens=1024)
```

यदि आप परिवेश चर सेट करना पसंद नहीं करते हैं तो आप Anthropic Chat Model क्लास को प्रारंभ करते समय कुंजी सीधे `api_key` नामक पैरामीटर के माध्यम से पास कर सकते हैं:

```python
llm = ChatAnthropic(api_key="...")
```

  </TabItem>
  <TabItem value="cohere" label="Cohere">

पहले हमें Cohere SDK पैकेज को इम्पोर्ट करना होगा।

```shell
pip install langchain-cohere
```

API को एक्सेस करने के लिए एक API कुंजी की आवश्यकता होती है, जिसे आप एक खाता बनाकर और [यहां](https://dashboard.cohere.com/api-keys) जाकर प्राप्त कर सकते हैं। एक बार जब हमारे पास कुंजी हो जाती है, तो हम इसे एक परिवेश चर के रूप में सेट करना चाहेंगे:

```shell
export COHERE_API_KEY="..."
```

फिर हम मॉडल को प्रारंभ कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Quickstart"}]-->
from langchain_cohere import ChatCohere

llm = ChatCohere()
```

यदि आप परिवेश चर सेट करना पसंद नहीं करते हैं तो आप Cohere LLM क्लास को प्रारंभ करते समय कुंजी सीधे `cohere_api_key` नामक पैरामीटर के माध्यम से पास कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Quickstart"}]-->
from langchain_cohere import ChatCohere

llm = ChatCohere(cohere_api_key="...")
```

  </TabItem>
</Tabs>

एक बार जब आपने अपनी पसंद का LLM इंस्टॉल और प्रारंभ कर लिया है, तो हम इसका उपयोग कर सकते हैं!
आइए इसे पूछें कि LangSmith क्या है - यह कुछ ऐसा है जो प्रशिक्षण डेटा में मौजूद नहीं था इसलिए इसका उत्तर बहुत अच्छा नहीं होना चाहिए।

```python
llm.invoke("how can langsmith help with testing?")
```

हम प्रॉम्प्ट टेम्प्लेट के साथ इसके उत्तर को भी गाइड कर सकते हैं।
प्रॉम्प्ट टेम्प्लेट कच्चे उपयोगकर्ता इनपुट को बेहतर इनपुट में बदलते हैं।

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Quickstart"}]-->
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a world class technical documentation writer."),
    ("user", "{input}")
])
```

अब हम इन्हें एक सरल LLM चेन में मिला सकते हैं:

```python
chain = prompt | llm
```

अब हम इसे इनवोक कर सकते हैं और वही प्रश्न पूछ सकते हैं। इसे अभी भी उत्तर नहीं पता होगा, लेकिन यह एक तकनीकी लेखक के लिए अधिक उचित स्वर में उत्तर देना चाहिए!

```python
chain.invoke({"input": "how can langsmith help with testing?"})
```

ChatModel का आउटपुट (और इसलिए, इस चेन का) एक संदेश होता है। हालांकि, स्ट्रिंग्स के साथ काम करना अक्सर बहुत अधिक सुविधाजनक होता है। चलिए एक सरल आउटपुट पार्सर जोड़ते हैं जो चैट संदेश को स्ट्रिंग में बदल देता है।

```python
<!--IMPORTS:[{"imported": "StrOutputParser", "source": "langchain_core.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html", "title": "Quickstart"}]-->
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()
```

अब हम इसे पिछले चेन में जोड़ सकते हैं:

```python
chain = prompt | llm | output_parser
```

अब हम इसे इनवोक कर सकते हैं और वही प्रश्न पूछ सकते हैं। उत्तर अब एक स्ट्रिंग होगा (ChatMessage के बजाय)।

```python
chain.invoke({"input": "how can langsmith help with testing?"})
```

### गहराई में गोता लगाना

अब हमने सफलतापूर्वक एक बुनियादी LLM चेन सेटअप कर लिया है। हमने केवल प्रॉम्प्ट्स, मॉडल्स और आउटपुट पार्सर्स के बुनियादी तत्वों को छुआ है - यहां उल्लेखित सभी चीजों की गहराई में जाने के लिए, [इस दस्तावेज़ के अनुभाग](/docs/modules/model_io) को देखें।

## रिट्रीवल चेन

मूल प्रश्न ("लैंगस्मिथ परीक्षण में कैसे मदद कर सकता है?") का सही उत्तर देने के लिए, हमें LLM को अतिरिक्त संदर्भ प्रदान करने की आवश्यकता है।
हम इसे *retrieval* के माध्यम से कर सकते हैं।
रिट्रीवल तब उपयोगी होता है जब आपके पास **बहुत अधिक डेटा** होता है जिसे सीधे LLM को पास नहीं कर सकते।
आप फिर एक रिट्रीवर का उपयोग करके केवल सबसे प्रासंगिक टुकड़े प्राप्त कर सकते हैं और उन्हें पास कर सकते हैं।

इस प्रक्रिया में, हम एक *Retriever* से प्रासंगिक दस्तावेज़ों को देखेंगे और फिर उन्हें prompt में पास करेंगे।
एक रिट्रीवर किसी भी चीज़ द्वारा समर्थित हो सकता है - एक SQL टेबल, इंटरनेट, आदि - लेकिन इस उदाहरण में हम एक वेक्टर स्टोर भरेंगे और उसे रिट्रीवर के रूप में उपयोग करेंगे। वेक्टरस्टोर्स पर अधिक जानकारी के लिए, [इस दस्तावेज़](/docs/modules/data_connection/vectorstores) को देखें।

पहले, हमें उस डेटा को लोड करने की आवश्यकता है जिसे हम इंडेक्स करना चाहते हैं। ऐसा करने के लिए, हम WebBaseLoader का उपयोग करेंगे। इसके लिए [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) को इंस्टॉल करना होगा:

```shell
pip install beautifulsoup4
```

उसके बाद, हम WebBaseLoader को इम्पोर्ट और उपयोग कर सकते हैं।

```python
<!--IMPORTS:[{"imported": "WebBaseLoader", "source": "langchain_community.document_loaders", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html", "title": "Quickstart"}]-->
from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://docs.smith.langchain.com/user_guide")

docs = loader.load()
```

अगला, हमें इसे एक वेक्टरस्टोर में इंडेक्स करना होगा। इसके लिए कुछ घटकों की आवश्यकता होती है, अर्थात् एक [embedding model](/docs/modules/data_connection/text_embedding) और एक [vectorstore](/docs/modules/data_connection/vectorstores)।

एम्बेडिंग मॉडलों के लिए, हम एक बार फिर API के माध्यम से पहुँचने या लोकल मॉडल चलाने के उदाहरण प्रदान करते हैं।

<Tabs>
  <TabItem value="openai" label="OpenAI (API)" default>

सुनिश्चित करें कि आपके पास `langchain_openai` पैकेज इंस्टॉल हो और उपयुक्त पर्यावरण वेरिएबल्स सेट हों (ये वही हैं जो LLM के लिए आवश्यक हैं)।

```python
<!--IMPORTS:[{"imported": "OpenAIEmbeddings", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html", "title": "Quickstart"}]-->
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
```

</TabItem>
<TabItem value="local" label="Local (using Ollama)">

सुनिश्चित करें कि आपका Ollama चल रहा हो (वही सेट अप जैसे LLM के साथ)।

```python
<!--IMPORTS:[{"imported": "OllamaEmbeddings", "source": "langchain_community.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html", "title": "Quickstart"}]-->
from langchain_community.embeddings import OllamaEmbeddings

embeddings = OllamaEmbeddings()
```

  </TabItem>
<TabItem value="cohere" label="Cohere (API)" default>

सुनिश्चित करें कि आपके पास `cohere` पैकेज इंस्टॉल हो और उपयुक्त पर्यावरण वेरिएबल्स सेट हों (ये वही हैं जो LLM के लिए आवश्यक हैं)।

```python
<!--IMPORTS:[{"imported": "CohereEmbeddings", "source": "langchain_cohere.embeddings", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_cohere.embeddings.CohereEmbeddings.html", "title": "Quickstart"}]-->
from langchain_cohere.embeddings import CohereEmbeddings

embeddings = CohereEmbeddings()
```

</TabItem>
</Tabs>

अब, हम इस एम्बेडिंग मॉडल का उपयोग दस्तावेजों को वेक्टरस्टोर में ingest करने के लिए कर सकते हैं।
हम सादगी के लिए एक सरल लोकल वेक्टरस्टोर, [FAISS](/docs/integrations/vectorstores/faiss), का उपयोग करेंगे।

पहले हमें इसके लिए आवश्यक पैकेज इंस्टॉल करने की आवश्यकता है:

```shell
pip install faiss-cpu
```

फिर हम अपना इंडेक्स बना सकते हैं:

```python
<!--IMPORTS:[{"imported": "FAISS", "source": "langchain_community.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html", "title": "Quickstart"}, {"imported": "RecursiveCharacterTextSplitter", "source": "langchain_text_splitters", "docs": "https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html", "title": "Quickstart"}]-->
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)
```

अब जब हमारे पास यह डेटा वेक्टरस्टोर में इंडेक्स हो गया है, हम एक रिट्रीवल चेन बनाएंगे।
यह चेन एक आने वाले प्रश्न को लेगी, प्रासंगिक दस्तावेज़ों को देखेगी, फिर उन दस्तावेज़ों को मूल प्रश्न के साथ LLM में पास करेगी और उससे मूल प्रश्न का उत्तर मांगेगी।

पहले, आइए उस चेन को सेट अप करें जो एक प्रश्न और प्राप्त दस्तावेज़ों को लेती है और एक उत्तर उत्पन्न करती है।

```python
<!--IMPORTS:[{"imported": "create_stuff_documents_chain", "source": "langchain.chains.combine_documents", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html", "title": "Quickstart"}]-->
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")

document_chain = create_stuff_documents_chain(llm, prompt)
```

अगर हम चाहें तो, हम इसे स्वयं चला सकते हैं और सीधे दस्तावेज़ पास कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "Document", "source": "langchain_core.documents", "docs": "https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html", "title": "Quickstart"}]-->
from langchain_core.documents import Document

document_chain.invoke({
    "input": "how can langsmith help with testing?",
    "context": [Document(page_content="langsmith can let you visualize test results")]
})
```

हालांकि, हम चाहते हैं कि दस्तावेज़ पहले हमारे द्वारा सेट अप किए गए रिट्रीवर से आएं।
इस तरह, हम रिट्रीवर का उपयोग करके एक दिए गए प्रश्न के लिए सबसे प्रासंगिक दस्तावेज़ों को गतिशील रूप से चुन सकते हैं और उन्हें पास कर सकते हैं।

```python
<!--IMPORTS:[{"imported": "create_retrieval_chain", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html", "title": "Quickstart"}]-->
from langchain.chains import create_retrieval_chain

retriever = vector.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)
```

अब हम इस चेन को invoke कर सकते हैं। यह एक डिक्शनरी को रिटर्न करता है - LLM का उत्तर `answer` कुंजी में होता है

```python
response = retrieval_chain.invoke({"input": "how can langsmith help with testing?"})
print(response["answer"])

# LangSmith offers several features that can help with testing:...

```

यह उत्तर अधिक सटीक होना चाहिए!

### गहराई में जाना

हमने अब सफलतापूर्वक एक बुनियादी रिट्रीवल चेन सेट अप कर ली है। हमने केवल रिट्रीवल की मूल बातें छूईं - यहां उल्लेखित सभी चीज़ों की गहरी जानकारी के लिए, [इस दस्तावेज़ के अनुभाग](/docs/modules/data_connection) को देखें।

## बातचीत रिट्रीवल चेन

अब तक बनाई गई चेन केवल एकल प्रश्नों का उत्तर दे सकती है। LLM अनुप्रयोगों के मुख्य प्रकारों में से एक चैट बॉट्स हैं। तो हम इस चेन को कैसे बदल सकते हैं ताकि यह फॉलो अप प्रश्नों का उत्तर दे सके?

हम अभी भी `create_retrieval_chain` फ़ंक्शन का उपयोग कर सकते हैं, लेकिन हमें दो चीज़ें बदलने की आवश्यकता है:

1. रिट्रीवल विधि को अब केवल सबसे हालिया इनपुट पर काम नहीं करना चाहिए, बल्कि पूरे इतिहास को ध्यान में रखना चाहिए।
2. अंतिम LLM चेन को भी पूरे इतिहास को ध्यान में रखना चाहिए

**रिट्रीवल को अपडेट करना**

रिट्रीवल को अपडेट करने के लिए, हम एक नई चेन बनाएंगे। यह चेन सबसे हालिया इनपुट (`input`) और बातचीत के इतिहास (`chat_history`) को लेगी और एक LLM का उपयोग करके एक सर्च क्वेरी उत्पन्न करेगी।

```python
<!--IMPORTS:[{"imported": "create_history_aware_retriever", "source": "langchain.chains", "docs": "https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html", "title": "Quickstart"}, {"imported": "MessagesPlaceholder", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html", "title": "Quickstart"}]-->
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

# First we need a prompt that we can pass into an LLM to generate this search query

prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    ("user", "Given the above conversation, generate a search query to look up to get information relevant to the conversation")
])
retriever_chain = create_history_aware_retriever(llm, retriever, prompt)
```

हम इसे उस स्थिति में पास करके परीक्षण कर सकते हैं जहां उपयोगकर्ता एक फॉलो-अप प्रश्न पूछता है।

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Quickstart"}, {"imported": "AIMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html", "title": "Quickstart"}]-->
from langchain_core.messages import HumanMessage, AIMessage

chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
retriever_chain.invoke({
    "chat_history": chat_history,
    "input": "Tell me how"
})
```

आपको देखना चाहिए कि यह लैंगस्मिथ में परीक्षण के बारे में दस्तावेज़ रिटर्न करता है। ऐसा इसलिए है क्योंकि LLM ने एक नई क्वेरी उत्पन्न की, जो चैट इतिहास को फॉलो-अप प्रश्न के साथ जोड़ती है।

अब जब हमारे पास यह नया रिट्रीवर है, तो हम एक नई चेन बना सकते हैं ताकि इन प्राप्त दस्तावेज़ों को ध्यान में रखते हुए बातचीत जारी रहे।

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the user's questions based on the below context:\n\n{context}"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
])
document_chain = create_stuff_documents_chain(llm, prompt)

retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)
```

हम अब इसे एंड-टू-एंड परीक्षण कर सकते हैं:

```python
chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
retrieval_chain.invoke({
    "chat_history": chat_history,
    "input": "Tell me how"
})
```

हम देख सकते हैं कि यह एक सुसंगत उत्तर देता है - हमने सफलतापूर्वक अपनी रिट्रीवल चेन को एक चैटबॉट में बदल दिया है!

## एजेंट

अब तक हमने चेन के उदाहरण बनाए हैं - जहां प्रत्येक चरण पहले से ज्ञात होता है।
अंतिम चीज जो हम बनाएंगे वह एक एजेंट है - जहां LLM यह तय करता है कि कौन से चरण उठाने हैं।

**नोट: इस उदाहरण के लिए हम केवल OpenAI मॉडल का उपयोग करके एजेंट बनाने का तरीका दिखाएंगे, क्योंकि लोकल मॉडल अभी तक पर्याप्त विश्वसनीय नहीं हैं।**

एक एजेंट बनाते समय करने वाली पहली चीजों में से एक यह निर्णय लेना है कि उसे किन टूल्स तक पहुंच होनी चाहिए।
इस उदाहरण के लिए, हम एजेंट को दो टूल्स तक पहुंच देंगे:

1. हमने जो रिट्रीवर अभी बनाया है। यह आसानी से लैंगस्मिथ के बारे में प्रश्नों का उत्तर दे सकेगा
2. एक सर्च टूल। यह आसानी से उन प्रश्नों का उत्तर दे सकेगा जिनके लिए अद्यतित जानकारी की आवश्यकता होती है।

पहले, आइए जो रिट्रीवर हमने अभी बनाया है उसके लिए एक टूल सेट अप करें:

```python
<!--IMPORTS:[{"imported": "create_retriever_tool", "source": "langchain.tools.retriever", "docs": "https://api.python.langchain.com/en/latest/tools/langchain_core.tools.create_retriever_tool.html", "title": "Quickstart"}]-->
from langchain.tools.retriever import create_retriever_tool

retriever_tool = create_retriever_tool(
    retriever,
    "langsmith_search",
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
)
```

हम जो सर्च टूल उपयोग करेंगे वह [Tavily](/docs/integrations/retrievers/tavily) है। इसके लिए एक API कुंजी की आवश्यकता होगी (उनकी मुफ्त टियर काफी उदार है)। उनके प्लेटफ़ॉर्म पर इसे बनाने के बाद, आपको इसे एक पर्यावरण वेरिएबल के रूप में सेट करने की आवश्यकता है:

```shell
export TAVILY_API_KEY=...
```

यदि आप एक API कुंजी सेट अप नहीं करना चाहते हैं, तो आप इस टूल को बनाने से बच सकते हैं।

```python
<!--IMPORTS:[{"imported": "TavilySearchResults", "source": "langchain_community.tools.tavily_search", "docs": "https://api.python.langchain.com/en/latest/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html", "title": "Quickstart"}]-->
from langchain_community.tools.tavily_search import TavilySearchResults

search = TavilySearchResults()
```

अब हम उन टूल्स की एक सूची बना सकते हैं जिनके साथ हम काम करना चाहते हैं:

```python
tools = [retriever_tool, search]
```

अब जब हमारे पास टूल्स हैं, तो हम उन्हें उपयोग करने के लिए एक एजेंट बना सकते हैं। हम इसे जल्दी से कवर करेंगे - यहां वास्तव में क्या हो रहा है, इसकी गहरी जानकारी के लिए, [Agent's Getting Started documentation](/docs/modules/agents) को देखें

पहले langchain hub इंस्टॉल करें

```bash
pip install langchainhub
```

langchain-openai पैकेज इंस्टॉल करें
OpenAI के साथ इंटरैक्ट करने के लिए हमें langchain-openai का उपयोग करना होगा जो OpenAI SDK[https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai] के साथ कनेक्ट करता है।

```bash
pip install langchain-openai
```

अब हम इसका उपयोग एक प्री-डिफाइंड प्रॉम्प्ट प्राप्त करने के लिए कर सकते हैं

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}, {"imported": "create_openai_functions_agent", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html", "title": "Quickstart"}, {"imported": "AgentExecutor", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.AgentExecutor.html", "title": "Quickstart"}]-->
from langchain_openai import ChatOpenAI
from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain.agents import AgentExecutor

# Get the prompt to use - you can modify this!

prompt = hub.pull("hwchase17/openai-functions-agent")

# You need to set OPENAI_API_KEY environment variable or pass it as argument `api_key`.

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```

हम अब एजेंट को invoke कर सकते हैं और देख सकते हैं कि यह कैसे प्रतिक्रिया देता है! हम इससे लैंगस्मिथ के बारे में प्रश्न पूछ सकते हैं:

```python
agent_executor.invoke({"input": "how can langsmith help with testing?"})
```

हम इससे मौसम के बारे में पूछ सकते हैं:

```python
agent_executor.invoke({"input": "what is the weather in SF?"})
```

हम इसके साथ बातचीत कर सकते हैं:

```python
chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
agent_executor.invoke({
    "chat_history": chat_history,
    "input": "Tell me how"
})
```

### गहराई में जाना

हमने अब सफलतापूर्वक एक बुनियादी एजेंट सेट अप कर लिया है। हमने केवल एजेंट्स की मूल बातें छूईं - यहां उल्लेखित सभी चीज़ों की गहरी जानकारी के लिए, [इस दस्तावेज़ के अनुभाग](/docs/modules/agents) को देखें।

## LangServe के साथ सेवा

अब जब हमने एक एप्लिकेशन बना लिया है, हमें इसे सेवा में डालने की आवश्यकता है। यहीं पर LangServe आता है।
LangServe डेवलपर्स को LangChain चेन को एक REST API के रूप में डिप्लॉय करने में मदद करता है। आपको LangChain का उपयोग करने के लिए LangServe का उपयोग करने की आवश्यकता नहीं है, लेकिन इस गाइड में हम दिखाएंगे कि आप LangServe के साथ अपने ऐप को कैसे डिप्लॉय कर सकते हैं।

इस गाइड का पहला भाग जुपिटर नोटबुक में चलाने के लिए था, अब हम उससे बाहर निकलेंगे। हम एक पायथन फ़ाइल बनाएंगे और फिर कमांड लाइन से इसके साथ इंटरैक्ट करेंगे।

इंस्टॉल करें:

```bash
pip install "langserve[all]"
```

### सर्वर

हमारे एप्लिकेशन के लिए एक सर्वर बनाने के लिए हम एक `serve.py` फ़ाइल बनाएंगे। इसमें हमारे एप्लिकेशन को सेवा देने के लिए हमारी लॉजिक होगी। इसमें तीन चीजें शामिल हैं:
1. हमारी चेन की परिभाषा जिसे हमने ऊपर बनाया है
2. हमारा FastAPI ऐप
3. एक रूट की परिभाषा जिससे चेन को सेवा दी जाएगी, जो `langserve.add_routes` के साथ की जाती है

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Quickstart"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Quickstart"}, {"imported": "WebBaseLoader", "source": "langchain_community.document_loaders", "docs": "https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html", "title": "Quickstart"}, {"imported": "OpenAIEmbeddings", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html", "title": "Quickstart"}, {"imported": "FAISS", "source": "langchain_community.vectorstores", "docs": "https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html", "title": "Quickstart"}, {"imported": "RecursiveCharacterTextSplitter", "source": "langchain_text_splitters", "docs": "https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html", "title": "Quickstart"}, {"imported": "create_retriever_tool", "source": "langchain.tools.retriever", "docs": "https://api.python.langchain.com/en/latest/tools/langchain_core.tools.create_retriever_tool.html", "title": "Quickstart"}, {"imported": "TavilySearchResults", "source": "langchain_community.tools.tavily_search", "docs": "https://api.python.langchain.com/en/latest/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html", "title": "Quickstart"}, {"imported": "create_openai_functions_agent", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html", "title": "Quickstart"}, {"imported": "AgentExecutor", "source": "langchain.agents", "docs": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.AgentExecutor.html", "title": "Quickstart"}, {"imported": "BaseMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.base.BaseMessage.html", "title": "Quickstart"}]-->
#!/usr/bin/env python
from typing import List

from fastapi import FastAPI
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import WebBaseLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain.agents import AgentExecutor
from langchain.pydantic_v1 import BaseModel, Field
from langchain_core.messages import BaseMessage
from langserve import add_routes

# 1. Load Retriever

loader = WebBaseLoader("https://docs.smith.langchain.com/user_guide")
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
embeddings = OpenAIEmbeddings()
vector = FAISS.from_documents(documents, embeddings)
retriever = vector.as_retriever()

# 2. Create Tools

retriever_tool = create_retriever_tool(
    retriever,
    "langsmith_search",
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
)
search = TavilySearchResults()
tools = [retriever_tool, search]

# 3. Create Agent

prompt = hub.pull("hwchase17/openai-functions-agent")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# 4. App definition

app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple API server using LangChain's Runnable interfaces",
)

# 5. Adding chain route

# We need to add these input/output schemas because the current AgentExecutor

# is lacking in schemas.

class Input(BaseModel):
    input: str
    chat_history: List[BaseMessage] = Field(
        ...,
        extra={"widget": {"type": "chat", "input": "location"}},
    )

class Output(BaseModel):
    output: str

add_routes(
    app,
    agent_executor.with_types(input_type=Input, output_type=Output),
    path="/agent",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

और यह सब हो गया! यदि हम इस फ़ाइल को निष्पादित करते हैं:

```bash
python serve.py
```

हमें देखना चाहिए कि हमारी चेन localhost:8000 पर सेवा दी जा रही है।

### प्लेग्राउंड

हर LangServe सेवा के साथ एक सरल बिल्ट-इन UI आता है जो एप्लिकेशन को स्ट्रीमिंग आउटपुट और इंटरमीडिएट चरणों में दृश्यता के साथ कॉन्फ़िगर और invoke करने के लिए होता है।
इसे आज़माने के लिए http://localhost:8000/agent/playground/ पर जाएं! वही प्रश्न पास करें जो पहले था - "लैंगस्मिथ परीक्षण में कैसे मदद कर सकता है?" - और यह पहले की तरह ही उत्तर देना चाहिए।

### क्लाइंट

अब आइए अपने सेवा के साथ प्रोग्रामेटिकली इंटरैक्ट करने के लिए एक क्लाइंट सेट अप करें। हम इसे आसानी से `[langserve.RemoteRunnable](/docs/langserve#client)` का उपयोग करके कर सकते हैं।
इसका उपयोग करके, हम सेवा की चेन के साथ इंटरैक्ट कर सकते हैं जैसे कि यह क्लाइंट-साइड चल रही हो।

```python
from langserve import RemoteRunnable

remote_chain = RemoteRunnable("http://localhost:8000/agent/")
remote_chain.invoke({
    "input": "how can langsmith help with testing?",
    "chat_history": []  # Providing an empty list as this is the first call
})
```

LangServe की कई अन्य विशेषताओं के बारे में जानने के लिए [यहां जाएं](/docs/langserve)।

## अगले कदम

हमने LangChain के साथ एक आवेदन बनाने, LangSmith के साथ इसे ट्रेस करने, और LangServe के साथ इसे सर्व करने के तरीके पर चर्चा की है।
इन तीनों में और भी कई सुविधाएँ हैं जिन्हें हम यहाँ कवर नहीं कर सकते।
आपकी यात्रा को जारी रखने के लिए, हम अनुशंसा करते हैं कि आप निम्नलिखित को (क्रम में) पढ़ें:

- इन सभी सुविधाओं का समर्थन [LangChain Expression Language (LCEL)](/docs/expression_language) द्वारा किया गया है - इन घटकों को एक साथ जोड़ने का एक तरीका। कस्टम चेन बनाने को बेहतर ढंग से समझने के लिए उस दस्तावेज़ीकरण को देखें।
- [Model IO](/docs/modules/model_io) प्रॉम्प्ट्स, LLMs, और आउटपुट पार्सर्स के अधिक विवरण को कवर करता है।
- [Retrieval](/docs/modules/data_connection) पुनर्प्राप्ति से संबंधित सब कुछ के अधिक विवरण को कवर करता है।
- [Agents](/docs/modules/agents) एजेंटों से संबंधित सब कुछ का विवरण कवर करता है।
- सामान्य [end-to-end use cases](/docs/use_cases/) और [template applications](/docs/templates) का अन्वेषण करें।
- [LangSmith](/docs/langsmith/) के बारे में पढ़ें, डिबगिंग, परीक्षण, मॉनिटरिंग और अधिक के लिए मंच।
- [LangServe](/docs/langserve) के साथ अपने अनुप्रयोगों को सर्व करने के बारे में अधिक जानें।
