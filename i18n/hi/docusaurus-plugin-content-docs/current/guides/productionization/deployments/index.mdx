---
translated: true
---

#배포

आज के तेज़ी से बदलते प्रौद्योगिकी परिदृश्य में, बड़े भाषा मॉडलों (LLMs) का उपयोग तेजी से बढ़ रहा है। इसके परिणामस्वरूप, डेवलपर्स के लिए ये समझना महत्वपूर्ण है कि उत्पादन वातावरणों में इन मॉडलों को कैसे प्रभावी ढंग से तैनात किया जाए। LLM इंटरफ़ेस आमतौर पर दो श्रेणियों में आते हैं:

- **मामला 1: बाहरी LLM प्रदाताओं (OpenAI, Anthropic आदि) का उपयोग करना**
    इस परिदृश्य में, अधिकांश गणना का भार LLM प्रदाताओं द्वारा संभाला जाता है, जबकि LangChain इन सेवाओं के आसपास व्यावसायिक तर्क को लागू करने को सरल बनाता है। इस アプローチ में प्रॉम्प्ट टेम्पलेटिंग, चैट संदेश जनरेशन, कैशिंग, वेक्टर एम्बेडिंग डेटाबेस निर्माण, पूर्व-प्रसंस्करण आदि जैसी सुविधाएं शामिल हैं।

- **मामला 2: स्वयं होस्ट किए गए ओपन-सोर्स मॉडल**
    वैकल्पिक रूप से, डेवलपर छोटे लेकिन तुलनीय क्षमता वाले स्वयं होस्ट किए गए ओपन-सोर्स LLM मॉडलों का उपयोग करने का विकल्प चुन सकते हैं। यह दृष्टिकोण लागत, लेटेंसी और बाहरी LLM प्रदाताओं को डेटा स्थानांतरित करने से जुड़ी गोपनीयता चिंताओं को काफी कम कर सकता है।

आपके उत्पाद का मूल ढांचा जो भी हो, LLM अनुप्रयोगों को तैनात करना अपने खुद के चुनौतियों के साथ आता है। सेवा फ्रेमवर्क का मूल्यांकन करते समय व्यापक समझ होना महत्वपूर्ण है।

## रूपरेखा

यह गाइड LLMs को उत्पादन सेटिंग में तैनात करने की आवश्यकताओं का व्यापक अवलोकन प्रदान करने का लक्ष्य रखता है, जिसमें निम्नलिखित पर ध्यान केंद्रित है:

- **एक मजबूत LLM अनुप्रयोग सेवा का डिज़ाइन करना**
- **लागत-कुशलता बनाए रखना**
- **तेज़ी से इटरेशन सुनिश्चित करना**

इन घटकों को समझना सेवा प्रणालियों का मूल्यांकन करते समय महत्वपूर्ण है। LangChain इन मुद्दों को संबोधित करने के लिए डिज़ाइन किए गए कई ओपन-सोर्स परियोजनाओं के साथ एकीकृत है, जो आपके LLM अनुप्रयोगों को उत्पादन में लाने के लिए एक मजबूत ढांचा प्रदान करता है। कुछ प्रमुख फ्रेमवर्क हैं:

- [Ray Serve](/docs/integrations/providers/ray_serve)
- [BentoML](https://github.com/bentoml/BentoML)
- [OpenLLM](/docs/integrations/providers/openllm)
- [Modal](/docs/integrations/providers/modal)
- [Jina](/docs/integrations/providers/jina)

ये लिंक प्रत्येक पारिस्थितिकी तंत्र पर अधिक जानकारी प्रदान करेंगे, जो आपके LLM तैनाती आवश्यकताओं के लिए सबसे अच्छा विकल्प ढूंढने में आपकी मदद करेंगे।

## एक मजबूत LLM अनुप्रयोग सेवा का डिज़ाइन करना

उत्पादन में एक LLM सेवा तैनात करते समय, बिना किसी व्यवधान के 24/7 सेवा उपलब्धता प्रदान करना अत्यंत महत्वपूर्ण है। आपके अनुप्रयोग के आसपास कई उप-प्रणालियों को बनाए और बनाए रखने से यह प्राप्त किया जा सकता है।

### मॉनिटरिंग

किसी भी उत्पादन वातावरण में चल रही प्रणाली का एक अभिन्न अंग मॉनिटरिंग है। LLMs के संदर्भ में, प्रदर्शन और गुणवत्ता मेट्रिक्स दोनों की निगरानी करना महत्वपूर्ण है।

**प्रदर्शन मेट्रिक्स:** ये मेट्रिक्स आपके मॉडल की दक्षता और क्षमता के बारे में अंतर्दृष्टि प्रदान करते हैं। यहां कुछ प्रमुख उदाहरण हैं:

- प्रति सेकंड क्वेरी (QPS): यह आपके मॉडल द्वारा प्रोसेस किए जाने वाले क्वेरी की संख्या को मापता है, जो इसके उपयोग के बारे में अंतर्दृष्टि प्रदान करता है।
- लेटेंसी: यह मेट्रिक आपके क्लाइंट द्वारा अनुरोध भेजने से लेकर प्रतिक्रिया प्राप्त करने तक का विलंब को मापता है।
- प्रति सेकंड टोकन (TPS): यह आपके मॉडल द्वारा एक सेकंड में जनरेट किए जा सकने वाले टोकनों की संख्या को दर्शाता है।

**गुणवत्ता मेट्रिक्स:** ये मेट्रिक्स आमतौर पर व्यावसायिक उपयोग-मामले के अनुसार अनुकूलित होते हैं। उदाहरण के लिए, आपके सिस्टम का आउटपुट किसी मानक, जैसे कि पिछले संस्करण, के साथ कैसे तुलना करता है? हालांकि ये मेट्रिक्स ऑफ़लाइन गणना की जा सकती हैं, लेकिन बाद में उपयोग करने के लिए आवश्यक डेटा को लॉग करना होगा।

### दोष सहिष्णुता

आपके अनुप्रयोग को मॉडल अनुमान या व्यावसायिक तर्क कोड में अपवाद जैसी त्रुटियों का सामना करना पड़ सकता है, जिससे विफलताएं होंगी और यातायात में व्यवधान होगा। अन्य संभावित समस्याएं आपके अनुप्रयोग को चलाने वाली मशीन से उत्पन्न हो सकती हैं, जैसे कि अप्रत्याशित हार्डवेयर खराबी या उच्च मांग वाले समयों के दौरान स्पॉट-इंस्टेंस का नुकसान। इन जोखिमों को कम करने का एक तरीका प्रतिकृति पैमाने और विफल प्रतिकृतियों के लिए बहाली तंत्र को लागू करके अधिक संरक्षण प्रदान करना है। हालांकि, मॉडल प्रतिकृतियां विफलता के एकमात्र संभावित बिंदु नहीं हैं। आपकी स्टैक में किसी भी बिंदु पर होने वाली विभिन्न विफलताओं के खिलाफ लचीलापन बनाना महत्वपूर्ण है।

### शून्य डाउनटाइम अपग्रेड

सिस्टम अपग्रेड अक्सर आवश्यक होते हैं, लेकिन सही ढंग से नहीं संभाले जाने पर सेवा व्यवधान का कारण बन सकते हैं। डाउनटाइम को रोकने का एक तरीका नए और पुराने संस्करण के बीच एक सुचारु संक्रमण प्रक्रिया लागू करना है। आदर्श रूप से, आपके LLM सेवा का नया संस्करण तैनात किया जाता है और पुराने से नए संस्करण पर यातायात धीरे-धीरे स्थानांतरित होता है, जिससे पूरी प्रक्रिया के दौरान स्थिर QPS बनी रहती है।

### लोड बैलेंसिंग

लोड बैलेंसिंग, सरल शब्दों में, एक तकनीक है जिसके द्वारा कई कंप्यूटरों, सर्वरों या अन्य संसाधनों पर कार्य को समान रूप से वितरित किया जाता है ताकि सिस्टम का अनुकूलन किया जा सके, अधिकतम थ्रूपुट प्राप्त किया जा सके, प्रतिक्रिया समय को कम किया जा सके और किसी भी एकल संसाधन पर ओवरलोड को टाला जा सके। इसे एक यातायात अधिकारी के रूप में सोचें जो कारों (अनुरोधों) को विभिन्न सड़कों (सर्वरों) पर निर्देशित करता है ताकि कोई भी सड़क अत्यधिक भीड़भाड़ वाली न हो।

लोड बैलेंसिंग के कई रणनीतियां हैं। उदाहरण के लिए, एक आम तरीका *राउंड रॉबिन* रणनीति है, जहां प्रत्येक अनुरोध अगले सर्वर को भेजा जाता है और जब सभी सर्वरों को एक अनुरोध मिल जाता है तो वापस पहले सर्वर पर आता है। यह तब अच्छा काम करता है जब सभी सर्वर समान रूप से सक्षम हों। हालांकि, यदि कुछ सर्वर अन्य सर्वरों से अधिक शक्तिशाली हैं, तो आप *वेटेड राउंड रॉबिन* या *लीस्ट कनेक्शंस* रणनीति का उपयोग कर सकते हैं, जहां अधिक अनुरोध अधिक शक्तिशाली सर्वरों या वर्तमान में सबसे कम सक्रिय अनुरोध संभाल रहे सर्वरों को भेजे जाते हैं। आइए कल्पना करें कि आप एक LLM श्रृंखला चला रहे हैं। यदि आपका अनुप्रयोग लोकप्रिय हो जाता है, तो आप एक साथ सैकड़ों या हजारों उपयोगकर्ताओं से प्रश्न पूछ सकते हैं। यदि एक सर्वर बहुत व्यस्त (उच्च लोड) हो जाता है, तो लोड बैलेंसर नए अनुरोधों को कम व्यस्त किसी अन्य सर्वर पर भेजेगा। इस तरह, आपके सभी उपयोगकर्ता समय पर प्रतिक्रिया प्राप्त करते हैं और प्रणाली स्थिर बनी रहती है।

## लागत-कुशलता और स्केलेबिलिटी बनाए रखना

एलएलएम सेवाओं को तैनात करना महंगा हो सकता है, खासकर जब आप बड़ी संख्या में उपयोगकर्ता इंटरैक्शन को संभाल रहे हों। एलएलएम प्रदाताओं द्वारा शुल्क आमतौर पर उपयोग किए गए टोकन पर आधारित होता है, जिससे इन मॉडलों पर आधारित एक चैट सिस्टम अनुमान महंगा हो सकता है। हालांकि, इन लागतों को प्रभावित किए बिना सेवा की गुणवत्ता को प्रबंधित करने के कई रणनीतियां हैं।

### स्वयं होस्ट मॉडल

एलएलएम प्रदाताओं पर निर्भरता को संबोधित करने के लिए कई छोटे और ओपन-सोर्स एलएलएम उभर रहे हैं। स्वयं होस्टिंग आपको एलएलएम प्रदाता मॉडल के समान गुणवत्ता को बनाए रखने की अनुमति देती है जबकि लागतों का प्रबंधन करती है। चुनौती अपने स्वयं के मशीनों पर एक विश्वसनीय, उच्च-प्रदर्शन वाले एलएलएम सर्विंग सिस्टम बनाने में है।

### संसाधन प्रबंधन और ऑटो-स्केलिंग

आपके अनुप्रयोग के भीतर की कंप्यूटेशनल तर्कशक्ति को सटीक संसाधन आवंटन की आवश्यकता होती है। उदाहरण के लिए, यदि आपके ट्रैफ़िक का एक हिस्सा OpenAI एंडपॉइंट द्वारा और दूसरा हिस्सा स्वयं होस्ट किए गए मॉडल द्वारा सर्व किया जाता है, तो प्रत्येक के लिए उपयुक्त संसाधन आवंटित करना महत्वपूर्ण है। ऑटो-स्केलिंग - ट्रैफ़िक के आधार पर संसाधन आवंटन को समायोजित करना - आपके अनुप्रयोग को चलाने की लागत को काफी प्रभावित कर सकता है। यह रणनीति लागत और प्रतिक्रियाशीलता के बीच संतुलन बनाने की आवश्यकता है, जिससे न तो संसाधन अधिप्रावधान हो और न ही अनुप्रयोग की प्रतिक्रियाशीलता समझौता किया जाए।

### स्पॉट इंस्टेंस का उपयोग

AWS जैसे प्लेटफ़ॉर्मों पर, स्पॉट इंस्टेंस काफी लागत बचत प्रदान करते हैं, जो आमतौर पर ऑन-डिमांड इंस्टेंस का लगभग एक तिहाई मूल्य होता है। व्यापक है कि उच्च क्रैश दर, प्रभावी उपयोग के लिए एक मजबूत दोष-सहिष्णुता तंत्र की आवश्यकता है।

### स्वतंत्र स्केलिंग

जब आप अपने मॉडल को स्वयं होस्ट करते हैं, तो आपको स्वतंत्र स्केलिंग पर विचार करना चाहिए। उदाहरण के लिए, यदि आपके पास दो अनुवाद मॉडल हैं, एक फ्रेंच के लिए और दूसरा स्पेनिश के लिए, तो आने वाले अनुरोधों के लिए प्रत्येक के लिए अलग-अलग स्केलिंग आवश्यकताएं हो सकती हैं।

### अनुरोधों का बैचिंग

बड़े भाषा मॉडलों के संदर्भ में, अनुरोधों का बैचिंग दक्षता को बढ़ा सकता है क्योंकि यह आपके जीपीयू संसाधनों का बेहतर उपयोग करता है। जीपीयू मूल रूप से समानांतर प्रोसेसर होते हैं, जो एक साथ कई कार्यों को संभालने के लिए डिज़ाइन किए गए हैं। यदि आप मॉडल को व्यक्तिगत अनुरोध भेजते हैं, तो जीपीयू पूरी तरह से उपयोग नहीं किया जा सकता क्योंकि यह केवल एक कार्य पर काम कर रहा है। दूसरी ओर, अनुरोधों को एक साथ बैच करके, आप जीपीयू को एक साथ कई कार्यों पर काम करने की अनुमति दे रहे हैं, जिससे इसका उपयोग अधिकतम हो जाता है और अनुमान गति में सुधार होता है। यह न केवल लागत बचत में मदद करता है, बल्कि आपके एलएलएम सेवा की समग्र लेटेंसी में भी सुधार कर सकता है।

संक्षेप में, अपने एलएलएम सेवाओं को स्केल करते समय लागतों का प्रबंधन करना एक रणनीतिक दृष्टिकोण की मांग करता है। स्वयं होस्ट मॉडल का उपयोग, संसाधनों का प्रभावी प्रबंधन, ऑटो-स्केलिंग का उपयोग, स्पॉट इंस्टेंस का उपयोग, स्वतंत्र रूप से मॉडल स्केलिंग और अनुरोधों का बैचिंग प्रमुख रणनीतियां हैं जिन पर विचार करना चाहिए। Ray Serve और BentoML जैसे ओपन-सोर्स लाइब्रेरी इन जटिलताओं से निपटने के लिए डिज़ाइन की गई हैं।

## तेजी से इटरेशन सुनिश्चित करना

एलएलएम परिदृश्य अभूतपूर्व गति से विकसित हो रहा है, जहां नए लाइब्रेरी और मॉडल वास्तुकला लगातार पेश किए जा रहे हैं। परिणामस्वरूप, किसी भी विशिष्ट फ्रेमवर्क के लिए एक समाधान से खुद को बांधने से बचना महत्वपूर्ण है। यह विशेष रूप से सर्विंग में प्रासंगिक है, जहां आपके बुनियादी ढांचे में परिवर्तन समय लेने वाले, महंगे और जोखिम भरे हो सकते हैं। एक सामान्य-उद्देश्य, स्केलेबल सर्विंग परत प्रदान करने वाले बुनियादी ढांचे के लिए प्रयास करें जो किसी भी विशिष्ट मशीन लर्निंग लाइब्रेरी या फ्रेमवर्क में बंद नहीं है। यहां कुछ पहलू हैं जहां लचीलापन महत्वपूर्ण भूमिका निभाता है:

### मॉडल संयोजन

LangChain जैसे प्रणालियों को तैनात करना विभिन्न मॉडलों को एक साथ जोड़ने और उन्हें तर्क के माध्यम से जोड़ने की क्षमता की मांग करता है। प्राकृतिक भाषा इनपुट एसक्यूएल क्वेरी इंजन बनाने के उदाहरण को लें। एक एलएलएम को क्वेरी करना और एसक्यूएल कमांड प्राप्त करना प्रणाली का केवल एक हिस्सा है। आपको जुड़े डेटाबेस से मेटाडेटा निकालने, एलएलएम के लिए प्रोम्प्ट का निर्माण करने, एक इंजन पर एसक्यूएल क्वेरी चलाने, क्वेरी चलते समय प्रतिक्रिया एकत्र करने और उपयोगकर्ता को परिणाम प्रस्तुत करने की आवश्यकता है। यह विभिन्न जटिल घटकों को पायथन में बनाकर एक गतिशील श्रृंखला के逻辑ब्लॉक में एकीकृत करने की आवश्यकता को दर्शाता है जिन्हें एक साथ सर्व किया जा सकता है।

## क्लाउड प्रदाता

कई होस्टेड समाधान एक ही क्लाउड प्रदाता तक सीमित हैं, जो आज के बहु-क्लाउड दुनिया में आपके विकल्पों को सीमित कर सकते हैं। यह आपके अन्य बुनियादी ढांचे के घटकों के निर्माण स्थान पर निर्भर कर सकता है, जहां आप अपने चुने हुए क्लाउड प्रदाता से चिपके रहना पसंद कर सकते हैं।

## इंफ्रास्ट्रक्चर एज कोड (IaC)

तेजी से इटरेशन में बुनियादी ढांचे को तेजी से और विश्वसनीय रूप से पुनर्निर्मित करने की क्षमता भी शामिल है। यही वह जगह है जहां इंफ्रास्ट्रक्चर एज कोड (IaC) टूल जैसे Terraform, CloudFormation या Kubernetes YAML फ़ाइलें आती हैं। वे आपको कोड फ़ाइलों में अपने बुनियादी ढांचे को परिभाषित करने की अनुमति देते हैं, जिन्हें संस्करण नियंत्रित किया जा सकता है और तेजी से तैनात किया जा सकता है, जिससे तेजी से और विश्वसनीय इटरेशन को सक्षम बनाया जा सकता है।

## CI/CD

तेजी से गतिशील वातावरण में, CI/CD पाइपलाइनों को लागू करना आपके एलएलएम अनुप्रयोगों की इटरेशन प्रक्रिया को काफी तेज कर सकता है। वे आपके एलएलएम अनुप्रयोगों के परीक्षण और तैनाती को स्वचालित करने में मदद करते हैं, गलतियों के जोखिम को कम करते हैं और तेजी से प्रतिक्रिया और इटरेशन को सक्षम बनाते हैं।
