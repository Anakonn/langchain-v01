---
translated: true
---

import DocCardList from "@theme/DocCardList";

# मूल्यांकन

भाषा मॉडल के साथ अनुप्रयोग बनाना कई गतिशील भागों को शामिल करता है। सबसे महत्वपूर्ण घटकों में से एक यह सुनिश्चित करना है कि आपके मॉडल द्वारा उत्पादित परिणाम विश्वसनीय और व्यापक इनपुट श्रृंखला पर उपयोगी हैं, और वे आपके अनुप्रयोग के अन्य सॉफ़्टवेयर घटकों के साथ अच्छी तरह काम करते हैं। विश्वसनीयता सुनिश्चित करना आमतौर पर अनुप्रयोग डिज़ाइन, परीक्षण और मूल्यांकन, और रनटाइम जांच के किसी भी संयोजन पर निर्भर करता है।

इस खंड में मार्गदर्शिकाएं LangChain द्वारा प्रदान की जाने वाली एपीआई और कार्यक्षमता की समीक्षा करती हैं ताकि आप अपने अनुप्रयोगों का बेहतर मूल्यांकन कर सकें। मूल्यांकन और परीक्षण दोनों LLM अनुप्रयोगों को तैनात करते समय महत्वपूर्ण हैं, क्योंकि उत्पादन वातावरण को दोहराने योग्य और उपयोगी परिणामों की आवश्यकता होती है।

LangChain विविध डेटा पर प्रदर्शन और अखंडता को मापने के लिए कई प्रकार के मूल्यांकनकर्ताओं प्रदान करता है, और हम समुदाय को अन्य उपयोगी मूल्यांकनकर्ताओं को बनाने और साझा करने के लिए प्रोत्साहित करने की उम्मीद करते हैं ताकि सभी को सुधार करने में मदद मिल सके। ये दस्तावेज़ मूल्यांकनकर्ता प्रकारों, उन्हें कैसे उपयोग करें, और वास्तविक दुनिया के परिदृश्यों में उनके उपयोग के कुछ उदाहरण प्रस्तुत करेंगे।

ये अंतर्निहित मूल्यांकनकर्ता [LangSmith](/docs/langsmith) के साथ सुचारू रूप से एकीकृत होते हैं, और आपको ऐसे प्रतिपुष्टि लूप बनाने की अनुमति देते हैं जो समय के साथ आपके अनुप्रयोग को बेहतर बनाते हैं और रिग्रेशन को रोकते हैं।

LangChain में प्रत्येक मूल्यांकनकर्ता प्रकार तैयार-इस्तेमाल के कार्यान्वयन और अपने अद्वितीय आवश्यकताओं के अनुसार अनुकूलन की अनुमति देने वाले एक विस्तारणीय एपीआई के साथ आता है। यहाँ हम जो मूल्यांकनकर्ता प्रकार प्रदान करते हैं उनमें से कुछ हैं:

- [स्ट्रिंग मूल्यांकनकर्ता](/docs/guides/productionization/evaluation/string/): ये मूल्यांकनकर्ता किसी दिए गए इनपुट के लिए अनुमानित स्ट्रिंग का मूल्यांकन करते हैं, आमतौर पर इसे संदर्भ स्ट्रिंग के खिलाफ तुलना करते हुए।
- [ट्रेजेक्टरी मूल्यांकनकर्ता](/docs/guides/productionization/evaluation/trajectory/): ये एजेंट कार्रवाई की पूरी ट्रेजेक्टरी का मूल्यांकन करने के लिए उपयोग किए जाते हैं।
- [तुलना मूल्यांकनकर्ता](/docs/guides/productionization/evaluation/comparison/): ये मूल्यांकनकर्ता किसी सामान्य इनपुट पर दो रनों से अनुमानों की तुलना करने के लिए डिज़ाइन किए गए हैं।

ये मूल्यांकनकर्ता विभिन्न पटरियों पर उपयोग किए जा सकते हैं और LangChain लाइब्रेरी में विभिन्न श्रृंखला और LLM कार्यान्वयनों पर लागू किए जा सकते हैं।

हम यह भी साझा करने के लिए मार्गदर्शिकाएं और कुकबुक साझा करने पर काम कर रहे हैं कि वास्तविक दुनिया के पटरियों में इन मूल्यांकनकर्ताओं का उपयोग कैसे किया जाए, जैसे:

- [श्रृंखला तुलनाएं](/docs/guides/productionization/evaluation/examples/comparisons): यह उदाहरण तुलना मूल्यांकनकर्ता का उपयोग करके वरीय आउटपुट का अनुमान लगाता है। यह विभिन्न मॉडल या प्रोम्प्ट्स के बीच कुल वरीयता स्कोर में सांख्यिकीय रूप से महत्वपूर्ण अंतर मापने के तरीकों की समीक्षा करता है।

## LangSmith मूल्यांकन

LangSmith एकीकृत मूल्यांकन और ट्रेसिंग फ्रेमवर्क प्रदान करता है जो आपको रिग्रेशन की जांच करने, सिस्टमों की तुलना करने, और किसी भी त्रुटि और प्रदर्शन समस्याओं को आसानी से पहचानने और ठीक करने में मदद करता है। [LangSmith मूल्यांकन](https://docs.smith.langchain.com/evaluation) और अतिरिक्त [कुकबुक](https://docs.smith.langchain.com/cookbook) पर अधिक विस्तृत जानकारी के लिए देखें।

## LangChain बेंचमार्क

आपके अनुप्रयोग की गुणवत्ता LLM जिसका आप चयन करते हैं और मॉडल संदर्भ प्रदान करने के लिए आप जो प्रोम्प्टिंग और डेटा पुनर्प्राप्ति रणनीतियां उपयोग करते हैं, दोनों पर निर्भर करती है। हमने [LangChain बेंचमार्क](https://langchain-ai.github.io/langchain-benchmarks/) पैकेज में विभिन्न LLM प्रणालियों को कार्यों जैसे निम्नलिखित पर ग्रेड करने के लिए कई बेंचमार्क कार्य प्रकाशित किए हैं:

- एजेंट टूल का उपयोग
- पुनर्प्राप्ति-सुदृढ़ प्रश्न-उत्तर
- संरचित निष्कर्षण

उदाहरणों और लीडरबोर्ड जानकारी के लिए दस्तावेज़ देखें।

## संदर्भ दस्तावेज़

उपलब्ध मूल्यांकनकर्ताओं के बारे में विस्तृत जानकारी के लिए, जिसमें उन्हें कैसे प्रारंभ, कॉन्फ़िगर और अनुकूलित करना है, [संदर्भ दस्तावेज़](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.evaluation) देखें।

<DocCardList />
