---
translated: true
---

# C ट्रांसफॉर्मर्स

यह पृष्ठ LangChain में [C ट्रांसफॉर्मर्स](https://github.com/marella/ctransformers) लाइब्रेरी का उपयोग करने के बारे में कवर करता है।
यह दो भागों में विभाजित है: स्थापना और सेटअप, और फिर विशिष्ट C ट्रांसफॉर्मर्स रैपर्स के संदर्भ।

## स्थापना और सेटअप

- `pip install ctransformers` के साथ Python पैकेज स्थापित करें
- समर्थित [GGML मॉडल](https://huggingface.co/TheBloke) डाउनलोड करें (देखें [समर्थित मॉडल](https://github.com/marella/ctransformers#supported-models)))

## रैपर्स

### LLM

CTransformers LLM रैपर मौजूद है, जिसका उपयोग आप इस प्रकार कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "CTransformers", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ctransformers.CTransformers.html", "title": "C Transformers"}]-->
from langchain_community.llms import CTransformers
```

यह सभी मॉडलों के लिए एकीकृत इंटरफ़ेस प्रदान करता है:

```python
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2')

print(llm.invoke('AI is going to'))
```

यदि आप `illegal instruction` त्रुटि प्राप्त कर रहे हैं, तो `lib='avx'` या `lib='basic'` का उपयोग करने का प्रयास करें:

```py
llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx')
```

इसका उपयोग Hugging Face Hub पर होस्ट किए गए मॉडलों के साथ किया जा सकता है:

```py
llm = CTransformers(model='marella/gpt-2-ggml')
```

यदि किसी मॉडल रिपॉजिटरी में कई मॉडल फ़ाइलें (`.bin` फ़ाइलें) हैं, तो निम्नलिखित का उपयोग करके मॉडल फ़ाइल निर्दिष्ट करें:

```py
llm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin')
```

अतिरिक्त पैरामीटर `config` पैरामीटर का उपयोग करके पास किए जा सकते हैं:

```py
config = {'max_new_tokens': 256, 'repetition_penalty': 1.1}

llm = CTransformers(model='marella/gpt-2-ggml', config=config)
```

उपलब्ध पैरामीटरों की सूची के लिए [प्रलेखन](https://github.com/marella/ctransformers#config) देखें।

इसके बारे में अधिक विस्तृत वॉकथ्रू के लिए, [यह नोटबुक](/docs/integrations/llms/ctransformers) देखें।
