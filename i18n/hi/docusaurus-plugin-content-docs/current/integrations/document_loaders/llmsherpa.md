---
translated: true
---

# एलएलएम शेर्पा

यह नोटबुक कवर करता है कि `एलएलएम शेर्पा` को कैसे उपयोग करें फ़ाइलों के कई प्रकारों को लोड करने के लिए। `एलएलएम शेर्पा` DOCX, PPTX, HTML, TXT और XML सहित विभिन्न फ़ाइल प्रारूपों का समर्थन करता है।

`एलएलएमशेर्पाफ़ाइललोडर` LayoutPDFReader का उपयोग करता है, जो एलएलएमशेर्पा लाइब्रेरी का हिस्सा है। यह उपकरण पीडीएफ को पार्स करने के लिए डिज़ाइन किया गया है और उनकी लेआउट जानकारी को बरकरार रखता है, जो अक्सर अधिकांश पीडीएफ से पाठ पार्सर का उपयोग करते समय खो जाती है।

LayoutPDFReader की कुछ प्रमुख सुविधाएं हैं:

* यह अनुभागों और उप-अनुभागों को पहचान और निकाल सकता है, साथ ही उनके स्तर भी।
* यह पंक्तियों को मिलाकर अनुच्छेद बनाता है।
* यह अनुभागों और अनुच्छेदों के बीच लिंक पहचान और निकाल सकता है।
* यह तालिकाओं को निकाल सकता है, साथ ही उस अनुभाग को भी जिसमें तालिकाएं पाई जाती हैं।
* यह सूचियों और संरचित सूचियों को पहचान और निकाल सकता है।
* यह पृष्ठों पर फैले सामग्री को जोड़ सकता है।
* यह बार-बार आने वाले हेडर और फुटर को हटा सकता है।
* यह वॉटरमार्क को हटा सकता है।

[एलएलएमशेर्पा](https://llmsherpa.readthedocs.io/en/latest/) प्रलेखन देखें।

`सूचना: यह लाइब्रेरी कुछ पीडीएफ फ़ाइलों के साथ विफल हो जाती है, इसलिए इसका सावधानी से उपयोग करें।`

```python
# Install package
# !pip install --upgrade --quiet llmsherpa
```

## एलएलएमशेर्पाफ़ाइललोडर

एलएलएमशेर्पाफ़ाइललोडर के तहत कुछ रणनीतियां फ़ाइल सामग्री लोड करने के लिए परिभाषित हैं: ["sections", "chunks", "html", "text"], [एनएलएम-इंजेस्टर](https://github.com/nlmatics/nlm-ingestor) सेट करें `llmsherpa_api_url` प्राप्त करने या डिफ़ॉल्ट का उपयोग करने के लिए।

### अनुभाग रणनीति: फ़ाइल को अनुभागों में पार्स करके लौटाता है

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="sections",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[1]
```

```output
Document(page_content='Abstract\nWe study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.\nThis underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing.\nWe propose STORM, a writing system for the Synthesis of Topic Outlines through\nReferences\nFull-length Article\nTopic\nOutline\n2022 Winter Olympics\nOpening Ceremony\nResearch via Question Asking\nRetrieval and Multi-perspective Question Asking.\nSTORM models the pre-writing stage by\nLLM\n(1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.\nFor evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage.\nWe further gather feedback from experienced Wikipedia editors.\nCompared to articles generated by an outlinedriven retrieval-augmented baseline, more of STORM’s articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%).\nThe expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.\n1. Can you provide any information about the transportation arrangements for the opening ceremony?\nLLM\n2. Can you provide any information about the budget for the 2022 Winter Olympics opening ceremony?…\nLLM- Role1\nLLM- Role2\nLLM- Role1', metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 1, 'section_title': 'Abstract'})
```

```python
len(docs)
```

```output
79
```

### टुकड़ा रणनीति: फ़ाइल को टुकड़ों में पार्स करके लौटाता है

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="chunks",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[1]
```

```output
Document(page_content='Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\nStanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu', metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 1, 'chunk_type': 'para'})
```

```python
len(docs)
```

```output
306
```

### एचटीएमएल रणनीति: फ़ाइल को एक एचटीएमएल दस्तावेज़ के रूप में लौटाता है

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="html",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[0].page_content[:400]
```

```output
'<html><h1>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</h1><table><th><td colSpan=1>Yijia Shao</td><td colSpan=1>Yucheng Jiang</td><td colSpan=1>Theodore A. Kanell</td><td colSpan=1>Peter Xu</td></th><tr><td colSpan=1></td><td colSpan=1>Omar Khattab</td><td colSpan=1>Monica S. Lam</td><td colSpan=1></td></tr></table><p>Stanford University {shaoyj, yuchengj, '
```

```python
len(docs)
```

```output
1
```

### पाठ रणनीति: फ़ाइल को एक पाठ दस्तावेज़ के रूप में लौटाता है

```python
from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader

loader = LLMSherpaFileLoader(
    file_path="https://arxiv.org/pdf/2402.14207.pdf",
    new_indent_parser=True,
    apply_ocr=True,
    strategy="text",
    llmsherpa_api_url="http://localhost:5010/api/parseDocument?renderFormat=all",
)
docs = loader.load()
```

```python
docs[0].page_content[:400]
```

```output
'Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\n | Yijia Shao | Yucheng Jiang | Theodore A. Kanell | Peter Xu\n | --- | --- | --- | ---\n |  | Omar Khattab | Monica S. Lam | \n\nStanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\nAbstract\nWe study how to apply large language models to write grounded and organized long'
```

```python
len(docs)
```

```output
1
```
