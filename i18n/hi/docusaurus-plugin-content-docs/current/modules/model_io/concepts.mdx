---
sidebar_class_name: hidden
sidebar_position: 1
translated: true
---

# अवधारणाएं

किसी भी भाषा मॉडल अनुप्रयोग का मूल तत्व है... मॉडल। LangChain आपको किसी भी भाषा मॉडल के साथ काम करने के लिए आवश्यक ब्लॉक प्रदान करता है। इस खंड में सब कुछ मॉडल के साथ काम करना आसान बनाने के बारे में है। इसमें मॉडल क्या है, मॉडल के इनपुट को बनाने में मदद करने वाले उपकरण, और मॉडल के आउटपुट के साथ काम करने में मदद करने वाले उपकरण के लिए एक स्पष्ट इंटरफ़ेस शामिल है।

## मॉडल

LangChain द्वारा एकीकृत दो प्रमुख प्रकार के मॉडल हैं: LLM और चैट मॉडल। ये उनके इनपुट और आउटपुट प्रकारों द्वारा परिभाषित होते हैं।

### LLM

LangChain में LLM का मतलब है शुद्ध पाठ पूर्ण मॉडल।
उनके द्वारा लिपटे API एक स्ट्रिंग प्रोम्प्ट को इनपुट के रूप में लेते हैं और एक स्ट्रिंग पूर्ण करण को आउटपुट करते हैं। OpenAI का GPT-3 एक LLM के रूप में कार्यान्वित है।

### चैट मॉडल

चैट मॉडल अक्सर LLM द्वारा समर्थित होते हैं लेकिन विशेष रूप से वार्तालाप के लिए अनुकूलित होते हैं।
महत्वपूर्ण बात यह है कि उनके प्रदाता API शुद्ध पाठ पूर्ण मॉडल से अलग इंटरफ़ेस का उपयोग करते हैं। एक स्ट्रिंग के बजाय, वे चैट संदेशों की एक सूची को इनपुट के रूप में लेते हैं और वे एक AI संदेश को आउटपुट करते हैं। एक संदेश की सटीक क्या है, इस बारे में अधिक जानकारी के लिए नीचे दिए गए खंड देखें। GPT-4 और Anthropic का Claude-2 दोनों चैट मॉडल के रूप में कार्यान्वित हैं।

### विचारणीय बिंदु

इन दो API प्रकारों में काफी अलग इनपुट और आउटपुट स्कीमा होती है। इसका मतलब है कि उनके साथ काम करने का सबसे अच्छा तरीका काफी अलग हो सकता है। हालांकि LangChain इन्हें आपस में बदलने योग्य बनाता है, लेकिन यह नहीं मतलब है कि आप **ऐसा** करना चाहिए। विशेष रूप से, LLM बनाम ChatModels के लिए प्रोम्प्टिंग रणनीतियां काफी अलग हो सकती हैं। इसका मतलब है कि आपको सुनिश्चित करना होगा कि आप जिस मॉडल प्रकार के साथ काम कर रहे हैं, उसके लिए आप जो प्रोम्प्ट का उपयोग कर रहे हैं वह उपयुक्त है।

इसके अलावा, सभी मॉडल एक समान नहीं हैं। विभिन्न मॉडलों के लिए अलग-अलग प्रोम्प्टिंग रणनीतियां सबसे अच्छी काम करती हैं। उदाहरण के लिए, Anthropic के मॉडल XML के साथ सबसे अच्छे काम करते हैं, जबकि OpenAI के मॉडल JSON के साथ सबसे अच्छे काम करते हैं। इसका मतलब है कि एक मॉडल के लिए आप जो प्रोम्प्ट का उपयोग कर रहे हैं, वह दूसरे मॉडल के लिए काम नहीं कर सकता। LangChain कई डिफ़ॉल्ट प्रोम्प्ट प्रदान करता है, लेकिन इन पर यह गारंटी नहीं है कि आप उपयोग कर रहे मॉडल के साथ अच्छी तरह से काम करेंगे। इतिहास में, अधिकांश प्रोम्प्ट OpenAI के साथ अच्छी तरह से काम करते हैं, लेकिन अन्य मॉडलों पर अधिक परीक्षण नहीं किए गए हैं। यह कुछ है जिस पर हम काम कर रहे हैं, लेकिन यह कुछ है जिसे आपको ध्यान में रखना चाहिए।

## संदेश

ChatModels संदेशों की एक सूची को इनपुट के रूप में लेते हैं और एक संदेश को आउटपुट करते हैं। संदेशों के कुछ अलग-अलग प्रकार हैं। सभी संदेशों में एक `role` और एक `content` गुण होता है। `role` बताता है कि यह संदेश किसके द्वारा कहा जा रहा है। LangChain में विभिन्न भूमिकाओं के लिए अलग-अलग संदेश वर्ग हैं। `content` गुण संदेश के विषय-वस्तु का वर्णन करता है। यह कुछ अलग-अलग चीजें हो सकती हैं:

- एक स्ट्रिंग (अधिकांश मॉडल इस तरह के हैं)
- एक डिक्शनरियों की सूची (यह बहु-मीडिया इनपुट के लिए उपयोग किया जाता है, जहां डिक्शनरी में उस इनपुट प्रकार और उस इनपुट स्थान के बारे में जानकारी होती है)

इसके अलावा, संदेशों में एक `additional_kwargs` गुण होता है। यह वह जगह है जहां संदेशों के बारे में अतिरिक्त जानकारी पारित की जा सकती है। इसका मुख्य उपयोग *प्रदाता विशिष्ट* इनपुट मापदंडों के लिए है, जो सामान्य नहीं हैं। इसका सबसे प्रसिद्ध उदाहरण OpenAI से `function_call` है।

### HumanMessage

यह उपयोगकर्ता से आने वाले संदेश का प्रतिनिधित्व करता है। आमतौर पर केवल विषय-वस्तु से बना होता है।

### AIMessage

यह मॉडल से आने वाले संदेश का प्रतिनिधित्व करता है। इसमें `additional_kwargs` हो सकते हैं - उदाहरण के लिए OpenAI फ़ंक्शन कॉलिंग का `functional_call`।

### SystemMessage

यह एक सिस्टम संदेश का प्रतिनिधित्व करता है। केवल कुछ मॉडल इसका समर्थन करते हैं। यह मॉडल को यह बताता है कि उसे कैसे व्यवहार करना चाहिए। यह आमतौर पर केवल विषय-वस्तु से बना होता है।

### FunctionMessage

यह एक फ़ंक्शन कॉल के परिणाम का प्रतिनिधित्व करता है। `role` और `content` के अलावा, इस संदेश में एक `name` मापदंड होता है जो उस फ़ंक्शन का नाम बताता है जिसे इस परिणाम को उत्पन्न करने के लिए कॉल किया गया था।

### ToolMessage

यह एक टूल कॉल के परिणाम का प्रतिनिधित्व करता है। यह OpenAI के `function` और `tool` संदेश प्रकारों को मिलान करने के लिए FunctionMessage से अलग है। `role` और `content` के अलावा, इस संदेश में एक `tool_call_id` मापदंड होता है जो उस कॉल की पहचान करता है जिसे इस परिणाम को उत्पन्न करने के लिए टूल पर किया गया था।

## प्रोम्प्ट

भाषा मॉडलों के इनपुट अक्सर प्रोम्प्ट कहे जाते हैं। अक्सर, आपके ऐप से उपयोगकर्ता इनपुट सीधे मॉडल के इनपुट नहीं होता है। बजाय इसके, उनका इनपुट किसी तरह से परिवर्तित किया जाता है ताकि अंतिम स्ट्रिंग या संदेश बन सके जो मॉडल में जाता है। उन वस्तुओं को जो उपयोगकर्ता इनपुट को लेकर उसे अंतिम स्ट्रिंग या संदेशों में बदलते हैं, "प्रोम्प्ट टेम्प्लेट" कहा जाता है। प्रोम्प्ट के साथ काम करना आसान बनाने के लिए LangChain कई अमूर्त अवधारणाएं प्रदान करता है।

### PromptValue

ChatModels और LLM अलग-अलग इनपुट प्रकार लेते हैं। PromptValue एक ऐसा वर्ग है जो दोनों के बीच अंतर-संचालनीय होने के लिए डिज़ाइन किया गया है। यह एक स्ट्रिंग में कास्ट करने (LLM के साथ काम करने के लिए) और संदेशों की एक सूची में कास्ट करने (ChatModels के साथ काम करने के लिए) का एक तरीका प्रदान करता है।

### PromptTemplate

[यह](/docs/modules/model_io/prompts/quick_start#prompttemplate) एक प्रोम्प्ट टेम्प्लेट का उदाहरण है। इसमें एक टेम्प्लेट स्ट्रिंग होती है। यह स्ट्रिंग फिर उपयोगकर्ता इनपुट के साथ स्वरूपित की जाती है ताकि एक अंतिम स्ट्रिंग बन सके।

### MessagePromptTemplate

इस प्रकार का टेम्प्लेट एक टेम्प्लेट **संदेश** से बना होता है - मतलब एक विशिष्ट भूमिका और एक PromptTemplate। यह PromptTemplate फिर उपयोगकर्ता इनपुट के साथ स्वरूपित की जाती है ताकि एक अंतिम स्ट्रिंग बन सके जो इस संदेश का `content` बन जाता है।

#### HumanMessagePromptTemplate

यह एक ऐसा MessagePromptTemplate है जो एक HumanMessage उत्पन्न करता है।

#### AIMessagePromptTemplate

यह एक ऐसा MessagePromptTemplate है जो एक AIMessage उत्पन्न करता है।

#### SystemMessagePromptTemplate

यह एक ऐसा MessagePromptTemplate है जो एक SystemMessage उत्पन्न करता है।

### MessagesPlaceholder

अक्सर प्रॉम्प्ट के इनपुट एक संदेशों की सूची हो सकते हैं। यह वह समय है जब आप एक MessagesPlaceholder का उपयोग करेंगे। इन वस्तुओं को एक `variable_name` तर्क द्वारा परैमीटरीकृत किया जाता है। इसी `variable_name` मान के साथ इनपुट होना चाहिए कि संदेशों की एक सूची।

### ChatPromptTemplate

[यह](/docs/modules/model_io/prompts/quick_start#chatprompttemplate) एक प्रॉम्प्ट टेम्प्लेट का उदाहरण है। यह MessagePromptTemplates या MessagePlaceholders की एक सूची से बना है। फिर उपयोगकर्ता इनपुट के साथ प्रारूपित किया जाता है ताकि अंतिम संदेशों की सूची तैयार हो।

## Output Parsers

मॉडलों का आउटपुट या तो स्ट्रिंग या संदेश होता है। अक्सर, स्ट्रिंग या संदेश में एक विशिष्ट प्रारूप में प्रारूपित जानकारी होती है ताकि इसका उपयोग आगे किया जा सके (उदाहरण के लिए, कॉमा से अलग सूची या JSON ब्लॉब)। आउटपुट पार्सर्स मॉडल के आउटपुट को लेकर उसे अधिक उपयोगी रूप में रूपांतरित करने के लिए जिम्मेदार हैं। ये आमतौर पर आउटपुट संदेश के `content` पर काम करते हैं, लेकिन कभी-कभी `additional_kwargs` फ़ील्ड के मानों पर भी काम करते हैं।

### StrOutputParser

यह एक सरल आउटपुट पार्सर है जो बस भाषा मॉडल (LLM या ChatModel) के आउटपुट को स्ट्रिंग में रूपांतरित करता है। यदि मॉडल एक LLM है (और इसलिए स्ट्रिंग आउटपुट करता है) तो वह उस स्ट्रिंग को पास करता है। यदि आउटपुट एक ChatModel है (और इसलिए संदेश आउटपुट करता है) तो वह संदेश के `.content` गुण को पास करता है।

### OpenAI Functions Parsers

OpenAI फ़ंक्शन कॉलिंग के साथ काम करने के लिए कुछ पार्सर हैं। वे `function_call` और `arguments` पैरामीटर (जो `additional_kwargs` के अंदर हैं) का आउटपुट लेते हैं और उनके साथ काम करते हैं, लगभग content को नजरअंदाज करते हुए।

### Agent Output Parsers

[एजेंट](/docs/modules/agents/) ऐसे सिस्टम हैं जो भाषा मॉडल का उपयोग करके यह निर्धारित करते हैं कि क्या कदम उठाने हैं। भाषा मॉडल का आउटपुट इसलिए किसी ऐसे स्कीमा में पार्स किया जाना चाहिए जो किए जाने वाले कार्रवाई (यदि कोई हो) का प्रतिनिधित्व कर सके। AgentOutputParsers कच्चे LLM या ChatModel आउटपुट को लेकर उस स्कीमा में रूपांतरित करने के लिए जिम्मेदार हैं। इन आउटपुट पार्सर्स के अंदर का तर्क उस मॉडल और प्रॉम्प्टिंग रणनीति पर निर्भर करता है जिसका उपयोग किया जा रहा है।
