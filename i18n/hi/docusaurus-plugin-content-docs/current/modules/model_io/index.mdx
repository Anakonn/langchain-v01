---
sidebar_class_name: hidden
sidebar_custom_props:
  description: भाषा मॉडल से संवाद करना
sidebar_position: 0
translated: true
---

# मॉडल इनपुट/आउटपुट

किसी भी भाषा मॉडल एप्लिकेशन का मूल तत्व... मॉडल है। LangChain आपको किसी भी भाषा मॉडल से संवाद करने के लिए आवश्यक घटकों को प्रदान करता है।

![मॉडल इनपुट/आउटपुट प्रक्रिया को दर्शाने वाला प्रवाह चार्ट, जिसमें प्रारूप, पूर्वानुमान और पार्स जैसे चरण दिखाए गए हैं, जो इनपुट चर से संरचित आउटपुट में रूपांतरण दर्शाते हैं।](/img/model_io.jpg "मॉडल इनपुट/आउटपुट प्रक्रिया का आरेख")

# त्वरित शुरुआत

नीचे दी गई त्वरित शुरुआत LangChain के मॉडल इनपुट/आउटपुट घटकों का उपयोग करने के मूलभूत पहलुओं को कवर करेगी। यह दो अलग-अलग प्रकार के मॉडल - LLM और चैट मॉडल को परिचित कराएगा। इसके बाद यह इन मॉडलों के इनपुट को प्रारूपित करने के लिए प्रॉम्प्ट टेम्प्लेट और आउटपुट को काम में लाने के लिए आउटपुट पार्सर का उपयोग करना कवर करेगा।

LangChain में भाषा मॉडल दो प्रकार के होते हैं:

### [चैट मॉडल](/docs/modules/model_io/chat/)

[चैट मॉडल](/docs/modules/model_io/chat/) अक्सर LLM द्वारा समर्थित होते हैं लेकिन विशेष रूप से संवाद करने के लिए अनुकूलित होते हैं।
महत्वपूर्ण बात यह है कि उनके प्रदाता API का इंटरफ़ेस शुद्ध पाठ पूर्णता मॉडलों से अलग होता है। एक एकल स्ट्रिंग के बजाय, वे इनपुट के रूप में चैट संदेशों की एक सूची लेते हैं और वे एक AI संदेश के रूप में आउटपुट देते हैं। संदेश की सटीक प्रकृति के बारे में अधिक जानकारी के लिए नीचे दिए गए खंड देखें। GPT-4 और Anthropic के Claude-2 दोनों को चैट मॉडल के रूप में कार्यान्वित किया गया है।

### [LLMs](/docs/modules/model_io/llms/)

[LLMs](/docs/modules/model_io/llms/) में LangChain में पूर्ण पाठ पूर्णता मॉडल शामिल हैं।
उनके द्वारा लिपटे API एक स्ट्रिंग प्रॉम्प्ट को इनपुट के रूप में लेते हैं और स्ट्रिंग पूर्णता को आउटपुट करते हैं। OpenAI का GPT-3 एक LLM के रूप में कार्यान्वित है।

इन दो API प्रकारों में अलग-अलग इनपुट और आउटपुट स्कीमा होती हैं।

इसके अलावा, सभी मॉडल एक समान नहीं हैं। विभिन्न मॉडलों में अलग-अलग प्रॉम्प्टिंग रणनीतियां होती हैं जो उनके लिए सबसे अच्छी काम करती हैं। उदाहरण के लिए, Anthropic के मॉडल XML के साथ सबसे अच्छे काम करते हैं, जबकि OpenAI के मॉडल JSON के साथ सबसे अच्छे काम करते हैं। आप अपने ऐप्स को डिज़ाइन करते समय इस बात का ध्यान रखना चाहिए।

इस शुरुआती गाइड के लिए, हम चैट मॉडल का उपयोग करेंगे और कुछ विकल्प प्रदान करेंगे: Anthropic या OpenAI जैसे API का उपयोग करना, या Ollama के माध्यम से स्थानीय ओपन सोर्स मॉडल का उपयोग करना।

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="openai" label="OpenAI" default>

सबसे पहले हमें उनके साझेदार पैकेज को स्थापित करना होगा:

```shell
pip install langchain-openai
```

API का उपयोग करने के लिए एक API कुंजी की आवश्यकता होती है, जिसे आप एक खाता बनाकर और [यहाँ](https://platform.openai.com/account/api-keys) जाकर प्राप्त कर सकते हैं। एक बार जब हमारे पास कुंजी हो जाए तो हम इसे एक पर्यावरण चर के रूप में सेट करना चाहेंगे:

```shell
export OPENAI_API_KEY="..."
```

फिर हम मॉडल को प्रारंभ कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Model I/O"}, {"imported": "OpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html", "title": "Model I/O"}]-->
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI

llm = OpenAI()
chat_model = ChatOpenAI(model="gpt-3.5-turbo-0125")
```

यदि आप पर्यावरण चर सेट करना नहीं चाहते हैं, तो आप OpenAI LLM वर्ग को प्रारंभ करते समय `api_key` नामित पैरामीटर के माध्यम से कुंजी को सीधे पास कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Model I/O"}]-->
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(api_key="...")
```

`llm` और `chat_model` दोनों ही किसी विशिष्ट मॉडल के कॉन्फ़िगरेशन को प्रतिनिधित्व करने वाले ऑब्जेक्ट हैं।
आप उन्हें `temperature` और अन्य जैसे पैरामीटरों के साथ प्रारंभ कर सकते हैं, और उन्हें आगे भेज सकते हैं।
उनके बीच मुख्य अंतर इनपुट और आउटपुट स्कीमा में है।
LLM ऑब्जेक्ट स्ट्रिंग को इनपुट और आउटपुट के रूप में लेते हैं।
ChatModel ऑब्जेक्ट संदेशों की एक सूची को इनपुट के रूप में लेते हैं और एक संदेश को आउटपुट करते हैं।

हम LLM और ChatModel के बीच के अंतर को देख सकते हैं जब हम इसे कॉल करते हैं।

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Model I/O"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

LLM एक स्ट्रिंग लौटाता है, जबकि ChatModel एक संदेश लौटाता है।

  </TabItem>
  <TabItem value="local" label="स्थानीय (Ollama का उपयोग करके)">

[Ollama](https://ollama.ai/) आपको स्थानीय रूप से ओपन सोर्स बड़े भाषा मॉडल, जैसे Llama 2, चलाने की अनुमति देता है।

पहले, [इन निर्देशों](https://github.com/jmorganca/ollama) का पालन करें ताकि एक स्थानीय Ollama इंस्टेंस सेट अप और चलाया जा सके:

* [डाउनलोड](https://ollama.ai/download)
* `ollama pull llama2` के माध्यम से एक मॉडल प्राप्त करें

फिर, यह सुनिश्चित करें कि Ollama सर्वर चल रहा है। इसके बाद, आप यह कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "Ollama", "source": "langchain_community.llms", "docs": "https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html", "title": "Model I/O"}, {"imported": "ChatOllama", "source": "langchain_community.chat_models", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html", "title": "Model I/O"}]-->
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

llm = Ollama(model="llama2")
chat_model = ChatOllama()
```

`llm` और `chat_model` दोनों ही किसी विशिष्ट मॉडल के कॉन्फ़िगरेशन को प्रतिनिधित्व करने वाले ऑब्जेक्ट हैं।
आप उन्हें `temperature` और अन्य जैसे पैरामीटरों के साथ प्रारंभ कर सकते हैं, और उन्हें आगे भेज सकते हैं।
उनके बीच मुख्य अंतर इनपुट और आउटपुट स्कीमा में है।
LLM ऑब्जेक्ट स्ट्रिंग को इनपुट और आउटपुट के रूप में लेते हैं।
ChatModel ऑब्जेक्ट संदेशों की एक सूची को इनपुट के रूप में लेते हैं और एक संदेश को आउटपुट करते हैं।

हम LLM और ChatModel के बीच के अंतर को देख सकते हैं जब हम इसे कॉल करते हैं।

```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html", "title": "Model I/O"}]-->
from langchain_core.messages import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

LLM एक स्ट्रिंग लौटाता है, जबकि ChatModel एक संदेश लौटाता है।

  </TabItem>
  <TabItem value="anthropic" label="Anthropic (केवल चैट मॉडल)">

सबसे पहले हमें LangChain x Anthropic पैकेज को आयात करना होगा।

```shell
pip install langchain-anthropic
```

API का उपयोग करने के लिए एक API कुंजी की आवश्यकता होती है, जिसे आप [यहाँ](https://claude.ai/login) एक खाता बनाकर प्राप्त कर सकते हैं। एक बार जब हमारे पास कुंजी हो जाए तो हम इसे एक पर्यावरण चर के रूप में सेट करना चाहेंगे:

```shell
export ANTHROPIC_API_KEY="..."
```

फिर हम मॉडल को प्रारंभ कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "Model I/O"}]-->
from langchain_anthropic import ChatAnthropic

chat_model = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0.2, max_tokens=1024)
```

यदि आप पर्यावरण चर सेट करना नहीं चाहते हैं, तो आप Anthropic Chat Model वर्ग को प्रारंभ करते समय `api_key` नामित पैरामीटर के माध्यम से कुंजी को सीधे पास कर सकते हैं:

```python
chat_model = ChatAnthropic(api_key="...")
```

  </TabItem>
  <TabItem value="cohere" label="Cohere (केवल चैट मॉडल)">

सबसे पहले हमें उनके साझेदार पैकेज को स्थापित करना होगा:

```shell
pip install langchain-cohere
```

API का उपयोग करने के लिए एक API कुंजी की आवश्यकता होती है, जिसे आप एक खाता बनाकर और [यहाँ](https://dashboard.cohere.com/api-keys) जाकर प्राप्त कर सकते हैं। एक बार जब हमारे पास कुंजी हो जाए तो हम इसे एक पर्यावरण चर के रूप में सेट करना चाहेंगे:

```shell
export COHERE_API_KEY="..."
```

फिर हम मॉडल को प्रारंभ कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Model I/O"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere()
```

यदि आप पर्यावरण चर सेट करना नहीं चाहते हैं, तो आप Cohere LLM वर्ग को प्रारंभ करते समय `cohere_api_key` नामित पैरामीटर के माध्यम से कुंजी को सीधे पास कर सकते हैं:

```python
<!--IMPORTS:[{"imported": "ChatCohere", "source": "langchain_cohere", "docs": "https://api.python.langchain.com/en/latest/chat_models/langchain_cohere.chat_models.ChatCohere.html", "title": "Model I/O"}]-->
from langchain_cohere import ChatCohere

chat_model = ChatCohere(cohere_api_key="...")
```

  </TabItem>
</Tabs>

## प्रॉम्प्ट टेम्पलेट्स

अधिकांश एलएलएम अनुप्रयोग उपयोगकर्ता इनपुट को सीधे एलएलएम में पास नहीं करते। आमतौर पर वे उपयोगकर्ता इनपुट को एक बड़े टुकड़े के साथ जोड़ेंगे, जिसे प्रॉम्प्ट टेम्पलेट कहा जाता है, जो विशिष्ट कार्य पर अतिरिक्त संदर्भ प्रदान करता है।

पिछले उदाहरण में, हमने मॉडल को कंपनी का नाम जेनरेट करने के लिए निर्देश दिए थे। हमारे अनुप्रयोग के लिए, यह बहुत अच्छा होगा अगर उपयोगकर्ता केवल कंपनी/उत्पाद का विवरण प्रदान करे, बिना मॉडल को निर्देश देने की चिंता किए।

PromptTemplates इसके लिए बहुत मदद करते हैं!
वे उपयोगकर्ता इनपुट से पूरी तरह से प्रारूपित प्रॉम्प्ट तक जाने के लिए सभी तर्क बंडल करते हैं।
यह बहुत ही सरल हो सकता है - उदाहरण के लिए, उपरोक्त स्ट्रिंग उत्पन्न करने के लिए एक प्रॉम्प्ट केवल यह होगा:

```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "Model I/O"}]-->
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```python
What is a good name for a company that makes colorful socks?
```

कच्चे स्ट्रिंग प्रारूपण के मुकाबले इनका कई फायदे हैं।
आप "आंशिक" रूप से चर को प्रारूपित कर सकते हैं - उदाहरण के लिए, आप केवल कुछ चर को प्रारूपित कर सकते हैं।
आप उन्हें एक साथ जोड़ सकते हैं, विभिन्न टेम्पलेट को आसानी से एक प्रॉम्प्ट में जोड़ सकते हैं।
इन सुविधाओं के लिए व्याख्या के लिए, [प्रॉम्प्ट्स पर खंड](/docs/modules/model_io/prompts) देखें।

`PromptTemplate`s का उपयोग संदेशों की सूची उत्पन्न करने के लिए भी किया जा सकता है।
इस मामले में, प्रॉम्प्ट न केवल सामग्री के बारे में जानकारी, बल्कि प्रत्येक संदेश (उसकी भूमिका, सूची में उसकी स्थिति आदि) के बारे में भी होता है।
यहां, जो सबसे अक्सर होता है वह यह है कि `ChatPromptTemplate` `ChatMessageTemplates` की एक सूची है।
प्रत्येक `ChatMessageTemplate` उस `ChatMessage` को प्रारूपित करने के लिए निर्देश शामिल करता है - उसकी भूमिका, और फिर उसकी सामग्री भी।
चलो इस पर नज़र डालते हैं:

```python
<!--IMPORTS:[{"imported": "ChatPromptTemplate", "source": "langchain_core.prompts.chat", "docs": "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Model I/O"}]-->
from langchain_core.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```pycon
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

ChatPromptTemplates को अन्य तरीकों से भी बनाया जा सकता है - [प्रॉम्प्ट्स पर खंड](/docs/modules/model_io/prompts) में और अधिक जानकारी देखें।

## आउटपुट पार्सर्स

`OutputParser`s भाषा मॉडल के कच्चे आउटपुट को एक ऐसे प्रारूप में बदलते हैं जिसका उपयोग बाद में किया जा सकता है।
`OutputParser`s के कुछ प्रमुख प्रकार हैं, जिनमें शामिल हैं:

- `LLM` से पाठ को संरचित जानकारी (जैसे JSON) में बदलना
- `ChatMessage` को केवल एक स्ट्रिंग में बदलना
- कॉल से प्राप्त अतिरिक्त जानकारी को (जैसे OpenAI फ़ंक्शन इन्वोकेशन) एक स्ट्रिंग में बदलना।

इस बारे में पूरी जानकारी के लिए, [आउटपुट पार्सर्स पर खंड](/docs/modules/model_io/output_parsers) देखें।

इस शुरुआती गाइड में, हम एक सरल पार्सर का उपयोग करते हैं जो कॉमा से अलग किए गए मूल्यों की सूची पार्स करता है।

```python
<!--IMPORTS:[{"imported": "CommaSeparatedListOutputParser", "source": "langchain.output_parsers", "docs": "https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html", "title": "Model I/O"}]-->
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
output_parser.parse("hi, bye")
# >> ['hi', 'bye']
```

## LCEL के साथ संयोजन

अब हम इन सभी को एक श्रृंखला में जोड़ सकते हैं।
यह श्रृंखला इनपुट चर लेगी, उन्हें प्रॉम्प्ट टेम्पलेट में पास करेगी ताकि एक प्रॉम्प्ट बना सके, प्रॉम्प्ट को भाषा मॉडल में पास करेगी, और फिर (वैकल्पिक) आउटपुट पार्सर से गुजरेगी।
यह एक मॉड्यूलर टुकड़े के लॉजिक को बंडल करने का एक सुविधाजनक तरीका है।
चलो इसे कार्य में देखते हैं!

```python
template = "Generate a list of 5 {text}.\n\n{format_instructions}"

chat_prompt = ChatPromptTemplate.from_template(template)
chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())
chain = chat_prompt | chat_model | output_parser
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

ध्यान दें कि हम इन घटकों को एक साथ जोड़ने के लिए `|` वाक्यविन्यास का उपयोग कर रहे हैं।
यह `|` वाक्यविन्यास LangChain Expression Language (LCEL) द्वारा संचालित होता है और सभी इन वस्तुओं द्वारा लागू किए गए सार्वभौमिक `Runnable` इंटरफ़ेस पर निर्भर करता है।
LCEL के बारे में अधिक जानने के लिए, [यहां](/docs/expression_language) दस्तावेज़ देखें।

## निष्कर्ष

प्रॉम्प्ट, मॉडल और आउटपुट पार्सर के साथ शुरुआत करने के लिए यही था! यह केवल सीखने के लिए सतह को ढका है। अधिक जानकारी के लिए, इन पर जाएं:

- [प्रॉम्प्ट्स खंड](./prompts) प्रॉम्प्ट टेम्पलेट के साथ काम करने के बारे में जानकारी के लिए
- [ChatModel खंड](./chat) ChatModel इंटरफ़ेस के बारे में अधिक जानकारी के लिए
- [LLM खंड](./llms) LLM इंटरफ़ेस के बारे में अधिक जानकारी के लिए
- [आउटपुट पार्सर खंड](./output_parsers) विभिन्न प्रकार के आउटपुट पार्सर के बारे में जानकारी के लिए।
