---
sidebar_class_name: hidden
title: सरंचित आउटपुट निकालना
translated: true
---

## अवलोकन

बड़े भाषा मॉडल (LLMs) सूचना निकालने के अनुप्रयोगों को संचालित करने के लिए एक अत्यंत सक्षम प्रौद्योगिकी के रूप में उभर रहे हैं।

सूचना निकालने के लिए पारंपरिक समाधान लोगों, (कई) हाथ से बनाए गए नियमों (जैसे, नियमित अभिव्यक्तियां), और कस्टम फाइन-ट्यून्ड एमएल मॉडलों के संयोजन पर निर्भर करते हैं।

ऐसे प्रणाली समय के साथ जटिल हो जाती हैं और बनाए रखने और बेहतर बनाने में क्रमिक रूप से अधिक महंगी हो जाती हैं।

LLMs को विशिष्ट निकालने के कार्यों के लिए त्वरित अनुकूलित किया जा सकता है केवल उचित निर्देश और उचित संदर्भ उदाहरण प्रदान करके।

यह गाइड आपको LLMs का उपयोग करके निकालने के अनुप्रयोगों का उपयोग करना दिखाएगी!

## दृष्टिकोण

LLMs का उपयोग करके सूचना निकालने के लिए 3 व्यापक दृष्टिकोण हैं:

- **उपकरण/कार्य कॉलिंग** मोड: कुछ LLMs एक *उपकरण या कार्य कॉलिंग* मोड का समर्थन करते हैं। ये LLMs एक दिए गए **स्कीमा** के अनुसार आउटपुट को संरचित कर सकते हैं। आमतौर पर, यह दृष्टिकोण काम करने में सबसे आसान है और अच्छे परिणाम देने की उम्मीद है।

- **JSON मोड**: कुछ LLMs को मजबूर किया जा सकता है कि वे मान्य JSON उत्पन्न करें। यह **उपकरण/कार्य कॉलिंग** दृष्टिकोण के समान है, लेकिन स्कीमा को प्रोम्प्ट के भाग के रूप में प्रदान किया जाता है। आमतौर पर, हमारा अनुमान है कि यह **उपकरण/कार्य कॉलिंग** दृष्टिकोण से खराब प्रदर्शन करता है, लेकिन हमारे पर भरोसा न करें और अपने उपयोग मामले के लिए सत्यापित करें!

- **प्रोम्प्टिंग आधारित**: LLMs जो निर्देशों का अच्छी तरह से पालन कर सकते हैं, उन्हें वांछित प्रारूप में पाठ उत्पन्न करने के लिए निर्देशित किया जा सकता है। उत्पन्न पाठ को मौजूदा [आउटपुट पार्सर](/docs/modules/model_io/output_parsers/) या [कस्टम पार्सर](/docs/modules/model_io/output_parsers/custom) का उपयोग करके JSON जैसे संरचित प्रारूप में पार्स किया जा सकता है। यह दृष्टिकोण उन LLMs के साथ उपयोग किया जा सकता है जो **JSON मोड** या उपकरण/कार्य कॉलिंग मोड का समर्थन नहीं करते हैं। यह दृष्टिकोण व्यापक रूप से लागू किया जा सकता है, हालांकि निकालने या कार्य कॉलिंग के लिए फाइन-ट्यून्ड मॉडलों की तुलना में खराब परिणाम दे सकता है।

## त्वरित शुरुआत

[त्वरित शुरुआत](/docs/use_cases/extraction/quickstart) पर जाएं ताकि आप **उपकरण/कार्य कॉलिंग** दृष्टिकोण का उपयोग करके LLMs का उपयोग करके सूचना निकालने का एक आधारभूत एंड-टू-एंड उदाहरण देख सकें।

त्वरित शुरुआत **उपकरण/कार्य कॉलिंग** दृष्टिकोण का उपयोग करके सूचना निकालने पर केंद्रित है।

## कैसे-करें गाइड

- [संदर्भ उदाहरणों का उपयोग करें](/docs/use_cases/extraction/how_to/examples): जानें कि **संदर्भ उदाहरणों** का उपयोग करके प्रदर्शन कैसे बेहतर किया जा सकता है।
- [लंबे पाठ को संभालें](/docs/use_cases/extraction/how_to/handle_long_text): LLM के संदर्भ विंडो में पाठ नहीं फिट होने पर क्या करना चाहिए?
- [फ़ाइलों को संभालें](/docs/use_cases/extraction/how_to/handle_files): PDF जैसी फ़ाइलों से निकालने के लिए LangChain दस्तावेज लोडर और पार्सर का उपयोग करने के उदाहरण।
- [एक पार्सिंग दृष्टिकोण का उपयोग करें](/docs/use_cases/extraction/how_to/parse): **उपकरण/कार्य कॉलिंग** का समर्थन नहीं करने वाले मॉडलों से निकालने के लिए प्रोम्प्ट आधारित दृष्टिकोण का उपयोग करें।

## दिशानिर्देश

[दिशानिर्देश](/docs/use_cases/extraction/guidelines) पृष्ठ पर जाएं ताकि आप निकालने के उपयोग मामलों के लिए सर्वश्रेष्ठ प्रदर्शन प्राप्त करने के लिए एक सूची देख सकें।

## उपयोग मामले एक्सेलरेंट

[langchain-extract](https://github.com/langchain-ai/langchain-extract) एक स्टार्टर रेपो है जो पाठ और फ़ाइलों से सूचना निकालने के लिए LLMs का उपयोग करने के लिए एक सरल वेब सर्वर को लागू करता है। यह **FastAPI**, **LangChain** और **Postgresql** का उपयोग करके बनाया गया है। अपने स्वयं के उपयोग मामलों के अनुकूल बनाने के लिए स्वतंत्र महसूस करें।

## अन्य संसाधन

* [आउटपुट पार्सर](/docs/modules/model_io/output_parsers/) प्रलेखन में विभिन्न पार्सर उदाहरण शामिल हैं (जैसे, सूची, दिनांक-समय, enum, आदि)।
* LangChain [दस्तावेज लोडर](/docs/modules/data_connection/document_loaders/) फ़ाइलों से सामग्री लोड करने के लिए। [एकीकरण](/docs/integrations/document_loaders) की सूची देखें।
* प्रयोगात्मक [Anthropic कार्य कॉलिंग](/docs/integrations/chat/anthropic_functions) समर्थन Anthropic चैट मॉडलों के समान कार्यक्षमता प्रदान करता है।
- [LlamaCPP](/docs/integrations/llms/llamacpp#grammars) स्थानीय LLMs का उपयोग करके कस्टम व्याकरण का उपयोग करके प्रतिबंधित डिकोडिंग का नैतिक समर्थन करता है, जिससे संरचित सामग्री उत्पन्न करना आसान हो जाता है।
- [JSONFormer](/docs/integrations/llms/jsonformer_experimental) JSON स्कीमा के एक उपसेट के लिए संरचित डिकोडिंग के लिए एक और तरीका प्रदान करता है।
- [Kor](https://eyurtsev.github.io/kor/) एक अन्य पुस्तकालय है जहां स्कीमा और उदाहरण LLM को प्रदान किए जा सकते हैं। Kor एक पार्सिंग दृष्टिकोण के लिए अनुकूलित है।
- [OpenAI के कार्य और उपकरण कॉलिंग](https://platform.openai.com/docs/guides/function-calling)
- उदाहरण के लिए, [OpenAI के JSON मोड](https://platform.openai.com/docs/guides/text-generation/json-mode) देखें।
